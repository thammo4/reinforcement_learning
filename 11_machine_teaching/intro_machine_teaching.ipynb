{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction to Machine Teaching\n",
    "\n",
    "### University of Virginia\n",
    "### Reinforcement Learning\n",
    "#### Last updated: November 21, 2023\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TO DO\n",
    "\n",
    "- Expand on Reward Shaping and other methods\n",
    "- Include Mountain Car example here or in separate Lab\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### SOURCES \n",
    "\n",
    "- Mastering Reinforcement Learning with Python, Enes Bilgin. Chapter 10\n",
    "\n",
    "### LEARNING OUTCOMES\n",
    "\n",
    "- Explain the benefit of Machine Teaching (MT)\n",
    "- Describe different methods for MT\n",
    "- Apply Reward Shaping to teach an agent to reach a goal\n",
    "- Describe a method for preventing impossible / unwanted actions from a given state\n",
    "\n",
    "### CONCEPTS\n",
    "\n",
    "- Machine Teaching\n",
    "- Concept (part of the skillset)\n",
    "- Reward Shaping\n",
    "- Action Masking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I. Machine Teaching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RL requires a large amount of data since the agent may be only exposed to training examples...and not guidance\n",
    "\n",
    "*Machine Teaching (MT)* focuses on extracting knowledge from a *teacher*\n",
    "\n",
    "A teacher is a subject-matter expert on a topic\n",
    "\n",
    "This can be more efficient (fewer samples / less training / less compute needed)\n",
    "\n",
    "This is certainly true for humans learning from a teacher as well\n",
    "\n",
    "The teacher infuses knowledge into the machine\n",
    "\n",
    "But how to do this?\n",
    "\n",
    "We will touch on several approaches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II. Concept"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A *concept* is a part of the skillset needed to solve a problem\n",
    "\n",
    "It can be helpful to break problems down this way, as it makes things easier\n",
    "\n",
    "Consider learning:\n",
    "\n",
    "- How to play chess if rewards are only given at the end. Can be helpful for intermediate rewards (capturing a queen, having a strong opening)\n",
    "\n",
    "- How to play basketball if rewards are based on final score. Can be helpful for learning skills such as:\n",
    "  - Passing\n",
    "  - Rebounding\n",
    "  - Shooting\n",
    "  - Dribbling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### III. Reward Shaping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Several properties of a problem can make it hard for the agent to learn the optimal policy, such as:\n",
    "\n",
    "- **Sparse rewards.** If feedback doesn't come often, it can be hard for the agent to learn if it's taking the right actions\n",
    "\n",
    "- **Qualitative objectives.** A task such as *walking* can be hard to define.\n",
    "\n",
    "- **Multi-objective task.** It may be necessary for the agent to learn to balance multiple priorities. Autonomous driving involves learning how to balance speed, safety, and fuel efficiency, for example. Ultimately, these components need to be weighted and combined into a scalar value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Reward shaping* involves designing a function that moves the agent towards success states and away from failure states. \n",
    "\n",
    "Positive rewards are given for moving toward good states  \n",
    "Negative rewards are given for moving toward bad states \n",
    "\n",
    "There are important considerations for this to work well, such as:\n",
    "- Providing the right incentive to reach a goal (and not linger near the goal)\n",
    "- Providing the right incentive to end ASAP when needed\n",
    "- Taking into account the relative size of rewards\n",
    "\n",
    "**Warning:** The agent learns behavior based on the rewards. Sometimes the agent learns behavior that maximizes reward but isn't what the designer had in mind. \n",
    "\n",
    "**Question:** Suppose we design a reward function that gives a constant reward for all states. Will the agent learn? Explain your answer.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we look at reward shaping examples.\n",
    "\n",
    "**Example 1: Shaped Reward Function**\n",
    "\n",
    "Imagine a robot that can have position in $[-1,1]$  \n",
    "The figure below shows a reward function for moving an agent toward the goal state=1  \n",
    "As the agent moves from 0 to 1, the reward grows larger (by the square of the state value)  \n",
    "As the agent moves from 0 to -1, the penalty grows larger \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./shaped_reward1.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example 2: Designing Rewards to Prevent Sepsis**\n",
    "\n",
    "In the paper *Deep Reinforcement Learning for Sepsis Treatment* by Raghu et. al., the task is to prevent sepsis by controlling **SOFA** and **Lactate**. \n",
    "\n",
    "These measures are proxies for overall patient health.\n",
    "\n",
    "This is a multi-objective task.\n",
    "\n",
    "The reward function is composed of intermediate rewards and terminal rewards: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./raghu_sepsis_reward.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IV. Action Masking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From a given state, taking certain actions may be unwanted or even impossible.  \n",
    "\n",
    "Example:\n",
    "- Prescribing a medication dose which is likely to put a patient at high risk\n",
    "- Moving a robot off a cliff\n",
    "\n",
    "We can prevent the agent from taking such actions with *action masking*\n",
    "\n",
    "This can avoid transitions to bad states and also limit the possible action space\n",
    "\n",
    "How to do this?\n",
    "\n",
    "- For value-based algorithms like Q-Learning, can assign value of $-\\infty$ for these states: $Q(s,a)=-\\infty$\n",
    "- For policy gradient algorithms, set logits to $-\\infty$. This results in assigning probability of zero \n",
    "to unwanted action given the state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

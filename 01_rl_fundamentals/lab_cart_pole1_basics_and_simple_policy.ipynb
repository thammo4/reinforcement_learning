{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-GBod1nv65dN"
   },
   "source": [
    "## Lab: Cart Pole using OpenAI gym\n",
    "## RL Basics and Simple Policy\n",
    "\n",
    "### University of Virginia\n",
    "### Reinforcement Learning\n",
    "#### Last updated: December 11, 2023\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Instructions:  \n",
    "\n",
    "Carefully read the notes below and run the provided code. Answer each question clearly and show all results.\n",
    "\n",
    "#### TOTAL POINTS: 10\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "krpOsEbE_aTj"
   },
   "source": [
    "### Agent and Environment\n",
    "\n",
    "It is essential for the agent to have a way to get the next state and reward from the environment. \n",
    "\n",
    "Sometimes it is possible for the agent to interact with environment in real life, but often this is expensive / dangerous / impossible.\n",
    "\n",
    "We use models and simulators in this latter case.\n",
    "\n",
    "For this reason, the Gym package is useful in RL\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fVwAKKjUBxFT"
   },
   "source": [
    "### Introduction to Gym\n",
    "\n",
    "We will work with [Gym](https://gym.openai.com/) from Open AI.\n",
    "\n",
    "Gym is a toolkit for developing and comparing RL algorithms.\n",
    "\n",
    "It comes with many pre-built environments.\n",
    "\n",
    "Users can build their own custom environments. See [here](https://towardsdatascience.com/creating-a-custom-openai-gym-environment-for-stock-trading-be532be3910e#:~:text=8%20min%20read-,Create%20custom%20gym%20environments%20from%20scratch%20%E2%80%94%20A%20stock%20market%20example,Atari%20games%20to%20experiment%20with.) for example.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jUTU3338AvT9"
   },
   "source": [
    "### Cart Pole Problem\n",
    "\n",
    "The [**CartPole**](https://gym.openai.com/videos/2019-10-21--mqt8Qj1mwo/CartPole-v1/original.mp4) problem has a small state space and action space, so it's popular for illustrating ideas. \n",
    "\n",
    "Pole is attached to a cart on a frictionless track.\n",
    "\n",
    "Pole starts upright\n",
    "\n",
    "**Goal** is to keep pole from falling over\n",
    "\n",
    "Control system by applying **force** -1 or +1 to cart.\n",
    "\n",
    "**Reward** of +1 for each timestep the pole remains upright\n",
    "\n",
    "**Episode** ends when pole is more than 12 degrees from vertical, or cart moves more than 2.4 units from center\n",
    "\n",
    "CartPole-v1 defines *solving* as getting average reward of 195.0 over 100 consecutive trials."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./cartpole.png\" alt=\"drawing\" width=\"150\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rA_dd0h6Eoap"
   },
   "source": [
    "### Setup and First Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OgSNDjuzow2U"
   },
   "source": [
    "This notebook can be easily run on [Google Colab](https://colab.research.google.com/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "ed_DyvHy6pbh"
   },
   "outputs": [],
   "source": [
    "import gym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KA4JgtuL5jC9"
   },
   "source": [
    "Load the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "UcGLRi6F6vcQ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([0.04225422, 0.02126478, 0.02520455, 0.00700802], dtype=float32), {})\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"CartPole-v1\")\n",
    "\n",
    "state = env.reset(seed=314)\n",
    "print(state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VtQFJsxj7aBj"
   },
   "source": [
    "Given the state, we take an action. The next state comes from the environment, which is encoded in `gym`.\n",
    "\n",
    "The first element holds components:   \n",
    "[0]: cart horizontal position (0.0 = center)  \n",
    "[1]: velocity (positive means right)  \n",
    "[2]: angle of the pole (0.0 = vertical)  \n",
    "[3]: pole's angular velocity (positive means clockwise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7A5peNKiBXsh",
    "outputId": "59a30da4-be9d-4f8d-a178-8fd5dbf66b3f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# state space number of components\n",
    "env.observation_space.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8GkZmsIN7ky6"
   },
   "source": [
    "The action space consists of two options: \n",
    "\n",
    "[0]: move cart left   \n",
    "[1]: move cart right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ySQ8EpsV7Ngr",
    "outputId": "bb78688d-b282-4338-a9ce-c229ed60906a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discrete(2)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_68lpUuVCc-_"
   },
   "source": [
    "Let's take an action, draw a sample and look at the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gKFEIHjL7n7h",
    "outputId": "504c31ce-d526-4bfa-9842-ab4af59d7e7a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state [ 0.04267951  0.21601637  0.02534471 -0.27761722]\n",
      "reward 1.0\n",
      "done False\n",
      "info {}\n"
     ]
    }
   ],
   "source": [
    "# move right\n",
    "action = 1\n",
    "\n",
    "# take a step and get next state, reward from environment\n",
    "state, reward, terminated, truncated, info = env.step(action)\n",
    "done = terminated or truncated\n",
    "\n",
    "print('state', state)\n",
    "print('reward', reward)\n",
    "print('done', done)\n",
    "print('info', info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note about DONE**  \n",
    "We need to understand if the episode is done after taking action  \n",
    "\n",
    "The API now gives more detail on this variable, which may reach `done` state for two reasons:  \n",
    "- **terminated**=True if environment terminates (eg. due to task completion, failure etc.)  \n",
    "- **truncated**=True if episode truncates due to a time limit or a reason that is not defined as part of the task MDP."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Run several steps by taking random actions**  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([ 0.04267951,  0.21601637,  0.02534471, -0.27761722], dtype=float32), 1.0, False, False, {})\n",
      "(array([ 0.04699984,  0.41076773,  0.01979236, -0.56219995], dtype=float32), 1.0, False, False, {})\n",
      "(array([ 0.05521519,  0.21537372,  0.00854836, -0.26334772], dtype=float32), 1.0, False, False, {})\n",
      "(array([ 0.05952267,  0.4103726 ,  0.00328141, -0.5533222 ], dtype=float32), 1.0, False, False, {})\n",
      "(array([ 0.06773012,  0.60544837, -0.00778503, -0.84496945], dtype=float32), 1.0, False, False, {})\n"
     ]
    }
   ],
   "source": [
    "state = env.reset(seed=314)\n",
    "for _ in range(5):\n",
    "    print(env.step(env.action_space.sample())) # take a random action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bqxI0vej88Kh"
   },
   "source": [
    "**Reward and Episode**  \n",
    "\n",
    "For each time step that the cart keeps the pole balanced, it earns reward 1.\n",
    "\n",
    "If the pole tilts too much or if the cart moves off screen, `reward=0` and `done=True` (the episode will end).\n",
    "\n",
    "When the episode ends, a new episode may begin. The process learns cumulatively from each episode.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Yv5gKVde9pvQ"
   },
   "source": [
    "#### 1) Defining a function that runs a simple policy\n",
    "**(POINTS: 1)**\n",
    "\n",
    "When the pole leans left (negative angle), move left. When the pole leans right (positive angle), move right.  \n",
    "\n",
    "The function should take the state and return an action. Test that it works properly.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2) Simulating episodes with the simple policy  \n",
    "\n",
    "2a) **(POINTS: 2)** Run 1000 episodes each with 100 time steps. Use the `simple_policy` for taking actions.  \n",
    "Each time step will call the `step()` method to get the next state and reward. Produce a boxplot of the rewards from each episode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2b) **(POINTS: 1)** Is this policy able to solve the cart pole problem? Explain your answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3) From Question [2], what is the mean and maximum reward (roughly)?\n",
    "**(POINTS: 1)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gnH4J3988wNe"
   },
   "source": [
    "#### 4) Reversed Simple Policy\n",
    "**(POINTS: 2)**\n",
    "\n",
    "What happens if you reverse the simple policy, moving left when the pole leans right, and moving right when the pole leans left? This is not a good idea, but it's instructive. To show the result, produce the boxplot from before, and calculate the mean reward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5) Modified Policy\n",
    "**(POINTS: 2)** Full points for attempt and clear explanation.\n",
    "\n",
    "Time to get creative! See if you can try a different policy that improves the mean reward. Clearly explain your strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t_61H87rA_MT"
   },
   "source": [
    "---  \n",
    "\n",
    "### Wrapup\n",
    "\n",
    "This demo illustrated some basic ideas of reinforcement learning and got you started with OpenAI Gym. \n",
    "\n",
    "We will revisit this example later, bringing in more tools for a better solution.\n",
    "\n",
    "---\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

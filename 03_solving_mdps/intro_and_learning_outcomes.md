
## INTRODUCTION TO THE MODULE

We learn three approaches to solving Markov Decision Processes: Dynamic Programming, Monte Carlo Simulation, and Temporal-Difference Methods. Each have strengths and weaknesses, and we study the tradeoffs. Q-learning has become extremely popular, as it is model free and makes updates after each transition.

## LEARNING OUTCOMES

At the conclusion of this module, you should be able to:

- Understand the conceptual ideas behind dynamic programming 
- Study the Bellman optimality equations for estimating the optimal value functions
- Discuss the strengths and limitations of Dynamic Programming
- Understand and apply the value iteration algorithm
- Explain how generalized policy iteration works to find optimality
- Apply Monte Carlo simulaton to estimate value functions
- Understand the strengths and limitations of Monte Carlo methods
- Explain how temporal-difference learning works, and contrast it with MC and DP methods
- Explain how Q-learning works

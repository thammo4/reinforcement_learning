{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Approaches to Solving MDPs III: Temporal Difference\n",
    "\n",
    "### University of Virginia\n",
    "### Reinforcement Learning\n",
    "#### Last updated: November 3, 2023\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### SOURCES \n",
    "\n",
    "- Reinforcement Learning, RS Sutton & AG Barto, 2nd edition. Chapter 6\n",
    "- Mastering Reinforcement Learning with Python, Enes Bilgin. Chapter 5\n",
    "\n",
    "### LEARNING OUTCOMES\n",
    "\n",
    "- Understand how the temporal-difference method makes updates\n",
    "- Explain how TD updates are an improvement over MC updates\n",
    "- Explain how Q-learning works\n",
    "\n",
    "### CONCEPTS\n",
    "\n",
    "- Temporal-Difference method\n",
    "- Temporal-Difference error\n",
    "- Sarsa\n",
    "- Q-learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Temporal-Difference (TD) Methods\n",
    "\n",
    "TD learning is a central and novel idea in RL\n",
    "\n",
    "Learns from experience without a model (like MC)\n",
    "\n",
    "Updates estimates based on other learned estimates (bootstrapping, like DP)\n",
    "\n",
    "Difference from MC: TD makes useful updates after each time step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Illustration Comparing MC Updates to TD Updates**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./mc_vs_td.png\"  width=700>  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One limitation of MC is that we must simulate a full trajectory before updating a policy. This is because the target is the gain $G_t$.\n",
    "\n",
    "**Revisiting the commute time example**  \n",
    "if we realize mid-trip that we're running 30 minutes late, we can use this information  \n",
    "mid-trip to update the total commute time estimate.\n",
    "\n",
    "\n",
    "**What do we mean by bootstrapping?**  \n",
    "Basing an update on an existing estimate. The initial estimates are incorrect but useful as we can refine them.\n",
    "\n",
    "\n",
    "**TD(0)** is a method that learns after one time step  \n",
    "\n",
    "The method is mathematically sound and for any policy $\\pi$, the estimate of value function $v_\\pi$ converges\n",
    "\n",
    "Modern RL algorithms implement TD methods with function approximation - specifically neural nets\n",
    "\n",
    "Here is the TD(0) backup diagram, which shows flow from state -> action -> next state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./td_backup.png\" width=30>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TD Prediction\n",
    "\n",
    "Let's start with state-value function of policy $\\pi$ starting from state *s*:\n",
    "\n",
    "$v_\\pi(s) = \\mathbb{E}_\\pi[R_{t+1} + \\gamma v_\\pi(S_{t+1}) | S_t=s]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taking a step from *s* under $\\pi$ produces action *a*, reward *r* and next state *s'*\n",
    "\n",
    "observed quantity $r + \\gamma v_\\pi(s')$ gives new estimate of $v_\\pi(s)$ based on one sample\n",
    "\n",
    "**Important idea: use this observation to update the existing value estimate by moving it closer to new observation** \n",
    "\n",
    "Don't want to discard old estimate, as new one may be noisy and it's sample size of one.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Update Rule\n",
    "\n",
    "We update $v_\\pi(s)$ after a single step by computing the convex combination of its old estimate and the observed quantity.  \n",
    "\n",
    "A weight $\\alpha\\in (0,1]$ is applied for mixing the two, where larger value weights the recent observation more heavily.  \n",
    "\n",
    "The $\\alpha$ portion is applied to the recent observation.\n",
    "\n",
    "We will use $V$ to denote values which are based on sample data under policy $\\pi$.\n",
    "\n",
    "$V(s) := (1-\\alpha) V(s) + \\alpha (r + \\gamma V(s'))$ \n",
    "\n",
    "Notice: we now have a way to update the state value after a single transition based on data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TD Error\n",
    "\n",
    "Let's rearrange the update equation:\n",
    "\n",
    "\n",
    "$\n",
    "\\begin{aligned}  \n",
    "V(s) &:= (1-\\alpha) V(s) + \\alpha (r + \\gamma V(s'))  \\\\\n",
    "&= V(s) + \\alpha [r + \\gamma V(s') -  V(s)]\n",
    "\\end{aligned}\n",
    "$\n",
    "\n",
    "This form make the update clear, and it presents a term $[r + \\gamma V(s') -  V(s)]$ which we call *TD error*.  \n",
    "\n",
    "We saw an update form like this earlier. A similar form appears in gradient descent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 1**  \n",
    "Think about the TD error and how it works. Does it make sense?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2**  \n",
    "What happens when $\\alpha=0$? What happens when $\\alpha=1$?  \n",
    "Is it a good idea to use these values?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finding the Optimal Policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We won't have the environment dynamics. To improve the policy, we need estimate of the action values $q(s,a)$.\n",
    "\n",
    "As a reminder, $q_\\pi(s,a)$ represents starting in state $s$, taking action $a$ and subsequently following policy $\\pi$.\n",
    "\n",
    "Why do we take action $a$? This helps us try to improve the policy.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sarsa\n",
    "\n",
    "A popular on-policy method is *Sarsa*.  \n",
    "This uses the behavior policy.  \n",
    "It considers transitions from one state-action pair $ (S_t, A_t) $ to another $ (S_{t+1}, A_{t+1}) $.\n",
    "\n",
    "Given the reward that occurs $R_{t+1}$, we have the tuple  \n",
    "$ (S_t, A_t, R_{t+1}, S_{t+1}, A_{t+1}) $  \n",
    "which is the source of the name $Sarsa$\n",
    "\n",
    "See Sutton & Barto Ch 6.4 for details.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q-learning\n",
    "\n",
    "A popular off-policy method is *Q-learning*.  \n",
    "Recall that in off-policy learning, there is a target policy and a separate behavior policy.\n",
    "\n",
    "Q-Learning and related methods are very popular in RL.\n",
    "\n",
    "We will discuss *Q-learning* going forward.  \n",
    "Taking TD(0) update steps will be essential.\n",
    "\n",
    "Algorithm shown below.   \n",
    "The update step uses the TD error, and $\\alpha$ controls learning rate.  \n",
    "Under target policy, the action used in the update step will be the one that maximizes the value estimate $Q$.  \n",
    "Under behavior policy, the action taken may be different; the policy can be $\\epsilon$-greedy, for example.  \n",
    "By keeping evaluation separate from the action taken, it allows for exploration without penalty.\n",
    "\n",
    "Notice the update happens after each transition from $(S,A)$ to $(S',a)$.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./q_learning.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example of separate policies**:  \n",
    "Your estimate of the shortest commute time from home to work is 20 minutes.  \n",
    "Under behavior policy, you explore and try a different turn (action=$A$)  \n",
    "and find it takes 5 minutes longer (reward $R$ will be lower)  \n",
    "The $Q(S,A)$ update will be lowered  \n",
    "The target policy (which is best thinking) won't be impacted by this bad action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Approaches to Solving MDPs II: Monte Carlo Simulation\n",
    "\n",
    "### University of Virginia\n",
    "### Reinforcement Learning\n",
    "#### Last updated: September 5, 2023\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### SOURCES \n",
    "\n",
    "- Reinforcement Learning, RS Sutton & AG Barto, 2nd edition. Chapter 5\n",
    "- Mastering Reinforcement Learning with Python, Enes Bilgin. Chapter 5\n",
    "\n",
    "### LEARNING OUTCOMES\n",
    "\n",
    "- Apply Monte Carlo simulaton to estimate value functions\n",
    "- Understand the strengths and limitations of Monte Carlo methods\n",
    "- Explain the purpose of exploring starts\n",
    "- Understand why importance sampling is needed for off-policy learning\n",
    "\n",
    "### CONCEPTS\n",
    "\n",
    "- Monte Carlo simulation can be used to measure value functions without knowledge of transition probabilities\n",
    "- Exploring starts\n",
    "- Simulation risk\n",
    "- On-policy and off-policy methods\n",
    "- Importance sampling\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monte Carlo (MC) Methods\n",
    "\n",
    "MC does not require complete knowledge of the environment.\n",
    "\n",
    "Instead, it relies on sampling by simulating many trajectories, where a trajectory looks like: $s_t,a_t,r_{t+1},s_{t+1},a_{t+1},r_{t+2},s_{t+2},...,r_T,s_T$\n",
    "\n",
    "For measuring state value function $v_\\pi(s)$ and action value function $q_\\pi(s,a)$,  \n",
    "- start with $s$ in the former and $s, a$ in the latter  \n",
    "- generate many sample trajectories: sequences of state-action-reward tuples\n",
    "- compute the mean\n",
    "\n",
    "One challenge with computing $q_\\pi(s,a)$ by simulation is that many state-action pairs $(s,a)$ may not be visited  \n",
    "Can force a given $(s,a)$ by starting an episode in that condition; called **exploring starts.**\n",
    "\n",
    "**MC is an approximation**: while the true values would require infinite simulated trajectories, we can get *close enough* by simulating a massive number of paths.\n",
    "\n",
    "MC makes updates when trajectories are completed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comparison to Dynamic Programming**\n",
    "\n",
    "DP uses one-step transitions and it calculates expectations of taking various paths\n",
    "\n",
    "MC uses full paths for making updates\n",
    "\n",
    "Backup diagram is simpler than DP: chain of state-action-state-action ... leading to final state.\n",
    "\n",
    "White circles are states, black circles are actions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./mc_backup_diagram.png\" width=40>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Independence**\n",
    "\n",
    "Estimates for each state in MC are independent. This makes it embarrisingly parallel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Blackjack!\n",
    "\n",
    "Next we simulate blackjack to study policies and values. If we wanted to use DP, this would be very hard as transition probabilities need to be computed in advance. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![basic_strategy](./blackjack_basic_strategy.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Blackjack Policy Simulator\n",
    "\n",
    "The object of the game is to obtain cards with total value as great as possible without exceeding 21 and beat the dealer.\n",
    "\n",
    "The rules:\n",
    "\n",
    "- Face card (Jack/Queen/King) values are 10\n",
    "- Ace value is 1 or 11. When player has option of either value, ace is called *usable*.\n",
    "- Two cards are dealt to player and dealer\n",
    "- One of the dealer's cards is face up\n",
    "- If the player has 21 and the dealer doesn't, the player wins\n",
    "- If the player has 21 and the dealer has 21, it is a draw\n",
    "- If the player does not have 21, he can draw cards until he stops (sticks) or exceeds 21 (goes bust)\n",
    "- If he goes bust, he loses\n",
    "- If he sticks, it is the dealer's turn\n",
    "- The dealer must stick on a sum of 17 or greater and hits otherwise\n",
    "- If the dealer goes bust, the player wins. Otherwise, the winner has final sum closer to 21.\n",
    "\n",
    "Assumption\n",
    "\n",
    "- infinite card deck\n",
    "\n",
    "Policy\n",
    "\n",
    "- a simple policy we can consider: player sticks if sum greater than *max_total_hit*, else hits  \n",
    "\n",
    "A more complete policy will choose based on 1) sum of player cards 2) dealer card shown 3) player having usable aces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** Based on these three variables, how many possible states are there?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CardGenerator():\n",
    "    def __init__(self):\n",
    "        self.card_values = [2,3,4,5,6,7,8,9,10,11]\n",
    "        self.weights=[1/13]*8 + [4/13] + [1/13]\n",
    "\n",
    "    def draw_card(self):\n",
    "        return random.choices(self.card_values, self.weights)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasePlayer():\n",
    "    def __init__(self, verbose, name):\n",
    "        self.verbose     = verbose\n",
    "        self.name        = name\n",
    "        self.total       = 0\n",
    "        self.usable_aces = 0\n",
    "        self.card_gen    = CardGenerator()\n",
    "        self.is_busted   = False\n",
    "        \n",
    "        # draw two cards and manage aces\n",
    "        new_card = self.card_gen.draw_card()\n",
    "        if self.verbose:\n",
    "            print(self.name)\n",
    "            print('card1', new_card)\n",
    "        self.total += new_card\n",
    "        self.manage_possible_ace(new_card)\n",
    "        new_card = self.card_gen.draw_card()\n",
    "        if self.verbose:\n",
    "            print('card2', new_card)\n",
    "        self.total += new_card\n",
    "        self.manage_possible_ace(new_card)\n",
    "        \n",
    "    def manage_possible_ace(self, new_card):\n",
    "        if new_card == 11:\n",
    "            if self.total > 21:\n",
    "                self.total -= 10\n",
    "            else:\n",
    "                self.usable_aces += 1\n",
    "\n",
    "    def is_over21(self):\n",
    "        if self.total > 21:\n",
    "            if self.usable_aces > 0:\n",
    "                self.total -= 10\n",
    "                self.usable_aces -= 1\n",
    "            else:\n",
    "                self.is_busted = True\n",
    "    \n",
    "    def report_status(self):\n",
    "        print('total', self.total)\n",
    "        print('usable_aces', self.usable_aces)\n",
    "        print('is_busted',self.is_busted)\n",
    "        print('\\n')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Player(BasePlayer):\n",
    "    def __init__(self, verbose, name, max_total_hit):\n",
    "        super().__init__(verbose, name)\n",
    "        self.max_total_hit = max_total_hit\n",
    "        \n",
    "        if self.verbose:\n",
    "            self.report_status()\n",
    "\n",
    "    def followup_play(self):\n",
    "        while self.total <= self.max_total_hit: \n",
    "            print(self.name, 'draws a card...')\n",
    "            new_card = self.card_gen.draw_card()\n",
    "            self.total += new_card  \n",
    "            self.manage_possible_ace(new_card)   \n",
    "            self.is_over21()\n",
    "            if self.verbose:\n",
    "                print('new_card', new_card)\n",
    "                self.report_status()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_end(player, dealer):\n",
    "    #print('player total', player.total)\n",
    "    #print('dealer total', dealer.total)\n",
    "    \n",
    "    if player.total > dealer.total:\n",
    "        return 1\n",
    "    elif player.total == dealer.total:\n",
    "        return 0\n",
    "    else:\n",
    "        return -1\n",
    "    \n",
    "def player_vs_dealer(player, dealer): \n",
    "    \n",
    "    if player.total == 21:\n",
    "        if dealer.total != 21:\n",
    "            return 1\n",
    "        elif dealer.total == 21:\n",
    "            return 0\n",
    "    else:\n",
    "        player.followup_play()\n",
    "        if player.is_busted:\n",
    "            print(player.name, 'busted')\n",
    "            return -1\n",
    "        else:\n",
    "            dealer.followup_play()\n",
    "            if dealer.is_busted:\n",
    "                print(dealer.name, 'busted')\n",
    "                return 1\n",
    "            else:\n",
    "                return compare_end(player, dealer)\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Briefly review the code for understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Testing the card generator\n",
    "\n",
    "First, create a card generator and use it to draw cards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To test whether it seems to generate values with the correct probabilities, draw 10,000 cards, keeping track of the frequency of each value. Does it seem to be working properly? You might increase the number of draws and repeat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Simulate a hand of blackjack\n",
    "\n",
    "Review how the hand plays out and validate that it works properly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "verbose = True\n",
    "max_total_hit = 18\n",
    "\n",
    "plr = Player(verbose, 'Austin Powers', max_total_hit)\n",
    "dealer = Player(verbose, 'Number Two', 16)\n",
    "\n",
    "reward = player_vs_dealer(plr, dealer)\n",
    "print('reward:', reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Simulate a large number of hands for various *max_total_hit* and compute mean reward for each"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "its = 1000\n",
    "\n",
    "thresholds = np.arange(10,20)\n",
    "mean_rewards = np.empty(len(thresholds))\n",
    "\n",
    "ix = 0\n",
    "for thresh in thresholds:\n",
    "    sum_rewards = 0\n",
    "    for it in range(its):\n",
    "        plr = Player(False, 'Player', thresh)\n",
    "        dealer = Player(False, 'Dealer', 16)\n",
    "        sum_rewards += player_vs_dealer(plr, dealer)\n",
    "    mean_rewards[ix] = sum_rewards/its\n",
    "    ix += 1\n",
    "\n",
    "plt.plot(thresholds, mean_rewards)\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "This gives understanding of value of different policies: at which sum should player stick"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(thresholds, mean_rewards)\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Policy to account for dealer upcard \n",
    "\n",
    "How can you can modify the code to allow the policy to additionally depend on the dealer upcard? Test it here and discuss if the results make sense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Policy: player sticks on sum 19-21 and hits otherwise\n",
    "\n",
    "Consider a policy where the player sticks on sum 19-21 and hits otherwise.  \n",
    "How might you estimate the value of this policy given a state where player sum is 13 and dealer upcard is 5?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Measuring value of state combinations\n",
    "\n",
    "If you wish to measure the value of various state combinations (player starting sum, dealer upcard, player usable aces), how can this be done efficiently?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### On-Policy versus Off-Policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two approaches to select all actions infinitely often: \n",
    "\n",
    "- on-policy methods: learns the value of the optimal policy based on agent's actions\n",
    "\n",
    "Agent commits to always exploring and tries to find best policy that still explores  \n",
    "Earlier, we learned an example: $\\epsilon$-greedy policies\n",
    "\n",
    "- off-policy methods: learns the value of the optimal policy independently of the agent's actions\n",
    "\n",
    "This uses two policies: \n",
    "\n",
    "1) policy for learning, which becomes optimal policy. Called the *target policy*\n",
    "\n",
    "2) policy that is exploratory and used for generating behavior. Called the *behavior policy*\n",
    "\n",
    "We will discuss these methods further throughout the course\n",
    "\n",
    "\n",
    "The policies will be functions. For off-policy methods, there will be a function (e.g., a neural network) for the target policy, and another for the behavior policy.\n",
    "\n",
    "**Off policy Example:**  \n",
    "target policy - Agent visits Joe's Shanghai for best dumplings, and Grimaldi's for best pizza. This is optimal from experience. But is it really best? The policy can be informed by the behavior policy and updated.\n",
    "\n",
    "behavior policy - Agent applies policy above, but will randomly explore from each state 5\\% of the time. These learnings feed the target policy.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Off-policy and Importance Sampling\n",
    "\n",
    "Given target policy $\\pi$ and behavior policy $b$\n",
    "\n",
    "Want to use episodes from $b$ to estimate values for $\\pi$ \n",
    "\n",
    "General problem is that we use a random variable with one distribution to compute expected value with a different distribution.\n",
    "\n",
    "We need to adjust by a ratio of the probabilities involving $\\frac{\\pi(a|s)}{b(a|s)}$\n",
    "\n",
    "Since ${b(a|s)}$ is in denominator, we need assumption of *coverage*:\n",
    "\n",
    "Any possible event for $\\pi$  must be possible for $b$. In terms of policies, $\\pi(a|s)>0$ implies $b(a|s)>0$.  \n",
    "\n",
    "**Importance Sampling (IS) Ratio**:\n",
    "\n",
    "$\\rho_{t:T-1} = \\prod_{k=t}^{T-1} \\frac{\\pi(a_k|s_k)}{b(a_k|s_k)}$\n",
    "\n",
    "Computationally, the gains $G$ from MC simulated paths must be weighted by the IS ratio. This amounts to adjusting the expected value.\n",
    "\n",
    "See Sutton and Barto Chapter 5 for more details and *weighted importance sampling*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Limitations to Monte Carlo\n",
    "\n",
    "- Need to wait for a trajectory to be finalized before value update can be made.  \n",
    "Example: you estimate commute time before the trip starts. Midway into the trip, you realize you're running 30 minutes late.  \n",
    "You can't update your estimate until you finish the trip. Is this realistic? Can you do better?\n",
    "\n",
    "- The simulated environment needs to match reality: **simulation risk**  \n",
    "If there is a gap between simulated environment and reality, the model may fail in deployment.\n",
    "\n",
    "For these reasons, there was a push to develop better approaches such as temporal-difference methods.  \n",
    "We discuss these next."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

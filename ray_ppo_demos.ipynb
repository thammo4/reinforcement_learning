{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demo of PPO Algorithm Running in Ray on Different Environments "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: ray[rllib] in /home/apt4c/.local/lib/python3.9/site-packages (2.8.1)\n",
      "Collecting tensorflow\n",
      "  Downloading tensorflow-2.15.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (475.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m475.2/475.2 MB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: torch in /home/apt4c/.local/lib/python3.9/site-packages (2.0.1)\n",
      "Requirement already satisfied: aiosignal in /home/apt4c/.local/lib/python3.9/site-packages (from ray[rllib]) (1.3.1)\n",
      "Requirement already satisfied: click>=7.0 in /home/apt4c/.local/lib/python3.9/site-packages (from ray[rllib]) (8.1.7)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.9/site-packages (from ray[rllib]) (2.28.2)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.9/site-packages (from ray[rllib]) (23.0)\n",
      "Requirement already satisfied: msgpack<2.0.0,>=1.0.0 in /home/apt4c/.local/lib/python3.9/site-packages (from ray[rllib]) (1.0.7)\n",
      "Requirement already satisfied: protobuf!=3.19.5,>=3.15.3 in /home/apt4c/.local/lib/python3.9/site-packages (from ray[rllib]) (3.19.3)\n",
      "Requirement already satisfied: numpy>=1.19.3 in /home/apt4c/.local/lib/python3.9/site-packages (from ray[rllib]) (1.22.4)\n",
      "Requirement already satisfied: frozenlist in /home/apt4c/.local/lib/python3.9/site-packages (from ray[rllib]) (1.4.0)\n",
      "Requirement already satisfied: pyyaml in /home/apt4c/.local/lib/python3.9/site-packages (from ray[rllib]) (6.0.1)\n",
      "Requirement already satisfied: filelock in /home/apt4c/.local/lib/python3.9/site-packages (from ray[rllib]) (3.12.4)\n",
      "Requirement already satisfied: jsonschema in /usr/local/lib/python3.9/site-packages (from ray[rllib]) (4.17.3)\n",
      "Collecting gymnasium==0.28.1\n",
      "  Downloading gymnasium-0.28.1-py3-none-any.whl (925 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m925.5/925.5 kB\u001b[0m \u001b[31m33.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pyarrow>=6.0.1 in /home/apt4c/.local/lib/python3.9/site-packages (from ray[rllib]) (11.0.0)\n",
      "Requirement already satisfied: fsspec in /home/apt4c/.local/lib/python3.9/site-packages (from ray[rllib]) (2023.9.0)\n",
      "Requirement already satisfied: scikit-image in /usr/local/lib/python3.9/site-packages (from ray[rllib]) (0.19.3)\n",
      "Requirement already satisfied: lz4 in /home/apt4c/.local/lib/python3.9/site-packages (from ray[rllib]) (4.3.2)\n",
      "Requirement already satisfied: rich in /usr/local/lib/python3.9/site-packages (from ray[rllib]) (13.3.3)\n",
      "Requirement already satisfied: dm-tree in /home/apt4c/.local/lib/python3.9/site-packages (from ray[rllib]) (0.1.8)\n",
      "Requirement already satisfied: pandas in /home/apt4c/.local/lib/python3.9/site-packages (from ray[rllib]) (2.1.0)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.9/site-packages (from ray[rllib]) (1.8.1)\n",
      "Collecting tensorboardX>=1.9\n",
      "  Downloading tensorboardX-2.6.2.2-py2.py3-none-any.whl (101 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.7/101.7 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting typer\n",
      "  Downloading typer-0.9.0-py3-none-any.whl (45 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.9/45.9 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: importlib-metadata>=4.8.0 in /usr/local/lib/python3.9/site-packages (from gymnasium==0.28.1->ray[rllib]) (6.1.0)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in /home/apt4c/.local/lib/python3.9/site-packages (from gymnasium==0.28.1->ray[rllib]) (4.7.1)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /home/apt4c/.local/lib/python3.9/site-packages (from gymnasium==0.28.1->ray[rllib]) (2.0.0)\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in /home/apt4c/.local/lib/python3.9/site-packages (from gymnasium==0.28.1->ray[rllib]) (0.0.4)\n",
      "Collecting jax-jumpy>=1.0.0\n",
      "  Downloading jax_jumpy-1.0.0-py3-none-any.whl (20 kB)\n",
      "Collecting termcolor>=1.1.0\n",
      "  Downloading termcolor-2.4.0-py3-none-any.whl (7.7 kB)\n",
      "Collecting ml-dtypes~=0.2.0\n",
      "  Downloading ml_dtypes-0.2.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m32.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting absl-py>=1.0.0\n",
      "  Downloading absl_py-2.0.0-py3-none-any.whl (130 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.2/130.2 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.9/site-packages (from tensorflow) (1.16.0)\n",
      "Collecting tensorflow-io-gcs-filesystem>=0.23.1\n",
      "  Downloading tensorflow_io_gcs_filesystem-0.34.0-cp39-cp39-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (2.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m60.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting keras<2.16,>=2.15.0\n",
      "  Downloading keras-2.15.0-py3-none-any.whl (1.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m48.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.9/site-packages (from tensorflow) (3.3.0)\n",
      "Collecting libclang>=13.0.0\n",
      "  Downloading libclang-16.0.6-py2.py3-none-manylinux2010_x86_64.whl (22.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m22.9/22.9 MB\u001b[0m \u001b[31m52.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1\n",
      "  Downloading gast-0.5.4-py3-none-any.whl (19 kB)\n",
      "Collecting tensorboard<2.16,>=2.15\n",
      "  Downloading tensorboard-2.15.1-py3-none-any.whl (5.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m65.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting h5py>=2.9.0\n",
      "  Downloading h5py-3.10.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.8/4.8 MB\u001b[0m \u001b[31m76.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hCollecting wrapt<1.15,>=1.11.0\n",
      "  Downloading wrapt-1.14.1-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (77 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.8/77.8 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting tensorflow-estimator<2.16,>=2.15.0\n",
      "  Downloading tensorflow_estimator-2.15.0-py2.py3-none-any.whl (441 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m442.0/442.0 kB\u001b[0m \u001b[31m17.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting google-pasta>=0.1.1\n",
      "  Using cached google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /home/apt4c/.local/lib/python3.9/site-packages (from tensorflow) (1.58.0)\n",
      "Collecting flatbuffers>=23.5.26\n",
      "  Downloading flatbuffers-23.5.26-py2.py3-none-any.whl (26 kB)\n",
      "Collecting numpy>=1.19.3\n",
      "  Downloading numpy-1.26.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.2/18.2 MB\u001b[0m \u001b[31m61.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.9/site-packages (from tensorflow) (58.1.0)\n",
      "Collecting astunparse>=1.6.0\n",
      "  Using cached astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Collecting protobuf!=3.19.5,>=3.15.3\n",
      "  Downloading protobuf-4.25.1-cp37-abi3-manylinux2014_x86_64.whl (294 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.6/294.6 kB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: nvidia-nccl-cu11==2.14.3 in /home/apt4c/.local/lib/python3.9/site-packages (from torch) (2.14.3)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.9/site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: nvidia-nvtx-cu11==11.7.91 in /home/apt4c/.local/lib/python3.9/site-packages (from torch) (11.7.91)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /home/apt4c/.local/lib/python3.9/site-packages (from torch) (8.5.0.96)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /home/apt4c/.local/lib/python3.9/site-packages (from torch) (11.10.3.66)\n",
      "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /home/apt4c/.local/lib/python3.9/site-packages (from torch) (10.9.0.58)\n",
      "Requirement already satisfied: nvidia-cusolver-cu11==11.4.0.1 in /home/apt4c/.local/lib/python3.9/site-packages (from torch) (11.4.0.1)\n",
      "Requirement already satisfied: nvidia-cusparse-cu11==11.7.4.91 in /home/apt4c/.local/lib/python3.9/site-packages (from torch) (11.7.4.91)\n",
      "Requirement already satisfied: triton==2.0.0 in /home/apt4c/.local/lib/python3.9/site-packages (from torch) (2.0.0)\n",
      "Requirement already satisfied: sympy in /home/apt4c/.local/lib/python3.9/site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /home/apt4c/.local/lib/python3.9/site-packages (from torch) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /home/apt4c/.local/lib/python3.9/site-packages (from torch) (11.7.99)\n",
      "Requirement already satisfied: nvidia-curand-cu11==10.2.10.91 in /home/apt4c/.local/lib/python3.9/site-packages (from torch) (10.2.10.91)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.9/site-packages (from torch) (3.0)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.7.101 in /home/apt4c/.local/lib/python3.9/site-packages (from torch) (11.7.101)\n",
      "Requirement already satisfied: wheel in /usr/local/lib/python3.9/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch) (0.37.1)\n",
      "Requirement already satisfied: cmake in /home/apt4c/.local/lib/python3.9/site-packages (from triton==2.0.0->torch) (3.27.4.1)\n",
      "Requirement already satisfied: lit in /home/apt4c/.local/lib/python3.9/site-packages (from triton==2.0.0->torch) (16.0.6)\n",
      "Collecting tensorboard-data-server<0.8.0,>=0.7.0\n",
      "  Downloading tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl (6.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m74.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hCollecting werkzeug>=1.0.1\n",
      "  Downloading werkzeug-3.0.1-py3-none-any.whl (226 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m226.7/226.7 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting google-auth<3,>=1.6.3\n",
      "  Downloading google_auth-2.24.0-py2.py3-none-any.whl (183 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.8/183.8 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting google-auth-oauthlib<2,>=0.5\n",
      "  Downloading google_auth_oauthlib-1.1.0-py2.py3-none-any.whl (19 kB)\n",
      "Collecting markdown>=2.6.8\n",
      "  Downloading Markdown-3.5.1-py3-none-any.whl (102 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m102.2/102.2 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting protobuf!=3.19.5,>=3.15.3\n",
      "  Downloading protobuf-4.23.4-cp37-abi3-manylinux2014_x86_64.whl (304 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m304.5/304.5 kB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/site-packages (from requests->ray[rllib]) (1.26.15)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/site-packages (from requests->ray[rllib]) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/site-packages (from requests->ray[rllib]) (2022.12.7)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.9/site-packages (from requests->ray[rllib]) (3.1.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.9/site-packages (from jinja2->torch) (2.1.2)\n",
      "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.9/site-packages (from jsonschema->ray[rllib]) (0.19.3)\n",
      "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.9/site-packages (from jsonschema->ray[rllib]) (22.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.9/site-packages (from pandas->ray[rllib]) (2.8.2)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /home/apt4c/.local/lib/python3.9/site-packages (from pandas->ray[rllib]) (2023.3)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.9/site-packages (from pandas->ray[rllib]) (2023.3)\n",
      "Requirement already satisfied: markdown-it-py<3.0.0,>=2.2.0 in /usr/local/lib/python3.9/site-packages (from rich->ray[rllib]) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.9/site-packages (from rich->ray[rllib]) (2.14.0)\n",
      "Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.9/site-packages (from scikit-image->ray[rllib]) (2023.3.21)\n",
      "Requirement already satisfied: pillow!=7.1.0,!=7.1.1,!=8.3.0,>=6.1.0 in /usr/local/lib/python3.9/site-packages (from scikit-image->ray[rllib]) (9.5.0)\n",
      "Requirement already satisfied: imageio>=2.4.1 in /usr/local/lib/python3.9/site-packages (from scikit-image->ray[rllib]) (2.27.0)\n",
      "Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.9/site-packages (from scikit-image->ray[rllib]) (1.4.1)\n",
      "Collecting numpy>=1.19.3\n",
      "  Downloading numpy-1.24.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.3/17.3 MB\u001b[0m \u001b[31m73.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: mpmath>=0.19 in /home/apt4c/.local/lib/python3.9/site-packages (from sympy->torch) (1.3.0)\n",
      "Collecting rsa<5,>=3.1.4\n",
      "  Downloading rsa-4.9-py3-none-any.whl (34 kB)\n",
      "Collecting cachetools<6.0,>=2.0.0\n",
      "  Downloading cachetools-5.3.2-py3-none-any.whl (9.3 kB)\n",
      "Collecting pyasn1-modules>=0.2.1\n",
      "  Downloading pyasn1_modules-0.3.0-py2.py3-none-any.whl (181 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m181.3/181.3 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting requests-oauthlib>=0.7.0\n",
      "  Downloading requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.9/site-packages (from importlib-metadata>=4.8.0->gymnasium==0.28.1->ray[rllib]) (3.15.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.9/site-packages (from markdown-it-py<3.0.0,>=2.2.0->rich->ray[rllib]) (0.1.2)\n",
      "Collecting pyasn1<0.6.0,>=0.4.6\n",
      "  Downloading pyasn1-0.5.1-py2.py3-none-any.whl (84 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.9/84.9 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting oauthlib>=3.0.0\n",
      "  Downloading oauthlib-3.2.2-py3-none-any.whl (151 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m151.7/151.7 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: libclang, flatbuffers, wrapt, werkzeug, typer, termcolor, tensorflow-io-gcs-filesystem, tensorflow-estimator, tensorboard-data-server, pyasn1, protobuf, oauthlib, numpy, keras, google-pasta, gast, cachetools, astunparse, absl-py, tensorboardX, rsa, requests-oauthlib, pyasn1-modules, ml-dtypes, markdown, jax-jumpy, h5py, gymnasium, google-auth, google-auth-oauthlib, tensorboard, tensorflow\n",
      "  Attempting uninstall: protobuf\n",
      "    Found existing installation: protobuf 3.19.3\n",
      "    Uninstalling protobuf-3.19.3:\n",
      "      Successfully uninstalled protobuf-3.19.3\n",
      "\u001b[31mERROR: Could not install packages due to an OSError: [Errno 16] Device or resource busy: '.nfs00000004c637df8100000447'\n",
      "\u001b[0m\u001b[31m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install \"ray[rllib]\" tensorflow torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Taxi-v3**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-03 18:33:12,280\tWARNING __init__.py:10 -- PG has/have been moved to `rllib_contrib` and will no longer be maintained by the RLlib team. You can still use it/them normally inside RLlib util Ray 2.8, but from Ray 2.9 on, all `rllib_contrib` algorithms will no longer be part of the core repo, and will therefore have to be installed separately with pinned dependencies for e.g. ray[rllib] and other packages! See https://github.com/ray-project/ray/tree/master/rllib_contrib#rllib-contrib for more information on the RLlib contrib effort.\n",
      "2023-12-03 18:33:12,295\tWARNING algorithm_config.py:797 -- You have specified 1 evaluation workers, but your `evaluation_interval` is None! Therefore, evaluation will not occur automatically with each call to `Algorithm.train()`. Instead, you will have to call `Algorithm.evaluate()` manually in order to trigger an evaluation run.\n",
      "/home/apt4c/.local/lib/python3.9/site-packages/ray/rllib/algorithms/algorithm.py:484: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "`UnifiedLogger` will be removed in Ray 2.7.\n",
      "  return UnifiedLogger(config, logdir, loggers=None)\n",
      "/home/apt4c/.local/lib/python3.9/site-packages/ray/tune/logger/unified.py:53: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "The `JsonLogger interface is deprecated in favor of the `ray.tune.json.JsonLoggerCallback` interface and will be removed in Ray 2.7.\n",
      "  self._loggers.append(cls(self.config, self.logdir, self.trial))\n",
      "/home/apt4c/.local/lib/python3.9/site-packages/ray/tune/logger/unified.py:53: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "The `CSVLogger interface is deprecated in favor of the `ray.tune.csv.CSVLoggerCallback` interface and will be removed in Ray 2.7.\n",
      "  self._loggers.append(cls(self.config, self.logdir, self.trial))\n",
      "/home/apt4c/.local/lib/python3.9/site-packages/ray/tune/logger/unified.py:53: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "The `TBXLogger interface is deprecated in favor of the `ray.tune.tensorboardx.TBXLoggerCallback` interface and will be removed in Ray 2.7.\n",
      "  self._loggers.append(cls(self.config, self.logdir, self.trial))\n",
      "2023-12-03 18:33:12,326\tINFO tensorboardx.py:48 -- pip install \"ray[tune]\" to see TensorBoard files.\n",
      "2023-12-03 18:33:12,327\tWARNING unified.py:56 -- Could not instantiate TBXLogger: No module named 'tensorboardX'.\n",
      "2023-12-03 18:33:17,899\tINFO worker.py:1673 -- Started a local Ray instance.\n",
      "\u001b[36m(RolloutWorker pid=819490)\u001b[0m 2023-12-03 18:34:26,661\tWARNING __init__.py:10 -- PG has/have been moved to `rllib_contrib` and will no longer be maintained by the RLlib team. You can still use it/them normally inside RLlib util Ray 2.8, but from Ray 2.9 on, all `rllib_contrib` algorithms will no longer be part of the core repo, and will therefore have to be installed separately with pinned dependencies for e.g. ray[rllib] and other packages! See https://github.com/ray-project/ray/tree/master/rllib_contrib#rllib-contrib for more information on the RLlib contrib effort.\n",
      "\u001b[36m(RolloutWorker pid=819490)\u001b[0m /home/apt4c/.local/lib/python3.9/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.single_observation_space to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.single_observation_space` for environment variables or `env.get_wrapper_attr('single_observation_space')` that will search the reminding wrappers.\u001b[0m\n",
      "\u001b[36m(RolloutWorker pid=819490)\u001b[0m   logger.warn(\n",
      "\u001b[36m(RolloutWorker pid=819490)\u001b[0m /home/apt4c/.local/lib/python3.9/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.single_action_space to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.single_action_space` for environment variables or `env.get_wrapper_attr('single_action_space')` that will search the reminding wrappers.\u001b[0m\n",
      "\u001b[36m(RolloutWorker pid=819490)\u001b[0m   logger.warn(\n",
      "/home/apt4c/.local/lib/python3.9/site-packages/torch/cuda/__init__.py:546: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n",
      "2023-12-03 18:34:27,006\tWARNING algorithm_config.py:797 -- You have specified 1 evaluation workers, but your `evaluation_interval` is None! Therefore, evaluation will not occur automatically with each call to `Algorithm.train()`. Instead, you will have to call `Algorithm.evaluate()` manually in order to trigger an evaluation run.\n",
      "\u001b[36m(RolloutWorker pid=819863)\u001b[0m 2023-12-03 18:34:35,761\tWARNING __init__.py:10 -- PG has/have been moved to `rllib_contrib` and will no longer be maintained by the RLlib team. You can still use it/them normally inside RLlib util Ray 2.8, but from Ray 2.9 on, all `rllib_contrib` algorithms will no longer be part of the core repo, and will therefore have to be installed separately with pinned dependencies for e.g. ray[rllib] and other packages! See https://github.com/ray-project/ray/tree/master/rllib_contrib#rllib-contrib for more information on the RLlib contrib effort.\u001b[32m [repeated 2x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/ray-logging.html#log-deduplication for more options.)\u001b[0m\n",
      "\u001b[36m(RolloutWorker pid=819489)\u001b[0m /home/apt4c/.local/lib/python3.9/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.single_observation_space to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.single_observation_space` for environment variables or `env.get_wrapper_attr('single_observation_space')` that will search the reminding wrappers.\u001b[0m\n",
      "\u001b[36m(RolloutWorker pid=819489)\u001b[0m   logger.warn(\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(RolloutWorker pid=819489)\u001b[0m /home/apt4c/.local/lib/python3.9/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.single_action_space to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.single_action_space` for environment variables or `env.get_wrapper_attr('single_action_space')` that will search the reminding wrappers.\u001b[0m\n",
      "\u001b[36m(RolloutWorker pid=819863)\u001b[0m /home/apt4c/.local/lib/python3.9/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.single_observation_space to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.single_observation_space` for environment variables or `env.get_wrapper_attr('single_observation_space')` that will search the reminding wrappers.\u001b[0m\n",
      "\u001b[36m(RolloutWorker pid=819863)\u001b[0m /home/apt4c/.local/lib/python3.9/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.single_action_space to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.single_action_space` for environment variables or `env.get_wrapper_attr('single_action_space')` that will search the reminding wrappers.\u001b[0m\n",
      "2023-12-03 18:34:35,913\tINFO trainable.py:164 -- Trainable.setup took 83.585 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
      "2023-12-03 18:34:35,920\tWARNING util.py:62 -- Install gputil for GPU system monitoring.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'custom_metrics': {}, 'episode_media': {}, 'info': {'learner': {'__all__': {'num_agent_steps_trained': 128.0, 'num_env_steps_trained': 4000.0, 'total_loss': 9.918105450774561}, 'default_policy': {'total_loss': 9.918105450774561, 'policy_loss': -0.006107433734815131, 'vf_loss': 9.923694909508548, 'vf_loss_unclipped': 56362.8119794443, 'vf_explained_var': 9.72940977702517e-06, 'entropy': 1.7854830752303605, 'mean_kl_loss': 0.0025898574588949703, 'default_optimizer_lr': 5.000000000000001e-05, 'curr_lr': 5e-05, 'curr_entropy_coeff': 0.0, 'curr_kl_coeff': 0.10000000149011612}}, 'num_env_steps_sampled': 4000, 'num_env_steps_trained': 0, 'num_agent_steps_sampled': 4000, 'num_agent_steps_trained': 0}, 'sampler_results': {'episode_reward_max': -226.0, 'episode_reward_min': -875.0, 'episode_reward_mean': -724.8095238095239, 'episode_len_mean': 186.0952380952381, 'episode_media': {}, 'episodes_this_iter': 21, 'policy_reward_min': {}, 'policy_reward_max': {}, 'policy_reward_mean': {}, 'custom_metrics': {}, 'hist_stats': {'episode_reward': [-722.0, -565.0, -731.0, -758.0, -839.0, -857.0, -740.0, -587.0, -803.0, -839.0, -794.0, -785.0, -803.0, -803.0, -839.0, -749.0, -875.0, -318.0, -226.0, -839.0, -749.0], 'episode_lengths': [200, 145, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 96, 67, 200, 200]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 2.1882588824620774, 'mean_inference_ms': 7.480710978192646, 'mean_action_processing_ms': 0.7481217903490708, 'mean_env_wait_ms': 0.34781876058808403, 'mean_env_render_ms': 0.0}, 'num_faulty_episodes': 0, 'connector_metrics': {'ObsPreprocessorConnector_ms': 0.07141771770658947, 'StateBufferConnector_ms': 0.005171412513369606, 'ViewRequirementAgentConnector_ms': 0.9212550662812733}}, 'episode_reward_max': -226.0, 'episode_reward_min': -875.0, 'episode_reward_mean': -724.8095238095239, 'episode_len_mean': 186.0952380952381, 'episodes_this_iter': 21, 'policy_reward_min': {}, 'policy_reward_max': {}, 'policy_reward_mean': {}, 'hist_stats': {'episode_reward': [-722.0, -565.0, -731.0, -758.0, -839.0, -857.0, -740.0, -587.0, -803.0, -839.0, -794.0, -785.0, -803.0, -803.0, -839.0, -749.0, -875.0, -318.0, -226.0, -839.0, -749.0], 'episode_lengths': [200, 145, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 96, 67, 200, 200]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 2.1882588824620774, 'mean_inference_ms': 7.480710978192646, 'mean_action_processing_ms': 0.7481217903490708, 'mean_env_wait_ms': 0.34781876058808403, 'mean_env_render_ms': 0.0}, 'num_faulty_episodes': 0, 'connector_metrics': {'ObsPreprocessorConnector_ms': 0.07141771770658947, 'StateBufferConnector_ms': 0.005171412513369606, 'ViewRequirementAgentConnector_ms': 0.9212550662812733}, 'num_healthy_workers': 2, 'num_in_flight_async_reqs': 0, 'num_remote_worker_restarts': 0, 'num_agent_steps_sampled': 4000, 'num_agent_steps_trained': 0, 'num_env_steps_sampled': 4000, 'num_env_steps_trained': 0, 'num_env_steps_sampled_this_iter': 4000, 'num_env_steps_trained_this_iter': 0, 'num_env_steps_sampled_throughput_per_sec': 140.61269081209517, 'num_env_steps_trained_throughput_per_sec': 0.0, 'timesteps_total': 4000, 'num_steps_trained_this_iter': 0, 'agent_timesteps_total': 4000, 'timers': {'training_iteration_time_ms': 28446.887, 'sample_time_ms': 21792.055, 'synch_weights_time_ms': 18.921}, 'counters': {'num_env_steps_sampled': 4000, 'num_env_steps_trained': 0, 'num_agent_steps_sampled': 4000, 'num_agent_steps_trained': 0}, 'done': False, 'episodes_total': 21, 'training_iteration': 1, 'trial_id': 'default', 'date': '2023-12-03_18-35-04', 'timestamp': 1701646504, 'time_this_iter_s': 28.453322410583496, 'time_total_s': 28.453322410583496, 'pid': 817316, 'hostname': 'udc-aw34-10c0', 'node_ip': '10.250.132.124', 'config': {'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'num_learner_workers': 0, 'num_gpus_per_learner_worker': 0, 'num_cpus_per_learner_worker': 1, 'local_gpu_idx': 0, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'env': 'Taxi-v3', 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'auto_wrap_old_gym_envs': True, 'action_mask_key': 'action_mask', '_is_atari': None, 'env_runner_cls': None, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': True, 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 5e-05, 'grad_clip': None, 'grad_clip_by': 'global_norm', 'train_batch_size': 4000, 'model': {'_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [64, 64], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'encoder_latent_dim': None, 'always_check_shapes': False, 'lstm_use_prev_action_reward': -1, '_use_default_native_models': -1}, 'optimizer': {}, 'max_requests_in_flight_per_sampler_worker': 2, '_learner_class': None, '_enable_learner_api': True, 'explore': True, 'exploration_config': {}, 'algorithm_config_overrides_per_module': {}, 'policy_map_capacity': 100, 'policy_mapping_fn': <function AlgorithmConfig.DEFAULT_POLICY_MAPPING_FN at 0x7fc583815a60>, 'policies_to_train': None, 'policy_states_are_swappable': False, 'observation_fn': None, 'count_steps_by': 'env_steps', 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'offline_sampling': False, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': None, 'off_policy_estimation_methods': {}, 'ope_split_batch_by_episode': True, 'evaluation_num_workers': 1, 'always_attach_evaluation_results': False, 'enable_async_evaluation': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'export_native_model_files': False, 'checkpoint_trainable_policies_only': False, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'max_num_worker_restarts': 1000, 'delay_between_worker_restarts_s': 60.0, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'worker_health_probe_timeout_s': 60, 'worker_restore_timeout_s': 1800, 'rl_module_spec': SingleAgentRLModuleSpec(module_class=<class 'ray.rllib.algorithms.ppo.torch.ppo_torch_rl_module.PPOTorchRLModule'>, observation_space=None, action_space=None, model_config_dict=None, catalog_class=<class 'ray.rllib.algorithms.ppo.ppo_catalog.PPOCatalog'>, load_state_path=None), '_enable_rl_module_api': True, '_AlgorithmConfig__prior_exploration_config': {'type': 'StochasticSampling'}, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, '_disable_initialize_loss_from_dummy_batch': False, 'simple_optimizer': False, 'policy_map_cache': -1, 'worker_cls': -1, 'synchronize_filters': -1, 'replay_sequence_length': None, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'use_kl_loss': True, 'kl_coeff': 0.2, 'kl_target': 0.01, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'policies': {'default_policy': (None, None, None, None)}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'torch', 'num_cpus_for_driver': 1, 'num_workers': 2}, 'time_since_restore': 28.453322410583496, 'iterations_since_restore': 1, 'perf': {'cpu_util_percent': 50.81, 'ram_util_percent': 24.375}}\n",
      "{'custom_metrics': {}, 'episode_media': {}, 'info': {'learner': {'__all__': {'num_agent_steps_trained': 128.0, 'num_env_steps_trained': 4000.0, 'total_loss': 9.909734973012766}, 'default_policy': {'total_loss': 9.909734973012766, 'policy_loss': -0.015317763486352843, 'vf_loss': 9.924109350135332, 'vf_loss_unclipped': 48565.68072548807, 'vf_explained_var': -0.00016025642850505772, 'entropy': 1.7547660698768681, 'mean_kl_loss': 0.009433938354916143, 'default_optimizer_lr': 5.000000000000001e-05, 'curr_lr': 5e-05, 'curr_entropy_coeff': 0.0, 'curr_kl_coeff': 0.10000000149011612}}, 'num_env_steps_sampled': 8000, 'num_env_steps_trained': 0, 'num_agent_steps_sampled': 8000, 'num_agent_steps_trained': 0}, 'sampler_results': {'episode_reward_max': -226.0, 'episode_reward_min': -875.0, 'episode_reward_mean': -721.0243902439024, 'episode_len_mean': 192.8780487804878, 'episode_media': {}, 'episodes_this_iter': 20, 'policy_reward_min': {}, 'policy_reward_max': {}, 'policy_reward_mean': {}, 'custom_metrics': {}, 'hist_stats': {'episode_reward': [-722.0, -565.0, -731.0, -758.0, -839.0, -857.0, -740.0, -587.0, -803.0, -839.0, -794.0, -785.0, -803.0, -803.0, -839.0, -749.0, -875.0, -318.0, -226.0, -839.0, -749.0, -677.0, -776.0, -722.0, -749.0, -704.0, -731.0, -668.0, -722.0, -659.0, -749.0, -614.0, -776.0, -812.0, -740.0, -749.0, -704.0, -668.0, -659.0, -731.0, -731.0], 'episode_lengths': [200, 145, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 96, 67, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 2.0039003500995496, 'mean_inference_ms': 6.7343052879144345, 'mean_action_processing_ms': 0.649115845908014, 'mean_env_wait_ms': 0.32082161588079644, 'mean_env_render_ms': 0.0}, 'num_faulty_episodes': 0, 'connector_metrics': {'ObsPreprocessorConnector_ms': 0.07775353222358518, 'StateBufferConnector_ms': 0.019052552013862425, 'ViewRequirementAgentConnector_ms': 0.8209565790688119}}, 'episode_reward_max': -226.0, 'episode_reward_min': -875.0, 'episode_reward_mean': -721.0243902439024, 'episode_len_mean': 192.8780487804878, 'episodes_this_iter': 20, 'policy_reward_min': {}, 'policy_reward_max': {}, 'policy_reward_mean': {}, 'hist_stats': {'episode_reward': [-722.0, -565.0, -731.0, -758.0, -839.0, -857.0, -740.0, -587.0, -803.0, -839.0, -794.0, -785.0, -803.0, -803.0, -839.0, -749.0, -875.0, -318.0, -226.0, -839.0, -749.0, -677.0, -776.0, -722.0, -749.0, -704.0, -731.0, -668.0, -722.0, -659.0, -749.0, -614.0, -776.0, -812.0, -740.0, -749.0, -704.0, -668.0, -659.0, -731.0, -731.0], 'episode_lengths': [200, 145, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 96, 67, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 2.0039003500995496, 'mean_inference_ms': 6.7343052879144345, 'mean_action_processing_ms': 0.649115845908014, 'mean_env_wait_ms': 0.32082161588079644, 'mean_env_render_ms': 0.0}, 'num_faulty_episodes': 0, 'connector_metrics': {'ObsPreprocessorConnector_ms': 0.07775353222358518, 'StateBufferConnector_ms': 0.019052552013862425, 'ViewRequirementAgentConnector_ms': 0.8209565790688119}, 'num_healthy_workers': 2, 'num_in_flight_async_reqs': 0, 'num_remote_worker_restarts': 0, 'num_agent_steps_sampled': 8000, 'num_agent_steps_trained': 0, 'num_env_steps_sampled': 8000, 'num_env_steps_trained': 0, 'num_env_steps_sampled_this_iter': 4000, 'num_env_steps_trained_this_iter': 0, 'num_env_steps_sampled_throughput_per_sec': 202.89025338631106, 'num_env_steps_trained_throughput_per_sec': 0.0, 'timesteps_total': 8000, 'num_steps_trained_this_iter': 0, 'agent_timesteps_total': 8000, 'timers': {'training_iteration_time_ms': 24080.974, 'sample_time_ms': 17434.485, 'synch_weights_time_ms': 16.103}, 'counters': {'num_env_steps_sampled': 8000, 'num_env_steps_trained': 0, 'num_agent_steps_sampled': 8000, 'num_agent_steps_trained': 0}, 'done': False, 'episodes_total': 41, 'training_iteration': 2, 'trial_id': 'default', 'date': '2023-12-03_18-35-24', 'timestamp': 1701646524, 'time_this_iter_s': 19.722851514816284, 'time_total_s': 48.17617392539978, 'pid': 817316, 'hostname': 'udc-aw34-10c0', 'node_ip': '10.250.132.124', 'config': {'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'num_learner_workers': 0, 'num_gpus_per_learner_worker': 0, 'num_cpus_per_learner_worker': 1, 'local_gpu_idx': 0, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'env': 'Taxi-v3', 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'auto_wrap_old_gym_envs': True, 'action_mask_key': 'action_mask', '_is_atari': None, 'env_runner_cls': None, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': True, 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 5e-05, 'grad_clip': None, 'grad_clip_by': 'global_norm', 'train_batch_size': 4000, 'model': {'_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [64, 64], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'encoder_latent_dim': None, 'always_check_shapes': False, 'lstm_use_prev_action_reward': -1, '_use_default_native_models': -1}, 'optimizer': {}, 'max_requests_in_flight_per_sampler_worker': 2, '_learner_class': None, '_enable_learner_api': True, 'explore': True, 'exploration_config': {}, 'algorithm_config_overrides_per_module': {}, 'policy_map_capacity': 100, 'policy_mapping_fn': <function AlgorithmConfig.DEFAULT_POLICY_MAPPING_FN at 0x7fc583815a60>, 'policies_to_train': None, 'policy_states_are_swappable': False, 'observation_fn': None, 'count_steps_by': 'env_steps', 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'offline_sampling': False, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': None, 'off_policy_estimation_methods': {}, 'ope_split_batch_by_episode': True, 'evaluation_num_workers': 1, 'always_attach_evaluation_results': False, 'enable_async_evaluation': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'export_native_model_files': False, 'checkpoint_trainable_policies_only': False, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'max_num_worker_restarts': 1000, 'delay_between_worker_restarts_s': 60.0, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'worker_health_probe_timeout_s': 60, 'worker_restore_timeout_s': 1800, 'rl_module_spec': SingleAgentRLModuleSpec(module_class=<class 'ray.rllib.algorithms.ppo.torch.ppo_torch_rl_module.PPOTorchRLModule'>, observation_space=None, action_space=None, model_config_dict=None, catalog_class=<class 'ray.rllib.algorithms.ppo.ppo_catalog.PPOCatalog'>, load_state_path=None), '_enable_rl_module_api': True, '_AlgorithmConfig__prior_exploration_config': {'type': 'StochasticSampling'}, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, '_disable_initialize_loss_from_dummy_batch': False, 'simple_optimizer': False, 'policy_map_cache': -1, 'worker_cls': -1, 'synchronize_filters': -1, 'replay_sequence_length': None, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'use_kl_loss': True, 'kl_coeff': 0.2, 'kl_target': 0.01, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'policies': {'default_policy': (None, None, None, None)}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'torch', 'num_cpus_for_driver': 1, 'num_workers': 2}, 'time_since_restore': 48.17617392539978, 'iterations_since_restore': 2, 'perf': {'cpu_util_percent': 50.59642857142857, 'ram_util_percent': 24.11428571428572}}\n",
      "{'custom_metrics': {}, 'episode_media': {}, 'info': {'learner': {'__all__': {'num_agent_steps_trained': 128.0, 'num_env_steps_trained': 4000.0, 'total_loss': 9.896153633020072}, 'default_policy': {'total_loss': 9.896153633020072, 'policy_loss': -0.01096206049232293, 'vf_loss': 9.906579797456006, 'vf_loss_unclipped': 36843.31769098148, 'vf_explained_var': 0.00024716254236347384, 'entropy': 1.7173084863213335, 'mean_kl_loss': 0.005359075896904908, 'default_optimizer_lr': 5.000000000000001e-05, 'curr_lr': 5e-05, 'curr_entropy_coeff': 0.0, 'curr_kl_coeff': 0.10000000149011612}}, 'num_env_steps_sampled': 12000, 'num_env_steps_trained': 0, 'num_agent_steps_sampled': 12000, 'num_agent_steps_trained': 0}, 'sampler_results': {'episode_reward_max': -226.0, 'episode_reward_min': -875.0, 'episode_reward_mean': -691.672131147541, 'episode_len_mean': 194.65573770491804, 'episode_media': {}, 'episodes_this_iter': 20, 'policy_reward_min': {}, 'policy_reward_max': {}, 'policy_reward_mean': {}, 'custom_metrics': {}, 'hist_stats': {'episode_reward': [-722.0, -565.0, -731.0, -758.0, -839.0, -857.0, -740.0, -587.0, -803.0, -839.0, -794.0, -785.0, -803.0, -803.0, -839.0, -749.0, -875.0, -318.0, -226.0, -839.0, -749.0, -677.0, -776.0, -722.0, -749.0, -704.0, -731.0, -668.0, -722.0, -659.0, -749.0, -614.0, -776.0, -812.0, -740.0, -749.0, -704.0, -668.0, -659.0, -731.0, -731.0, -632.0, -659.0, -803.0, -659.0, -569.0, -505.0, -578.0, -668.0, -596.0, -596.0, -641.0, -632.0, -614.0, -731.0, -623.0, -641.0, -569.0, -650.0, -704.0, -560.0], 'episode_lengths': [200, 145, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 96, 67, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 166, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 1.899026506835911, 'mean_inference_ms': 6.305269147965837, 'mean_action_processing_ms': 0.5924256206455092, 'mean_env_wait_ms': 0.3043859485630655, 'mean_env_render_ms': 0.0}, 'num_faulty_episodes': 0, 'connector_metrics': {'ObsPreprocessorConnector_ms': 0.07045894372658651, 'StateBufferConnector_ms': 0.01450171236132012, 'ViewRequirementAgentConnector_ms': 0.8244811511430584}}, 'episode_reward_max': -226.0, 'episode_reward_min': -875.0, 'episode_reward_mean': -691.672131147541, 'episode_len_mean': 194.65573770491804, 'episodes_this_iter': 20, 'policy_reward_min': {}, 'policy_reward_max': {}, 'policy_reward_mean': {}, 'hist_stats': {'episode_reward': [-722.0, -565.0, -731.0, -758.0, -839.0, -857.0, -740.0, -587.0, -803.0, -839.0, -794.0, -785.0, -803.0, -803.0, -839.0, -749.0, -875.0, -318.0, -226.0, -839.0, -749.0, -677.0, -776.0, -722.0, -749.0, -704.0, -731.0, -668.0, -722.0, -659.0, -749.0, -614.0, -776.0, -812.0, -740.0, -749.0, -704.0, -668.0, -659.0, -731.0, -731.0, -632.0, -659.0, -803.0, -659.0, -569.0, -505.0, -578.0, -668.0, -596.0, -596.0, -641.0, -632.0, -614.0, -731.0, -623.0, -641.0, -569.0, -650.0, -704.0, -560.0], 'episode_lengths': [200, 145, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 96, 67, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 166, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200, 200]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 1.899026506835911, 'mean_inference_ms': 6.305269147965837, 'mean_action_processing_ms': 0.5924256206455092, 'mean_env_wait_ms': 0.3043859485630655, 'mean_env_render_ms': 0.0}, 'num_faulty_episodes': 0, 'connector_metrics': {'ObsPreprocessorConnector_ms': 0.07045894372658651, 'StateBufferConnector_ms': 0.01450171236132012, 'ViewRequirementAgentConnector_ms': 0.8244811511430584}, 'num_healthy_workers': 2, 'num_in_flight_async_reqs': 0, 'num_remote_worker_restarts': 0, 'num_agent_steps_sampled': 12000, 'num_agent_steps_trained': 0, 'num_env_steps_sampled': 12000, 'num_env_steps_trained': 0, 'num_env_steps_sampled_this_iter': 4000, 'num_env_steps_trained_this_iter': 0, 'num_env_steps_sampled_throughput_per_sec': 199.80119221552565, 'num_env_steps_trained_throughput_per_sec': 0.0, 'timesteps_total': 12000, 'num_steps_trained_this_iter': 0, 'agent_timesteps_total': 12000, 'timers': {'training_iteration_time_ms': 22727.267, 'sample_time_ms': 16005.641, 'synch_weights_time_ms': 14.369}, 'counters': {'num_env_steps_sampled': 12000, 'num_env_steps_trained': 0, 'num_agent_steps_sampled': 12000, 'num_agent_steps_trained': 0}, 'done': False, 'episodes_total': 61, 'training_iteration': 3, 'trial_id': 'default', 'date': '2023-12-03_18-35-44', 'timestamp': 1701646544, 'time_this_iter_s': 20.030336141586304, 'time_total_s': 68.20651006698608, 'pid': 817316, 'hostname': 'udc-aw34-10c0', 'node_ip': '10.250.132.124', 'config': {'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'num_learner_workers': 0, 'num_gpus_per_learner_worker': 0, 'num_cpus_per_learner_worker': 1, 'local_gpu_idx': 0, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'env': 'Taxi-v3', 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'auto_wrap_old_gym_envs': True, 'action_mask_key': 'action_mask', '_is_atari': None, 'env_runner_cls': None, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': True, 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 5e-05, 'grad_clip': None, 'grad_clip_by': 'global_norm', 'train_batch_size': 4000, 'model': {'_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [64, 64], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'encoder_latent_dim': None, 'always_check_shapes': False, 'lstm_use_prev_action_reward': -1, '_use_default_native_models': -1}, 'optimizer': {}, 'max_requests_in_flight_per_sampler_worker': 2, '_learner_class': None, '_enable_learner_api': True, 'explore': True, 'exploration_config': {}, 'algorithm_config_overrides_per_module': {}, 'policy_map_capacity': 100, 'policy_mapping_fn': <function AlgorithmConfig.DEFAULT_POLICY_MAPPING_FN at 0x7fc583815a60>, 'policies_to_train': None, 'policy_states_are_swappable': False, 'observation_fn': None, 'count_steps_by': 'env_steps', 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'offline_sampling': False, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': None, 'off_policy_estimation_methods': {}, 'ope_split_batch_by_episode': True, 'evaluation_num_workers': 1, 'always_attach_evaluation_results': False, 'enable_async_evaluation': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'export_native_model_files': False, 'checkpoint_trainable_policies_only': False, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'max_num_worker_restarts': 1000, 'delay_between_worker_restarts_s': 60.0, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'worker_health_probe_timeout_s': 60, 'worker_restore_timeout_s': 1800, 'rl_module_spec': SingleAgentRLModuleSpec(module_class=<class 'ray.rllib.algorithms.ppo.torch.ppo_torch_rl_module.PPOTorchRLModule'>, observation_space=None, action_space=None, model_config_dict=None, catalog_class=<class 'ray.rllib.algorithms.ppo.ppo_catalog.PPOCatalog'>, load_state_path=None), '_enable_rl_module_api': True, '_AlgorithmConfig__prior_exploration_config': {'type': 'StochasticSampling'}, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, '_disable_initialize_loss_from_dummy_batch': False, 'simple_optimizer': False, 'policy_map_cache': -1, 'worker_cls': -1, 'synchronize_filters': -1, 'replay_sequence_length': None, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'use_kl_loss': True, 'kl_coeff': 0.2, 'kl_target': 0.01, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'policies': {'default_policy': (None, None, None, None)}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'torch', 'num_cpus_for_driver': 1, 'num_workers': 2}, 'time_since_restore': 68.20651006698608, 'iterations_since_restore': 3, 'perf': {'cpu_util_percent': 49.828571428571415, 'ram_util_percent': 23.975000000000005}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from ray.rllib.algorithms.ppo import PPOConfig\n",
    "\n",
    "config = (  # 1. Configure the algorithm,\n",
    "    PPOConfig()\n",
    "    .environment(\"Taxi-v3\")\n",
    "    .rollouts(num_rollout_workers=2)\n",
    "    .framework(\"torch\")\n",
    "    .training(model={\"fcnet_hiddens\": [64, 64]})\n",
    "    .evaluation(evaluation_num_workers=1)\n",
    ")\n",
    "\n",
    "algo = config.build()  # 2. build the algorithm,\n",
    "\n",
    "for _ in range(5):\n",
    "    print(algo.train())  # 3. train it,\n",
    "\n",
    "algo.evaluate()  # 4. and evaluate it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CartPole-v1**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-03 18:39:11,053\tWARNING algorithm_config.py:797 -- You have specified 1 evaluation workers, but your `evaluation_interval` is None! Therefore, evaluation will not occur automatically with each call to `Algorithm.train()`. Instead, you will have to call `Algorithm.evaluate()` manually in order to trigger an evaluation run.\n",
      "\u001b[36m(RolloutWorker pid=821411)\u001b[0m 2023-12-03 18:39:23,214\tWARNING __init__.py:10 -- PG has/have been moved to `rllib_contrib` and will no longer be maintained by the RLlib team. You can still use it/them normally inside RLlib util Ray 2.8, but from Ray 2.9 on, all `rllib_contrib` algorithms will no longer be part of the core repo, and will therefore have to be installed separately with pinned dependencies for e.g. ray[rllib] and other packages! See https://github.com/ray-project/ray/tree/master/rllib_contrib#rllib-contrib for more information on the RLlib contrib effort.\n",
      "\u001b[36m(RolloutWorker pid=821411)\u001b[0m /home/apt4c/.local/lib/python3.9/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.single_observation_space to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.single_observation_space` for environment variables or `env.get_wrapper_attr('single_observation_space')` that will search the reminding wrappers.\u001b[0m\n",
      "\u001b[36m(RolloutWorker pid=821411)\u001b[0m   logger.warn(\n",
      "\u001b[36m(RolloutWorker pid=821411)\u001b[0m /home/apt4c/.local/lib/python3.9/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.single_action_space to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.single_action_space` for environment variables or `env.get_wrapper_attr('single_action_space')` that will search the reminding wrappers.\u001b[0m\n",
      "\u001b[36m(RolloutWorker pid=821411)\u001b[0m   logger.warn(\n",
      "2023-12-03 18:39:23,350\tWARNING algorithm_config.py:797 -- You have specified 1 evaluation workers, but your `evaluation_interval` is None! Therefore, evaluation will not occur automatically with each call to `Algorithm.train()`. Instead, you will have to call `Algorithm.evaluate()` manually in order to trigger an evaluation run.\n",
      "\u001b[36m(RolloutWorker pid=821543)\u001b[0m 2023-12-03 18:39:29,505\tWARNING __init__.py:10 -- PG has/have been moved to `rllib_contrib` and will no longer be maintained by the RLlib team. You can still use it/them normally inside RLlib util Ray 2.8, but from Ray 2.9 on, all `rllib_contrib` algorithms will no longer be part of the core repo, and will therefore have to be installed separately with pinned dependencies for e.g. ray[rllib] and other packages! See https://github.com/ray-project/ray/tree/master/rllib_contrib#rllib-contrib for more information on the RLlib contrib effort.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(RolloutWorker pid=821543)\u001b[0m /home/apt4c/.local/lib/python3.9/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.single_observation_space to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.single_observation_space` for environment variables or `env.get_wrapper_attr('single_observation_space')` that will search the reminding wrappers.\u001b[0m\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(RolloutWorker pid=821543)\u001b[0m   logger.warn(\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(RolloutWorker pid=821543)\u001b[0m /home/apt4c/.local/lib/python3.9/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.single_action_space to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.single_action_space` for environment variables or `env.get_wrapper_attr('single_action_space')` that will search the reminding wrappers.\u001b[0m\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "2023-12-03 18:39:29,592\tINFO trainable.py:164 -- Trainable.setup took 18.520 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
      "2023-12-03 18:39:29,594\tWARNING util.py:62 -- Install gputil for GPU system monitoring.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'custom_metrics': {}, 'episode_media': {}, 'info': {'learner': {'__all__': {'num_agent_steps_trained': 128.0, 'num_env_steps_trained': 4000.0, 'total_loss': 8.660188125903165}, 'default_policy': {'total_loss': 8.660188125903165, 'policy_loss': -0.01705715892764964, 'vf_loss': 8.674689502858405, 'vf_loss_unclipped': 252.44721485455153, 'vf_explained_var': 0.007999553672794594, 'entropy': 0.6716243913178759, 'mean_kl_loss': 0.012779106115911439, 'default_optimizer_lr': 5.000000000000001e-05, 'curr_lr': 5e-05, 'curr_entropy_coeff': 0.0, 'curr_kl_coeff': 0.20000000298023224}}, 'num_env_steps_sampled': 4000, 'num_env_steps_trained': 0, 'num_agent_steps_sampled': 4000, 'num_agent_steps_trained': 0}, 'sampler_results': {'episode_reward_max': 83.0, 'episode_reward_min': 8.0, 'episode_reward_mean': 23.17543859649123, 'episode_len_mean': 23.17543859649123, 'episode_media': {}, 'episodes_this_iter': 171, 'policy_reward_min': {}, 'policy_reward_max': {}, 'policy_reward_mean': {}, 'custom_metrics': {}, 'hist_stats': {'episode_reward': [8.0, 26.0, 27.0, 14.0, 10.0, 23.0, 41.0, 22.0, 46.0, 11.0, 25.0, 16.0, 24.0, 15.0, 14.0, 75.0, 18.0, 10.0, 18.0, 11.0, 83.0, 34.0, 22.0, 25.0, 29.0, 12.0, 14.0, 22.0, 33.0, 21.0, 13.0, 13.0, 14.0, 35.0, 17.0, 12.0, 34.0, 47.0, 12.0, 14.0, 21.0, 20.0, 27.0, 15.0, 12.0, 21.0, 14.0, 23.0, 24.0, 24.0, 12.0, 39.0, 31.0, 11.0, 11.0, 24.0, 52.0, 11.0, 13.0, 39.0, 14.0, 23.0, 11.0, 23.0, 18.0, 25.0, 14.0, 23.0, 26.0, 22.0, 15.0, 42.0, 13.0, 23.0, 18.0, 21.0, 42.0, 16.0, 26.0, 14.0, 14.0, 23.0, 11.0, 33.0, 20.0, 14.0, 17.0, 19.0, 10.0, 32.0, 22.0, 27.0, 17.0, 24.0, 24.0, 20.0, 14.0, 15.0, 48.0, 13.0, 34.0, 21.0, 29.0, 27.0, 12.0, 45.0, 11.0, 70.0, 13.0, 26.0, 49.0, 13.0, 29.0, 12.0, 31.0, 37.0, 23.0, 11.0, 11.0, 15.0, 12.0, 22.0, 15.0, 34.0, 20.0, 21.0, 15.0, 23.0, 36.0, 16.0, 16.0, 39.0, 35.0, 10.0, 20.0, 16.0, 13.0, 19.0, 13.0, 21.0, 55.0, 14.0, 15.0, 26.0, 16.0, 18.0, 38.0, 36.0, 30.0, 33.0, 12.0, 22.0, 26.0, 9.0, 20.0, 18.0, 21.0, 26.0, 27.0, 54.0, 14.0, 63.0, 12.0, 15.0, 9.0, 17.0, 32.0, 18.0, 27.0, 20.0, 35.0], 'episode_lengths': [8, 26, 27, 14, 10, 23, 41, 22, 46, 11, 25, 16, 24, 15, 14, 75, 18, 10, 18, 11, 83, 34, 22, 25, 29, 12, 14, 22, 33, 21, 13, 13, 14, 35, 17, 12, 34, 47, 12, 14, 21, 20, 27, 15, 12, 21, 14, 23, 24, 24, 12, 39, 31, 11, 11, 24, 52, 11, 13, 39, 14, 23, 11, 23, 18, 25, 14, 23, 26, 22, 15, 42, 13, 23, 18, 21, 42, 16, 26, 14, 14, 23, 11, 33, 20, 14, 17, 19, 10, 32, 22, 27, 17, 24, 24, 20, 14, 15, 48, 13, 34, 21, 29, 27, 12, 45, 11, 70, 13, 26, 49, 13, 29, 12, 31, 37, 23, 11, 11, 15, 12, 22, 15, 34, 20, 21, 15, 23, 36, 16, 16, 39, 35, 10, 20, 16, 13, 19, 13, 21, 55, 14, 15, 26, 16, 18, 38, 36, 30, 33, 12, 22, 26, 9, 20, 18, 21, 26, 27, 54, 14, 63, 12, 15, 9, 17, 32, 18, 27, 20, 35]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 1.481120744446315, 'mean_inference_ms': 4.243212919094954, 'mean_action_processing_ms': 0.28781684886458414, 'mean_env_wait_ms': 0.1530506277035158, 'mean_env_render_ms': 0.0}, 'num_faulty_episodes': 0, 'connector_metrics': {'ObsPreprocessorConnector_ms': 0.013755683080959504, 'StateBufferConnector_ms': 0.010173316122728017, 'ViewRequirementAgentConnector_ms': 0.7638103703231761}}, 'episode_reward_max': 83.0, 'episode_reward_min': 8.0, 'episode_reward_mean': 23.17543859649123, 'episode_len_mean': 23.17543859649123, 'episodes_this_iter': 171, 'policy_reward_min': {}, 'policy_reward_max': {}, 'policy_reward_mean': {}, 'hist_stats': {'episode_reward': [8.0, 26.0, 27.0, 14.0, 10.0, 23.0, 41.0, 22.0, 46.0, 11.0, 25.0, 16.0, 24.0, 15.0, 14.0, 75.0, 18.0, 10.0, 18.0, 11.0, 83.0, 34.0, 22.0, 25.0, 29.0, 12.0, 14.0, 22.0, 33.0, 21.0, 13.0, 13.0, 14.0, 35.0, 17.0, 12.0, 34.0, 47.0, 12.0, 14.0, 21.0, 20.0, 27.0, 15.0, 12.0, 21.0, 14.0, 23.0, 24.0, 24.0, 12.0, 39.0, 31.0, 11.0, 11.0, 24.0, 52.0, 11.0, 13.0, 39.0, 14.0, 23.0, 11.0, 23.0, 18.0, 25.0, 14.0, 23.0, 26.0, 22.0, 15.0, 42.0, 13.0, 23.0, 18.0, 21.0, 42.0, 16.0, 26.0, 14.0, 14.0, 23.0, 11.0, 33.0, 20.0, 14.0, 17.0, 19.0, 10.0, 32.0, 22.0, 27.0, 17.0, 24.0, 24.0, 20.0, 14.0, 15.0, 48.0, 13.0, 34.0, 21.0, 29.0, 27.0, 12.0, 45.0, 11.0, 70.0, 13.0, 26.0, 49.0, 13.0, 29.0, 12.0, 31.0, 37.0, 23.0, 11.0, 11.0, 15.0, 12.0, 22.0, 15.0, 34.0, 20.0, 21.0, 15.0, 23.0, 36.0, 16.0, 16.0, 39.0, 35.0, 10.0, 20.0, 16.0, 13.0, 19.0, 13.0, 21.0, 55.0, 14.0, 15.0, 26.0, 16.0, 18.0, 38.0, 36.0, 30.0, 33.0, 12.0, 22.0, 26.0, 9.0, 20.0, 18.0, 21.0, 26.0, 27.0, 54.0, 14.0, 63.0, 12.0, 15.0, 9.0, 17.0, 32.0, 18.0, 27.0, 20.0, 35.0], 'episode_lengths': [8, 26, 27, 14, 10, 23, 41, 22, 46, 11, 25, 16, 24, 15, 14, 75, 18, 10, 18, 11, 83, 34, 22, 25, 29, 12, 14, 22, 33, 21, 13, 13, 14, 35, 17, 12, 34, 47, 12, 14, 21, 20, 27, 15, 12, 21, 14, 23, 24, 24, 12, 39, 31, 11, 11, 24, 52, 11, 13, 39, 14, 23, 11, 23, 18, 25, 14, 23, 26, 22, 15, 42, 13, 23, 18, 21, 42, 16, 26, 14, 14, 23, 11, 33, 20, 14, 17, 19, 10, 32, 22, 27, 17, 24, 24, 20, 14, 15, 48, 13, 34, 21, 29, 27, 12, 45, 11, 70, 13, 26, 49, 13, 29, 12, 31, 37, 23, 11, 11, 15, 12, 22, 15, 34, 20, 21, 15, 23, 36, 16, 16, 39, 35, 10, 20, 16, 13, 19, 13, 21, 55, 14, 15, 26, 16, 18, 38, 36, 30, 33, 12, 22, 26, 9, 20, 18, 21, 26, 27, 54, 14, 63, 12, 15, 9, 17, 32, 18, 27, 20, 35]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 1.481120744446315, 'mean_inference_ms': 4.243212919094954, 'mean_action_processing_ms': 0.28781684886458414, 'mean_env_wait_ms': 0.1530506277035158, 'mean_env_render_ms': 0.0}, 'num_faulty_episodes': 0, 'connector_metrics': {'ObsPreprocessorConnector_ms': 0.013755683080959504, 'StateBufferConnector_ms': 0.010173316122728017, 'ViewRequirementAgentConnector_ms': 0.7638103703231761}, 'num_healthy_workers': 2, 'num_in_flight_async_reqs': 0, 'num_remote_worker_restarts': 0, 'num_agent_steps_sampled': 4000, 'num_agent_steps_trained': 0, 'num_env_steps_sampled': 4000, 'num_env_steps_trained': 0, 'num_env_steps_sampled_this_iter': 4000, 'num_env_steps_trained_this_iter': 0, 'num_env_steps_sampled_throughput_per_sec': 224.52342048094286, 'num_env_steps_trained_throughput_per_sec': 0.0, 'timesteps_total': 4000, 'num_steps_trained_this_iter': 0, 'agent_timesteps_total': 4000, 'timers': {'training_iteration_time_ms': 17815.479, 'sample_time_ms': 12584.672, 'synch_weights_time_ms': 6.277}, 'counters': {'num_env_steps_sampled': 4000, 'num_env_steps_trained': 0, 'num_agent_steps_sampled': 4000, 'num_agent_steps_trained': 0}, 'done': False, 'episodes_total': 171, 'training_iteration': 1, 'trial_id': 'default', 'date': '2023-12-03_18-39-47', 'timestamp': 1701646787, 'time_this_iter_s': 17.829705953598022, 'time_total_s': 17.829705953598022, 'pid': 817316, 'hostname': 'udc-aw34-10c0', 'node_ip': '10.250.132.124', 'config': {'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'num_learner_workers': 0, 'num_gpus_per_learner_worker': 0, 'num_cpus_per_learner_worker': 1, 'local_gpu_idx': 0, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'env': 'CartPole-v1', 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'auto_wrap_old_gym_envs': True, 'action_mask_key': 'action_mask', '_is_atari': None, 'env_runner_cls': None, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': True, 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 5e-05, 'grad_clip': None, 'grad_clip_by': 'global_norm', 'train_batch_size': 4000, 'model': {'_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [64, 64], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'encoder_latent_dim': None, 'always_check_shapes': False, 'lstm_use_prev_action_reward': -1, '_use_default_native_models': -1}, 'optimizer': {}, 'max_requests_in_flight_per_sampler_worker': 2, '_learner_class': None, '_enable_learner_api': True, 'explore': True, 'exploration_config': {}, 'algorithm_config_overrides_per_module': {}, 'policy_map_capacity': 100, 'policy_mapping_fn': <function AlgorithmConfig.DEFAULT_POLICY_MAPPING_FN at 0x7fc583815a60>, 'policies_to_train': None, 'policy_states_are_swappable': False, 'observation_fn': None, 'count_steps_by': 'env_steps', 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'offline_sampling': False, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': None, 'off_policy_estimation_methods': {}, 'ope_split_batch_by_episode': True, 'evaluation_num_workers': 1, 'always_attach_evaluation_results': False, 'enable_async_evaluation': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'export_native_model_files': False, 'checkpoint_trainable_policies_only': False, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'max_num_worker_restarts': 1000, 'delay_between_worker_restarts_s': 60.0, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'worker_health_probe_timeout_s': 60, 'worker_restore_timeout_s': 1800, 'rl_module_spec': SingleAgentRLModuleSpec(module_class=<class 'ray.rllib.algorithms.ppo.torch.ppo_torch_rl_module.PPOTorchRLModule'>, observation_space=None, action_space=None, model_config_dict=None, catalog_class=<class 'ray.rllib.algorithms.ppo.ppo_catalog.PPOCatalog'>, load_state_path=None), '_enable_rl_module_api': True, '_AlgorithmConfig__prior_exploration_config': {'type': 'StochasticSampling'}, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, '_disable_initialize_loss_from_dummy_batch': False, 'simple_optimizer': False, 'policy_map_cache': -1, 'worker_cls': -1, 'synchronize_filters': -1, 'replay_sequence_length': None, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'use_kl_loss': True, 'kl_coeff': 0.2, 'kl_target': 0.01, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'policies': {'default_policy': (None, None, None, None)}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'torch', 'num_cpus_for_driver': 1, 'num_workers': 2}, 'time_since_restore': 17.829705953598022, 'iterations_since_restore': 1, 'perf': {'cpu_util_percent': 45.74, 'ram_util_percent': 23.808000000000003}}\n",
      "{'custom_metrics': {}, 'episode_media': {}, 'info': {'learner': {'__all__': {'num_agent_steps_trained': 128.0, 'num_env_steps_trained': 4000.0, 'total_loss': 8.725296398724065}, 'default_policy': {'total_loss': 8.725296398724065, 'policy_loss': -0.01952592186482826, 'vf_loss': 8.742686334195168, 'vf_loss_unclipped': 487.3209982101343, 'vf_explained_var': 0.010654081540829592, 'entropy': 0.6435405936703753, 'mean_kl_loss': 0.010680055589274125, 'default_optimizer_lr': 5.000000000000001e-05, 'curr_lr': 5e-05, 'curr_entropy_coeff': 0.0, 'curr_kl_coeff': 0.20000000298023224}}, 'num_env_steps_sampled': 8000, 'num_env_steps_trained': 0, 'num_agent_steps_sampled': 8000, 'num_agent_steps_trained': 0}, 'sampler_results': {'episode_reward_max': 155.0, 'episode_reward_min': 11.0, 'episode_reward_mean': 36.0, 'episode_len_mean': 36.0, 'episode_media': {}, 'episodes_this_iter': 111, 'policy_reward_min': {}, 'policy_reward_max': {}, 'policy_reward_mean': {}, 'custom_metrics': {}, 'hist_stats': {'episode_reward': [101.0, 31.0, 42.0, 18.0, 80.0, 14.0, 41.0, 31.0, 33.0, 89.0, 64.0, 51.0, 33.0, 51.0, 20.0, 11.0, 18.0, 61.0, 44.0, 62.0, 12.0, 44.0, 11.0, 22.0, 22.0, 49.0, 20.0, 62.0, 35.0, 30.0, 31.0, 35.0, 12.0, 51.0, 14.0, 13.0, 23.0, 46.0, 30.0, 43.0, 36.0, 19.0, 155.0, 43.0, 33.0, 22.0, 15.0, 34.0, 79.0, 29.0, 19.0, 102.0, 39.0, 26.0, 36.0, 43.0, 46.0, 28.0, 39.0, 49.0, 42.0, 17.0, 18.0, 29.0, 73.0, 25.0, 52.0, 24.0, 18.0, 16.0, 26.0, 27.0, 65.0, 16.0, 57.0, 35.0, 17.0, 32.0, 15.0, 46.0, 40.0, 22.0, 20.0, 71.0, 33.0, 20.0, 32.0, 11.0, 29.0, 17.0, 61.0, 15.0, 57.0, 35.0, 31.0, 38.0, 59.0, 14.0, 18.0, 34.0, 19.0, 14.0, 71.0, 27.0, 29.0, 19.0, 14.0, 16.0, 28.0, 33.0, 27.0], 'episode_lengths': [101, 31, 42, 18, 80, 14, 41, 31, 33, 89, 64, 51, 33, 51, 20, 11, 18, 61, 44, 62, 12, 44, 11, 22, 22, 49, 20, 62, 35, 30, 31, 35, 12, 51, 14, 13, 23, 46, 30, 43, 36, 19, 155, 43, 33, 22, 15, 34, 79, 29, 19, 102, 39, 26, 36, 43, 46, 28, 39, 49, 42, 17, 18, 29, 73, 25, 52, 24, 18, 16, 26, 27, 65, 16, 57, 35, 17, 32, 15, 46, 40, 22, 20, 71, 33, 20, 32, 11, 29, 17, 61, 15, 57, 35, 31, 38, 59, 14, 18, 34, 19, 14, 71, 27, 29, 19, 14, 16, 28, 33, 27]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 1.4137968049452367, 'mean_inference_ms': 4.186177019068381, 'mean_action_processing_ms': 0.30601994249506753, 'mean_env_wait_ms': 0.16381899094901609, 'mean_env_render_ms': 0.0}, 'num_faulty_episodes': 0, 'connector_metrics': {'ObsPreprocessorConnector_ms': 0.013206241367099521, 'StateBufferConnector_ms': 0.008607769871617222, 'ViewRequirementAgentConnector_ms': 0.69084425230284}}, 'episode_reward_max': 155.0, 'episode_reward_min': 11.0, 'episode_reward_mean': 36.0, 'episode_len_mean': 36.0, 'episodes_this_iter': 111, 'policy_reward_min': {}, 'policy_reward_max': {}, 'policy_reward_mean': {}, 'hist_stats': {'episode_reward': [101.0, 31.0, 42.0, 18.0, 80.0, 14.0, 41.0, 31.0, 33.0, 89.0, 64.0, 51.0, 33.0, 51.0, 20.0, 11.0, 18.0, 61.0, 44.0, 62.0, 12.0, 44.0, 11.0, 22.0, 22.0, 49.0, 20.0, 62.0, 35.0, 30.0, 31.0, 35.0, 12.0, 51.0, 14.0, 13.0, 23.0, 46.0, 30.0, 43.0, 36.0, 19.0, 155.0, 43.0, 33.0, 22.0, 15.0, 34.0, 79.0, 29.0, 19.0, 102.0, 39.0, 26.0, 36.0, 43.0, 46.0, 28.0, 39.0, 49.0, 42.0, 17.0, 18.0, 29.0, 73.0, 25.0, 52.0, 24.0, 18.0, 16.0, 26.0, 27.0, 65.0, 16.0, 57.0, 35.0, 17.0, 32.0, 15.0, 46.0, 40.0, 22.0, 20.0, 71.0, 33.0, 20.0, 32.0, 11.0, 29.0, 17.0, 61.0, 15.0, 57.0, 35.0, 31.0, 38.0, 59.0, 14.0, 18.0, 34.0, 19.0, 14.0, 71.0, 27.0, 29.0, 19.0, 14.0, 16.0, 28.0, 33.0, 27.0], 'episode_lengths': [101, 31, 42, 18, 80, 14, 41, 31, 33, 89, 64, 51, 33, 51, 20, 11, 18, 61, 44, 62, 12, 44, 11, 22, 22, 49, 20, 62, 35, 30, 31, 35, 12, 51, 14, 13, 23, 46, 30, 43, 36, 19, 155, 43, 33, 22, 15, 34, 79, 29, 19, 102, 39, 26, 36, 43, 46, 28, 39, 49, 42, 17, 18, 29, 73, 25, 52, 24, 18, 16, 26, 27, 65, 16, 57, 35, 17, 32, 15, 46, 40, 22, 20, 71, 33, 20, 32, 11, 29, 17, 61, 15, 57, 35, 31, 38, 59, 14, 18, 34, 19, 14, 71, 27, 29, 19, 14, 16, 28, 33, 27]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 1.4137968049452367, 'mean_inference_ms': 4.186177019068381, 'mean_action_processing_ms': 0.30601994249506753, 'mean_env_wait_ms': 0.16381899094901609, 'mean_env_render_ms': 0.0}, 'num_faulty_episodes': 0, 'connector_metrics': {'ObsPreprocessorConnector_ms': 0.013206241367099521, 'StateBufferConnector_ms': 0.008607769871617222, 'ViewRequirementAgentConnector_ms': 0.69084425230284}, 'num_healthy_workers': 2, 'num_in_flight_async_reqs': 0, 'num_remote_worker_restarts': 0, 'num_agent_steps_sampled': 8000, 'num_agent_steps_trained': 0, 'num_env_steps_sampled': 8000, 'num_env_steps_trained': 0, 'num_env_steps_sampled_this_iter': 4000, 'num_env_steps_trained_this_iter': 0, 'num_env_steps_sampled_throughput_per_sec': 229.4379092489775, 'num_env_steps_trained_throughput_per_sec': 0.0, 'timesteps_total': 8000, 'num_steps_trained_this_iter': 0, 'agent_timesteps_total': 8000, 'timers': {'training_iteration_time_ms': 17624.681, 'sample_time_ms': 12343.93, 'synch_weights_time_ms': 7.19}, 'counters': {'num_env_steps_sampled': 8000, 'num_env_steps_trained': 0, 'num_agent_steps_sampled': 8000, 'num_agent_steps_trained': 0}, 'done': False, 'episodes_total': 282, 'training_iteration': 2, 'trial_id': 'default', 'date': '2023-12-03_18-40-04', 'timestamp': 1701646804, 'time_this_iter_s': 17.443274974822998, 'time_total_s': 35.27298092842102, 'pid': 817316, 'hostname': 'udc-aw34-10c0', 'node_ip': '10.250.132.124', 'config': {'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'num_learner_workers': 0, 'num_gpus_per_learner_worker': 0, 'num_cpus_per_learner_worker': 1, 'local_gpu_idx': 0, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'env': 'CartPole-v1', 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'auto_wrap_old_gym_envs': True, 'action_mask_key': 'action_mask', '_is_atari': None, 'env_runner_cls': None, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': True, 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 5e-05, 'grad_clip': None, 'grad_clip_by': 'global_norm', 'train_batch_size': 4000, 'model': {'_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [64, 64], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'encoder_latent_dim': None, 'always_check_shapes': False, 'lstm_use_prev_action_reward': -1, '_use_default_native_models': -1}, 'optimizer': {}, 'max_requests_in_flight_per_sampler_worker': 2, '_learner_class': None, '_enable_learner_api': True, 'explore': True, 'exploration_config': {}, 'algorithm_config_overrides_per_module': {}, 'policy_map_capacity': 100, 'policy_mapping_fn': <function AlgorithmConfig.DEFAULT_POLICY_MAPPING_FN at 0x7fc583815a60>, 'policies_to_train': None, 'policy_states_are_swappable': False, 'observation_fn': None, 'count_steps_by': 'env_steps', 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'offline_sampling': False, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': None, 'off_policy_estimation_methods': {}, 'ope_split_batch_by_episode': True, 'evaluation_num_workers': 1, 'always_attach_evaluation_results': False, 'enable_async_evaluation': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'export_native_model_files': False, 'checkpoint_trainable_policies_only': False, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'max_num_worker_restarts': 1000, 'delay_between_worker_restarts_s': 60.0, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'worker_health_probe_timeout_s': 60, 'worker_restore_timeout_s': 1800, 'rl_module_spec': SingleAgentRLModuleSpec(module_class=<class 'ray.rllib.algorithms.ppo.torch.ppo_torch_rl_module.PPOTorchRLModule'>, observation_space=None, action_space=None, model_config_dict=None, catalog_class=<class 'ray.rllib.algorithms.ppo.ppo_catalog.PPOCatalog'>, load_state_path=None), '_enable_rl_module_api': True, '_AlgorithmConfig__prior_exploration_config': {'type': 'StochasticSampling'}, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, '_disable_initialize_loss_from_dummy_batch': False, 'simple_optimizer': False, 'policy_map_cache': -1, 'worker_cls': -1, 'synchronize_filters': -1, 'replay_sequence_length': None, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'use_kl_loss': True, 'kl_coeff': 0.2, 'kl_target': 0.01, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'policies': {'default_policy': (None, None, None, None)}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'torch', 'num_cpus_for_driver': 1, 'num_workers': 2}, 'time_since_restore': 35.27298092842102, 'iterations_since_restore': 2, 'perf': {'cpu_util_percent': 44.22, 'ram_util_percent': 23.8}}\n",
      "{'custom_metrics': {}, 'episode_media': {}, 'info': {'learner': {'__all__': {'num_agent_steps_trained': 128.0, 'num_env_steps_trained': 4000.0, 'total_loss': 9.107311185743255}, 'default_policy': {'total_loss': 9.107311185743255, 'policy_loss': -0.013695058371645332, 'vf_loss': 9.119270313523216, 'vf_loss_unclipped': 874.3651734781164, 'vf_explained_var': 0.008778691736619864, 'entropy': 0.6120999447826637, 'mean_kl_loss': 0.008679603598312942, 'default_optimizer_lr': 5.000000000000001e-05, 'curr_lr': 5e-05, 'curr_entropy_coeff': 0.0, 'curr_kl_coeff': 0.20000000298023224}}, 'num_env_steps_sampled': 12000, 'num_env_steps_trained': 0, 'num_agent_steps_sampled': 12000, 'num_agent_steps_trained': 0}, 'sampler_results': {'episode_reward_max': 158.0, 'episode_reward_min': 12.0, 'episode_reward_mean': 45.63, 'episode_len_mean': 45.63, 'episode_media': {}, 'episodes_this_iter': 80, 'policy_reward_min': {}, 'policy_reward_max': {}, 'policy_reward_mean': {}, 'custom_metrics': {}, 'hist_stats': {'episode_reward': [15.0, 57.0, 35.0, 31.0, 38.0, 59.0, 14.0, 18.0, 34.0, 19.0, 14.0, 71.0, 27.0, 29.0, 19.0, 14.0, 16.0, 28.0, 33.0, 27.0, 37.0, 21.0, 32.0, 56.0, 41.0, 137.0, 151.0, 104.0, 62.0, 73.0, 50.0, 30.0, 44.0, 47.0, 37.0, 52.0, 30.0, 18.0, 41.0, 135.0, 25.0, 158.0, 69.0, 79.0, 59.0, 60.0, 22.0, 17.0, 37.0, 24.0, 21.0, 41.0, 58.0, 36.0, 59.0, 20.0, 33.0, 35.0, 60.0, 18.0, 103.0, 12.0, 40.0, 43.0, 27.0, 47.0, 27.0, 52.0, 15.0, 35.0, 44.0, 74.0, 22.0, 52.0, 30.0, 78.0, 25.0, 18.0, 53.0, 27.0, 55.0, 52.0, 50.0, 66.0, 19.0, 46.0, 35.0, 61.0, 149.0, 65.0, 32.0, 45.0, 65.0, 37.0, 34.0, 30.0, 94.0, 18.0, 24.0, 35.0], 'episode_lengths': [15, 57, 35, 31, 38, 59, 14, 18, 34, 19, 14, 71, 27, 29, 19, 14, 16, 28, 33, 27, 37, 21, 32, 56, 41, 137, 151, 104, 62, 73, 50, 30, 44, 47, 37, 52, 30, 18, 41, 135, 25, 158, 69, 79, 59, 60, 22, 17, 37, 24, 21, 41, 58, 36, 59, 20, 33, 35, 60, 18, 103, 12, 40, 43, 27, 47, 27, 52, 15, 35, 44, 74, 22, 52, 30, 78, 25, 18, 53, 27, 55, 52, 50, 66, 19, 46, 35, 61, 149, 65, 32, 45, 65, 37, 34, 30, 94, 18, 24, 35]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 1.393235047086967, 'mean_inference_ms': 4.171755250101437, 'mean_action_processing_ms': 0.32010302890385356, 'mean_env_wait_ms': 0.1634431022464702, 'mean_env_render_ms': 0.0}, 'num_faulty_episodes': 0, 'connector_metrics': {'ObsPreprocessorConnector_ms': 0.018295764923095703, 'StateBufferConnector_ms': 0.018635272979736328, 'ViewRequirementAgentConnector_ms': 0.7700626850128174}}, 'episode_reward_max': 158.0, 'episode_reward_min': 12.0, 'episode_reward_mean': 45.63, 'episode_len_mean': 45.63, 'episodes_this_iter': 80, 'policy_reward_min': {}, 'policy_reward_max': {}, 'policy_reward_mean': {}, 'hist_stats': {'episode_reward': [15.0, 57.0, 35.0, 31.0, 38.0, 59.0, 14.0, 18.0, 34.0, 19.0, 14.0, 71.0, 27.0, 29.0, 19.0, 14.0, 16.0, 28.0, 33.0, 27.0, 37.0, 21.0, 32.0, 56.0, 41.0, 137.0, 151.0, 104.0, 62.0, 73.0, 50.0, 30.0, 44.0, 47.0, 37.0, 52.0, 30.0, 18.0, 41.0, 135.0, 25.0, 158.0, 69.0, 79.0, 59.0, 60.0, 22.0, 17.0, 37.0, 24.0, 21.0, 41.0, 58.0, 36.0, 59.0, 20.0, 33.0, 35.0, 60.0, 18.0, 103.0, 12.0, 40.0, 43.0, 27.0, 47.0, 27.0, 52.0, 15.0, 35.0, 44.0, 74.0, 22.0, 52.0, 30.0, 78.0, 25.0, 18.0, 53.0, 27.0, 55.0, 52.0, 50.0, 66.0, 19.0, 46.0, 35.0, 61.0, 149.0, 65.0, 32.0, 45.0, 65.0, 37.0, 34.0, 30.0, 94.0, 18.0, 24.0, 35.0], 'episode_lengths': [15, 57, 35, 31, 38, 59, 14, 18, 34, 19, 14, 71, 27, 29, 19, 14, 16, 28, 33, 27, 37, 21, 32, 56, 41, 137, 151, 104, 62, 73, 50, 30, 44, 47, 37, 52, 30, 18, 41, 135, 25, 158, 69, 79, 59, 60, 22, 17, 37, 24, 21, 41, 58, 36, 59, 20, 33, 35, 60, 18, 103, 12, 40, 43, 27, 47, 27, 52, 15, 35, 44, 74, 22, 52, 30, 78, 25, 18, 53, 27, 55, 52, 50, 66, 19, 46, 35, 61, 149, 65, 32, 45, 65, 37, 34, 30, 94, 18, 24, 35]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 1.393235047086967, 'mean_inference_ms': 4.171755250101437, 'mean_action_processing_ms': 0.32010302890385356, 'mean_env_wait_ms': 0.1634431022464702, 'mean_env_render_ms': 0.0}, 'num_faulty_episodes': 0, 'connector_metrics': {'ObsPreprocessorConnector_ms': 0.018295764923095703, 'StateBufferConnector_ms': 0.018635272979736328, 'ViewRequirementAgentConnector_ms': 0.7700626850128174}, 'num_healthy_workers': 2, 'num_in_flight_async_reqs': 0, 'num_remote_worker_restarts': 0, 'num_agent_steps_sampled': 12000, 'num_agent_steps_trained': 0, 'num_env_steps_sampled': 12000, 'num_env_steps_trained': 0, 'num_env_steps_sampled_this_iter': 4000, 'num_env_steps_trained_this_iter': 0, 'num_env_steps_sampled_throughput_per_sec': 229.53807988676067, 'num_env_steps_trained_throughput_per_sec': 0.0, 'timesteps_total': 12000, 'num_steps_trained_this_iter': 0, 'agent_timesteps_total': 12000, 'timers': {'training_iteration_time_ms': 17558.527, 'sample_time_ms': 12282.475, 'synch_weights_time_ms': 11.555}, 'counters': {'num_env_steps_sampled': 12000, 'num_env_steps_trained': 0, 'num_agent_steps_sampled': 12000, 'num_agent_steps_trained': 0}, 'done': False, 'episodes_total': 362, 'training_iteration': 3, 'trial_id': 'default', 'date': '2023-12-03_18-40-22', 'timestamp': 1701646822, 'time_this_iter_s': 17.432199716567993, 'time_total_s': 52.705180644989014, 'pid': 817316, 'hostname': 'udc-aw34-10c0', 'node_ip': '10.250.132.124', 'config': {'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'num_learner_workers': 0, 'num_gpus_per_learner_worker': 0, 'num_cpus_per_learner_worker': 1, 'local_gpu_idx': 0, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'env': 'CartPole-v1', 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'auto_wrap_old_gym_envs': True, 'action_mask_key': 'action_mask', '_is_atari': None, 'env_runner_cls': None, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': True, 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 5e-05, 'grad_clip': None, 'grad_clip_by': 'global_norm', 'train_batch_size': 4000, 'model': {'_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [64, 64], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'encoder_latent_dim': None, 'always_check_shapes': False, 'lstm_use_prev_action_reward': -1, '_use_default_native_models': -1}, 'optimizer': {}, 'max_requests_in_flight_per_sampler_worker': 2, '_learner_class': None, '_enable_learner_api': True, 'explore': True, 'exploration_config': {}, 'algorithm_config_overrides_per_module': {}, 'policy_map_capacity': 100, 'policy_mapping_fn': <function AlgorithmConfig.DEFAULT_POLICY_MAPPING_FN at 0x7fc583815a60>, 'policies_to_train': None, 'policy_states_are_swappable': False, 'observation_fn': None, 'count_steps_by': 'env_steps', 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'offline_sampling': False, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': None, 'off_policy_estimation_methods': {}, 'ope_split_batch_by_episode': True, 'evaluation_num_workers': 1, 'always_attach_evaluation_results': False, 'enable_async_evaluation': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'export_native_model_files': False, 'checkpoint_trainable_policies_only': False, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'max_num_worker_restarts': 1000, 'delay_between_worker_restarts_s': 60.0, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'worker_health_probe_timeout_s': 60, 'worker_restore_timeout_s': 1800, 'rl_module_spec': SingleAgentRLModuleSpec(module_class=<class 'ray.rllib.algorithms.ppo.torch.ppo_torch_rl_module.PPOTorchRLModule'>, observation_space=None, action_space=None, model_config_dict=None, catalog_class=<class 'ray.rllib.algorithms.ppo.ppo_catalog.PPOCatalog'>, load_state_path=None), '_enable_rl_module_api': True, '_AlgorithmConfig__prior_exploration_config': {'type': 'StochasticSampling'}, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, '_disable_initialize_loss_from_dummy_batch': False, 'simple_optimizer': False, 'policy_map_cache': -1, 'worker_cls': -1, 'synchronize_filters': -1, 'replay_sequence_length': None, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'use_kl_loss': True, 'kl_coeff': 0.2, 'kl_target': 0.01, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'policies': {'default_policy': (None, None, None, None)}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'torch', 'num_cpus_for_driver': 1, 'num_workers': 2}, 'time_since_restore': 52.705180644989014, 'iterations_since_restore': 3, 'perf': {'cpu_util_percent': 47.5875, 'ram_util_percent': 23.8}}\n",
      "{'custom_metrics': {}, 'episode_media': {}, 'info': {'learner': {'__all__': {'num_agent_steps_trained': 128.0, 'num_env_steps_trained': 4000.0, 'total_loss': 9.503488282404984}, 'default_policy': {'total_loss': 9.503488282404984, 'policy_loss': -0.012350725911573561, 'vf_loss': 9.514447373876186, 'vf_loss_unclipped': 1745.8381468034756, 'vf_explained_var': 0.0104828987167334, 'entropy': 0.5939271188875251, 'mean_kl_loss': 0.006958009533303707, 'default_optimizer_lr': 5.000000000000001e-05, 'curr_lr': 5e-05, 'curr_entropy_coeff': 0.0, 'curr_kl_coeff': 0.20000000298023224}}, 'num_env_steps_sampled': 16000, 'num_env_steps_trained': 0, 'num_agent_steps_sampled': 16000, 'num_agent_steps_trained': 0}, 'sampler_results': {'episode_reward_max': 210.0, 'episode_reward_min': 12.0, 'episode_reward_mean': 65.45, 'episode_len_mean': 65.45, 'episode_media': {}, 'episodes_this_iter': 43, 'policy_reward_min': {}, 'policy_reward_max': {}, 'policy_reward_mean': {}, 'custom_metrics': {}, 'hist_stats': {'episode_reward': [79.0, 59.0, 60.0, 22.0, 17.0, 37.0, 24.0, 21.0, 41.0, 58.0, 36.0, 59.0, 20.0, 33.0, 35.0, 60.0, 18.0, 103.0, 12.0, 40.0, 43.0, 27.0, 47.0, 27.0, 52.0, 15.0, 35.0, 44.0, 74.0, 22.0, 52.0, 30.0, 78.0, 25.0, 18.0, 53.0, 27.0, 55.0, 52.0, 50.0, 66.0, 19.0, 46.0, 35.0, 61.0, 149.0, 65.0, 32.0, 45.0, 65.0, 37.0, 34.0, 30.0, 94.0, 18.0, 24.0, 35.0, 160.0, 63.0, 25.0, 14.0, 99.0, 95.0, 36.0, 110.0, 174.0, 93.0, 68.0, 97.0, 54.0, 210.0, 42.0, 121.0, 30.0, 101.0, 91.0, 194.0, 35.0, 74.0, 64.0, 142.0, 83.0, 86.0, 181.0, 116.0, 182.0, 47.0, 42.0, 128.0, 46.0, 80.0, 144.0, 58.0, 62.0, 30.0, 156.0, 63.0, 101.0, 124.0, 109.0], 'episode_lengths': [79, 59, 60, 22, 17, 37, 24, 21, 41, 58, 36, 59, 20, 33, 35, 60, 18, 103, 12, 40, 43, 27, 47, 27, 52, 15, 35, 44, 74, 22, 52, 30, 78, 25, 18, 53, 27, 55, 52, 50, 66, 19, 46, 35, 61, 149, 65, 32, 45, 65, 37, 34, 30, 94, 18, 24, 35, 160, 63, 25, 14, 99, 95, 36, 110, 174, 93, 68, 97, 54, 210, 42, 121, 30, 101, 91, 194, 35, 74, 64, 142, 83, 86, 181, 116, 182, 47, 42, 128, 46, 80, 144, 58, 62, 30, 156, 63, 101, 124, 109]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 1.4033105381531084, 'mean_inference_ms': 4.212891225602756, 'mean_action_processing_ms': 0.3300250700890645, 'mean_env_wait_ms': 0.1638622544416405, 'mean_env_render_ms': 0.0}, 'num_faulty_episodes': 0, 'connector_metrics': {'ObsPreprocessorConnector_ms': 0.023898839950561523, 'StateBufferConnector_ms': 0.007944822311401367, 'ViewRequirementAgentConnector_ms': 0.8427889347076416}}, 'episode_reward_max': 210.0, 'episode_reward_min': 12.0, 'episode_reward_mean': 65.45, 'episode_len_mean': 65.45, 'episodes_this_iter': 43, 'policy_reward_min': {}, 'policy_reward_max': {}, 'policy_reward_mean': {}, 'hist_stats': {'episode_reward': [79.0, 59.0, 60.0, 22.0, 17.0, 37.0, 24.0, 21.0, 41.0, 58.0, 36.0, 59.0, 20.0, 33.0, 35.0, 60.0, 18.0, 103.0, 12.0, 40.0, 43.0, 27.0, 47.0, 27.0, 52.0, 15.0, 35.0, 44.0, 74.0, 22.0, 52.0, 30.0, 78.0, 25.0, 18.0, 53.0, 27.0, 55.0, 52.0, 50.0, 66.0, 19.0, 46.0, 35.0, 61.0, 149.0, 65.0, 32.0, 45.0, 65.0, 37.0, 34.0, 30.0, 94.0, 18.0, 24.0, 35.0, 160.0, 63.0, 25.0, 14.0, 99.0, 95.0, 36.0, 110.0, 174.0, 93.0, 68.0, 97.0, 54.0, 210.0, 42.0, 121.0, 30.0, 101.0, 91.0, 194.0, 35.0, 74.0, 64.0, 142.0, 83.0, 86.0, 181.0, 116.0, 182.0, 47.0, 42.0, 128.0, 46.0, 80.0, 144.0, 58.0, 62.0, 30.0, 156.0, 63.0, 101.0, 124.0, 109.0], 'episode_lengths': [79, 59, 60, 22, 17, 37, 24, 21, 41, 58, 36, 59, 20, 33, 35, 60, 18, 103, 12, 40, 43, 27, 47, 27, 52, 15, 35, 44, 74, 22, 52, 30, 78, 25, 18, 53, 27, 55, 52, 50, 66, 19, 46, 35, 61, 149, 65, 32, 45, 65, 37, 34, 30, 94, 18, 24, 35, 160, 63, 25, 14, 99, 95, 36, 110, 174, 93, 68, 97, 54, 210, 42, 121, 30, 101, 91, 194, 35, 74, 64, 142, 83, 86, 181, 116, 182, 47, 42, 128, 46, 80, 144, 58, 62, 30, 156, 63, 101, 124, 109]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 1.4033105381531084, 'mean_inference_ms': 4.212891225602756, 'mean_action_processing_ms': 0.3300250700890645, 'mean_env_wait_ms': 0.1638622544416405, 'mean_env_render_ms': 0.0}, 'num_faulty_episodes': 0, 'connector_metrics': {'ObsPreprocessorConnector_ms': 0.023898839950561523, 'StateBufferConnector_ms': 0.007944822311401367, 'ViewRequirementAgentConnector_ms': 0.8427889347076416}, 'num_healthy_workers': 2, 'num_in_flight_async_reqs': 0, 'num_remote_worker_restarts': 0, 'num_agent_steps_sampled': 16000, 'num_agent_steps_trained': 0, 'num_env_steps_sampled': 16000, 'num_env_steps_trained': 0, 'num_env_steps_sampled_this_iter': 4000, 'num_env_steps_trained_this_iter': 0, 'num_env_steps_sampled_throughput_per_sec': 213.32943800515525, 'num_env_steps_trained_throughput_per_sec': 0.0, 'timesteps_total': 16000, 'num_steps_trained_this_iter': 0, 'agent_timesteps_total': 16000, 'timers': {'training_iteration_time_ms': 17856.475, 'sample_time_ms': 12572.999, 'synch_weights_time_ms': 10.772}, 'counters': {'num_env_steps_sampled': 16000, 'num_env_steps_trained': 0, 'num_agent_steps_sampled': 16000, 'num_agent_steps_trained': 0}, 'done': False, 'episodes_total': 405, 'training_iteration': 4, 'trial_id': 'default', 'date': '2023-12-03_18-40-41', 'timestamp': 1701646841, 'time_this_iter_s': 18.75824999809265, 'time_total_s': 71.46343064308167, 'pid': 817316, 'hostname': 'udc-aw34-10c0', 'node_ip': '10.250.132.124', 'config': {'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'num_learner_workers': 0, 'num_gpus_per_learner_worker': 0, 'num_cpus_per_learner_worker': 1, 'local_gpu_idx': 0, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'env': 'CartPole-v1', 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'auto_wrap_old_gym_envs': True, 'action_mask_key': 'action_mask', '_is_atari': None, 'env_runner_cls': None, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': True, 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 5e-05, 'grad_clip': None, 'grad_clip_by': 'global_norm', 'train_batch_size': 4000, 'model': {'_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [64, 64], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'encoder_latent_dim': None, 'always_check_shapes': False, 'lstm_use_prev_action_reward': -1, '_use_default_native_models': -1}, 'optimizer': {}, 'max_requests_in_flight_per_sampler_worker': 2, '_learner_class': None, '_enable_learner_api': True, 'explore': True, 'exploration_config': {}, 'algorithm_config_overrides_per_module': {}, 'policy_map_capacity': 100, 'policy_mapping_fn': <function AlgorithmConfig.DEFAULT_POLICY_MAPPING_FN at 0x7fc583815a60>, 'policies_to_train': None, 'policy_states_are_swappable': False, 'observation_fn': None, 'count_steps_by': 'env_steps', 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'offline_sampling': False, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': None, 'off_policy_estimation_methods': {}, 'ope_split_batch_by_episode': True, 'evaluation_num_workers': 1, 'always_attach_evaluation_results': False, 'enable_async_evaluation': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'export_native_model_files': False, 'checkpoint_trainable_policies_only': False, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'max_num_worker_restarts': 1000, 'delay_between_worker_restarts_s': 60.0, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'worker_health_probe_timeout_s': 60, 'worker_restore_timeout_s': 1800, 'rl_module_spec': SingleAgentRLModuleSpec(module_class=<class 'ray.rllib.algorithms.ppo.torch.ppo_torch_rl_module.PPOTorchRLModule'>, observation_space=None, action_space=None, model_config_dict=None, catalog_class=<class 'ray.rllib.algorithms.ppo.ppo_catalog.PPOCatalog'>, load_state_path=None), '_enable_rl_module_api': True, '_AlgorithmConfig__prior_exploration_config': {'type': 'StochasticSampling'}, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, '_disable_initialize_loss_from_dummy_batch': False, 'simple_optimizer': False, 'policy_map_cache': -1, 'worker_cls': -1, 'synchronize_filters': -1, 'replay_sequence_length': None, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'use_kl_loss': True, 'kl_coeff': 0.2, 'kl_target': 0.01, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'policies': {'default_policy': (None, None, None, None)}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'torch', 'num_cpus_for_driver': 1, 'num_workers': 2}, 'time_since_restore': 71.46343064308167, 'iterations_since_restore': 4, 'perf': {'cpu_util_percent': 46.12307692307694, 'ram_util_percent': 23.803846153846152}}\n",
      "{'custom_metrics': {}, 'episode_media': {}, 'info': {'learner': {'__all__': {'num_agent_steps_trained': 128.0, 'num_env_steps_trained': 4000.0, 'total_loss': 9.720943680704275}, 'default_policy': {'total_loss': 9.720943680704275, 'policy_loss': -0.005054400874369307, 'vf_loss': 9.725228664463263, 'vf_loss_unclipped': 3020.0859353527076, 'vf_explained_var': 0.021918416785787164, 'entropy': 0.5885194635340399, 'mean_kl_loss': 0.003847020450674171, 'default_optimizer_lr': 5.000000000000001e-05, 'curr_lr': 5e-05, 'curr_entropy_coeff': 0.0, 'curr_kl_coeff': 0.10000000149011612}}, 'num_env_steps_sampled': 20000, 'num_env_steps_trained': 0, 'num_agent_steps_sampled': 20000, 'num_agent_steps_trained': 0}, 'sampler_results': {'episode_reward_max': 327.0, 'episode_reward_min': 14.0, 'episode_reward_mean': 94.63, 'episode_len_mean': 94.63, 'episode_media': {}, 'episodes_this_iter': 24, 'policy_reward_min': {}, 'policy_reward_max': {}, 'policy_reward_mean': {}, 'custom_metrics': {}, 'hist_stats': {'episode_reward': [52.0, 15.0, 35.0, 44.0, 74.0, 22.0, 52.0, 30.0, 78.0, 25.0, 18.0, 53.0, 27.0, 55.0, 52.0, 50.0, 66.0, 19.0, 46.0, 35.0, 61.0, 149.0, 65.0, 32.0, 45.0, 65.0, 37.0, 34.0, 30.0, 94.0, 18.0, 24.0, 35.0, 160.0, 63.0, 25.0, 14.0, 99.0, 95.0, 36.0, 110.0, 174.0, 93.0, 68.0, 97.0, 54.0, 210.0, 42.0, 121.0, 30.0, 101.0, 91.0, 194.0, 35.0, 74.0, 64.0, 142.0, 83.0, 86.0, 181.0, 116.0, 182.0, 47.0, 42.0, 128.0, 46.0, 80.0, 144.0, 58.0, 62.0, 30.0, 156.0, 63.0, 101.0, 124.0, 109.0, 124.0, 218.0, 191.0, 44.0, 112.0, 154.0, 151.0, 63.0, 327.0, 70.0, 279.0, 234.0, 101.0, 87.0, 250.0, 188.0, 116.0, 182.0, 175.0, 94.0, 202.0, 266.0, 119.0, 149.0], 'episode_lengths': [52, 15, 35, 44, 74, 22, 52, 30, 78, 25, 18, 53, 27, 55, 52, 50, 66, 19, 46, 35, 61, 149, 65, 32, 45, 65, 37, 34, 30, 94, 18, 24, 35, 160, 63, 25, 14, 99, 95, 36, 110, 174, 93, 68, 97, 54, 210, 42, 121, 30, 101, 91, 194, 35, 74, 64, 142, 83, 86, 181, 116, 182, 47, 42, 128, 46, 80, 144, 58, 62, 30, 156, 63, 101, 124, 109, 124, 218, 191, 44, 112, 154, 151, 63, 327, 70, 279, 234, 101, 87, 250, 188, 116, 182, 175, 94, 202, 266, 119, 149]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 1.4057015575319969, 'mean_inference_ms': 4.236006550741467, 'mean_action_processing_ms': 0.3313930214141748, 'mean_env_wait_ms': 0.1640853970625865, 'mean_env_render_ms': 0.0}, 'num_faulty_episodes': 0, 'connector_metrics': {'ObsPreprocessorConnector_ms': 0.023985624313354492, 'StateBufferConnector_ms': 0.007885217666625977, 'ViewRequirementAgentConnector_ms': 0.8398294448852539}}, 'episode_reward_max': 327.0, 'episode_reward_min': 14.0, 'episode_reward_mean': 94.63, 'episode_len_mean': 94.63, 'episodes_this_iter': 24, 'policy_reward_min': {}, 'policy_reward_max': {}, 'policy_reward_mean': {}, 'hist_stats': {'episode_reward': [52.0, 15.0, 35.0, 44.0, 74.0, 22.0, 52.0, 30.0, 78.0, 25.0, 18.0, 53.0, 27.0, 55.0, 52.0, 50.0, 66.0, 19.0, 46.0, 35.0, 61.0, 149.0, 65.0, 32.0, 45.0, 65.0, 37.0, 34.0, 30.0, 94.0, 18.0, 24.0, 35.0, 160.0, 63.0, 25.0, 14.0, 99.0, 95.0, 36.0, 110.0, 174.0, 93.0, 68.0, 97.0, 54.0, 210.0, 42.0, 121.0, 30.0, 101.0, 91.0, 194.0, 35.0, 74.0, 64.0, 142.0, 83.0, 86.0, 181.0, 116.0, 182.0, 47.0, 42.0, 128.0, 46.0, 80.0, 144.0, 58.0, 62.0, 30.0, 156.0, 63.0, 101.0, 124.0, 109.0, 124.0, 218.0, 191.0, 44.0, 112.0, 154.0, 151.0, 63.0, 327.0, 70.0, 279.0, 234.0, 101.0, 87.0, 250.0, 188.0, 116.0, 182.0, 175.0, 94.0, 202.0, 266.0, 119.0, 149.0], 'episode_lengths': [52, 15, 35, 44, 74, 22, 52, 30, 78, 25, 18, 53, 27, 55, 52, 50, 66, 19, 46, 35, 61, 149, 65, 32, 45, 65, 37, 34, 30, 94, 18, 24, 35, 160, 63, 25, 14, 99, 95, 36, 110, 174, 93, 68, 97, 54, 210, 42, 121, 30, 101, 91, 194, 35, 74, 64, 142, 83, 86, 181, 116, 182, 47, 42, 128, 46, 80, 144, 58, 62, 30, 156, 63, 101, 124, 109, 124, 218, 191, 44, 112, 154, 151, 63, 327, 70, 279, 234, 101, 87, 250, 188, 116, 182, 175, 94, 202, 266, 119, 149]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 1.4057015575319969, 'mean_inference_ms': 4.236006550741467, 'mean_action_processing_ms': 0.3313930214141748, 'mean_env_wait_ms': 0.1640853970625865, 'mean_env_render_ms': 0.0}, 'num_faulty_episodes': 0, 'connector_metrics': {'ObsPreprocessorConnector_ms': 0.023985624313354492, 'StateBufferConnector_ms': 0.007885217666625977, 'ViewRequirementAgentConnector_ms': 0.8398294448852539}, 'num_healthy_workers': 2, 'num_in_flight_async_reqs': 0, 'num_remote_worker_restarts': 0, 'num_agent_steps_sampled': 20000, 'num_agent_steps_trained': 0, 'num_env_steps_sampled': 20000, 'num_env_steps_trained': 0, 'num_env_steps_sampled_this_iter': 4000, 'num_env_steps_trained_this_iter': 0, 'num_env_steps_sampled_throughput_per_sec': 228.88964928273276, 'num_env_steps_trained_throughput_per_sec': 0.0, 'timesteps_total': 20000, 'num_steps_trained_this_iter': 0, 'agent_timesteps_total': 20000, 'timers': {'training_iteration_time_ms': 17780.305, 'sample_time_ms': 12490.999, 'synch_weights_time_ms': 12.126}, 'counters': {'num_env_steps_sampled': 20000, 'num_env_steps_trained': 0, 'num_agent_steps_sampled': 20000, 'num_agent_steps_trained': 0}, 'done': False, 'episodes_total': 429, 'training_iteration': 5, 'trial_id': 'default', 'date': '2023-12-03_18-40-58', 'timestamp': 1701646858, 'time_this_iter_s': 17.4796199798584, 'time_total_s': 88.94305062294006, 'pid': 817316, 'hostname': 'udc-aw34-10c0', 'node_ip': '10.250.132.124', 'config': {'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'num_learner_workers': 0, 'num_gpus_per_learner_worker': 0, 'num_cpus_per_learner_worker': 1, 'local_gpu_idx': 0, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'env': 'CartPole-v1', 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'auto_wrap_old_gym_envs': True, 'action_mask_key': 'action_mask', '_is_atari': None, 'env_runner_cls': None, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': True, 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 5e-05, 'grad_clip': None, 'grad_clip_by': 'global_norm', 'train_batch_size': 4000, 'model': {'_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [64, 64], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'encoder_latent_dim': None, 'always_check_shapes': False, 'lstm_use_prev_action_reward': -1, '_use_default_native_models': -1}, 'optimizer': {}, 'max_requests_in_flight_per_sampler_worker': 2, '_learner_class': None, '_enable_learner_api': True, 'explore': True, 'exploration_config': {}, 'algorithm_config_overrides_per_module': {}, 'policy_map_capacity': 100, 'policy_mapping_fn': <function AlgorithmConfig.DEFAULT_POLICY_MAPPING_FN at 0x7fc583815a60>, 'policies_to_train': None, 'policy_states_are_swappable': False, 'observation_fn': None, 'count_steps_by': 'env_steps', 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'offline_sampling': False, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': None, 'off_policy_estimation_methods': {}, 'ope_split_batch_by_episode': True, 'evaluation_num_workers': 1, 'always_attach_evaluation_results': False, 'enable_async_evaluation': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'export_native_model_files': False, 'checkpoint_trainable_policies_only': False, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'max_num_worker_restarts': 1000, 'delay_between_worker_restarts_s': 60.0, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'worker_health_probe_timeout_s': 60, 'worker_restore_timeout_s': 1800, 'rl_module_spec': SingleAgentRLModuleSpec(module_class=<class 'ray.rllib.algorithms.ppo.torch.ppo_torch_rl_module.PPOTorchRLModule'>, observation_space=None, action_space=None, model_config_dict=None, catalog_class=<class 'ray.rllib.algorithms.ppo.ppo_catalog.PPOCatalog'>, load_state_path=None), '_enable_rl_module_api': True, '_AlgorithmConfig__prior_exploration_config': {'type': 'StochasticSampling'}, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, '_disable_initialize_loss_from_dummy_batch': False, 'simple_optimizer': False, 'policy_map_cache': -1, 'worker_cls': -1, 'synchronize_filters': -1, 'replay_sequence_length': None, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'use_kl_loss': True, 'kl_coeff': 0.2, 'kl_target': 0.01, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'policies': {'default_policy': (None, None, None, None)}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'torch', 'num_cpus_for_driver': 1, 'num_workers': 2}, 'time_since_restore': 88.94305062294006, 'iterations_since_restore': 5, 'perf': {'cpu_util_percent': 47.52800000000001, 'ram_util_percent': 23.808000000000003}}\n",
      "{'custom_metrics': {}, 'episode_media': {}, 'info': {'learner': {'__all__': {'num_agent_steps_trained': 128.0, 'num_env_steps_trained': 4000.0, 'total_loss': 9.790043307265748}, 'default_policy': {'total_loss': 9.790043307265748, 'policy_loss': -0.005966225527286498, 'vf_loss': 9.79543487857908, 'vf_loss_unclipped': 3863.9129965971274, 'vf_explained_var': 0.01970257592607917, 'entropy': 0.5938878347243327, 'mean_kl_loss': 0.005746804958953116, 'default_optimizer_lr': 5.000000000000001e-05, 'curr_lr': 5e-05, 'curr_entropy_coeff': 0.0, 'curr_kl_coeff': 0.10000000149011612}}, 'num_env_steps_sampled': 24000, 'num_env_steps_trained': 0, 'num_agent_steps_sampled': 24000, 'num_agent_steps_trained': 0}, 'sampler_results': {'episode_reward_max': 403.0, 'episode_reward_min': 14.0, 'episode_reward_mean': 127.56, 'episode_len_mean': 127.56, 'episode_media': {}, 'episodes_this_iter': 17, 'policy_reward_min': {}, 'policy_reward_max': {}, 'policy_reward_mean': {}, 'custom_metrics': {}, 'hist_stats': {'episode_reward': [19.0, 46.0, 35.0, 61.0, 149.0, 65.0, 32.0, 45.0, 65.0, 37.0, 34.0, 30.0, 94.0, 18.0, 24.0, 35.0, 160.0, 63.0, 25.0, 14.0, 99.0, 95.0, 36.0, 110.0, 174.0, 93.0, 68.0, 97.0, 54.0, 210.0, 42.0, 121.0, 30.0, 101.0, 91.0, 194.0, 35.0, 74.0, 64.0, 142.0, 83.0, 86.0, 181.0, 116.0, 182.0, 47.0, 42.0, 128.0, 46.0, 80.0, 144.0, 58.0, 62.0, 30.0, 156.0, 63.0, 101.0, 124.0, 109.0, 124.0, 218.0, 191.0, 44.0, 112.0, 154.0, 151.0, 63.0, 327.0, 70.0, 279.0, 234.0, 101.0, 87.0, 250.0, 188.0, 116.0, 182.0, 175.0, 94.0, 202.0, 266.0, 119.0, 149.0, 403.0, 374.0, 173.0, 347.0, 327.0, 202.0, 149.0, 237.0, 251.0, 218.0, 79.0, 153.0, 225.0, 201.0, 198.0, 214.0, 290.0], 'episode_lengths': [19, 46, 35, 61, 149, 65, 32, 45, 65, 37, 34, 30, 94, 18, 24, 35, 160, 63, 25, 14, 99, 95, 36, 110, 174, 93, 68, 97, 54, 210, 42, 121, 30, 101, 91, 194, 35, 74, 64, 142, 83, 86, 181, 116, 182, 47, 42, 128, 46, 80, 144, 58, 62, 30, 156, 63, 101, 124, 109, 124, 218, 191, 44, 112, 154, 151, 63, 327, 70, 279, 234, 101, 87, 250, 188, 116, 182, 175, 94, 202, 266, 119, 149, 403, 374, 173, 347, 327, 202, 149, 237, 251, 218, 79, 153, 225, 201, 198, 214, 290]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 1.4066483327922674, 'mean_inference_ms': 4.254464069200928, 'mean_action_processing_ms': 0.3330475178660264, 'mean_env_wait_ms': 0.16416729917383907, 'mean_env_render_ms': 0.0}, 'num_faulty_episodes': 0, 'connector_metrics': {'ObsPreprocessorConnector_ms': 0.019082069396972656, 'StateBufferConnector_ms': 0.008596181869506836, 'ViewRequirementAgentConnector_ms': 0.8254990577697754}}, 'episode_reward_max': 403.0, 'episode_reward_min': 14.0, 'episode_reward_mean': 127.56, 'episode_len_mean': 127.56, 'episodes_this_iter': 17, 'policy_reward_min': {}, 'policy_reward_max': {}, 'policy_reward_mean': {}, 'hist_stats': {'episode_reward': [19.0, 46.0, 35.0, 61.0, 149.0, 65.0, 32.0, 45.0, 65.0, 37.0, 34.0, 30.0, 94.0, 18.0, 24.0, 35.0, 160.0, 63.0, 25.0, 14.0, 99.0, 95.0, 36.0, 110.0, 174.0, 93.0, 68.0, 97.0, 54.0, 210.0, 42.0, 121.0, 30.0, 101.0, 91.0, 194.0, 35.0, 74.0, 64.0, 142.0, 83.0, 86.0, 181.0, 116.0, 182.0, 47.0, 42.0, 128.0, 46.0, 80.0, 144.0, 58.0, 62.0, 30.0, 156.0, 63.0, 101.0, 124.0, 109.0, 124.0, 218.0, 191.0, 44.0, 112.0, 154.0, 151.0, 63.0, 327.0, 70.0, 279.0, 234.0, 101.0, 87.0, 250.0, 188.0, 116.0, 182.0, 175.0, 94.0, 202.0, 266.0, 119.0, 149.0, 403.0, 374.0, 173.0, 347.0, 327.0, 202.0, 149.0, 237.0, 251.0, 218.0, 79.0, 153.0, 225.0, 201.0, 198.0, 214.0, 290.0], 'episode_lengths': [19, 46, 35, 61, 149, 65, 32, 45, 65, 37, 34, 30, 94, 18, 24, 35, 160, 63, 25, 14, 99, 95, 36, 110, 174, 93, 68, 97, 54, 210, 42, 121, 30, 101, 91, 194, 35, 74, 64, 142, 83, 86, 181, 116, 182, 47, 42, 128, 46, 80, 144, 58, 62, 30, 156, 63, 101, 124, 109, 124, 218, 191, 44, 112, 154, 151, 63, 327, 70, 279, 234, 101, 87, 250, 188, 116, 182, 175, 94, 202, 266, 119, 149, 403, 374, 173, 347, 327, 202, 149, 237, 251, 218, 79, 153, 225, 201, 198, 214, 290]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 1.4066483327922674, 'mean_inference_ms': 4.254464069200928, 'mean_action_processing_ms': 0.3330475178660264, 'mean_env_wait_ms': 0.16416729917383907, 'mean_env_render_ms': 0.0}, 'num_faulty_episodes': 0, 'connector_metrics': {'ObsPreprocessorConnector_ms': 0.019082069396972656, 'StateBufferConnector_ms': 0.008596181869506836, 'ViewRequirementAgentConnector_ms': 0.8254990577697754}, 'num_healthy_workers': 2, 'num_in_flight_async_reqs': 0, 'num_remote_worker_restarts': 0, 'num_agent_steps_sampled': 24000, 'num_agent_steps_trained': 0, 'num_env_steps_sampled': 24000, 'num_env_steps_trained': 0, 'num_env_steps_sampled_this_iter': 4000, 'num_env_steps_trained_this_iter': 0, 'num_env_steps_sampled_throughput_per_sec': 225.85751009900702, 'num_env_steps_trained_throughput_per_sec': 0.0, 'timesteps_total': 24000, 'num_steps_trained_this_iter': 0, 'agent_timesteps_total': 24000, 'timers': {'training_iteration_time_ms': 17768.631, 'sample_time_ms': 12470.777, 'synch_weights_time_ms': 11.394}, 'counters': {'num_env_steps_sampled': 24000, 'num_env_steps_trained': 0, 'num_agent_steps_sampled': 24000, 'num_agent_steps_trained': 0}, 'done': False, 'episodes_total': 446, 'training_iteration': 6, 'trial_id': 'default', 'date': '2023-12-03_18-41-16', 'timestamp': 1701646876, 'time_this_iter_s': 17.716686964035034, 'time_total_s': 106.6597375869751, 'pid': 817316, 'hostname': 'udc-aw34-10c0', 'node_ip': '10.250.132.124', 'config': {'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'num_learner_workers': 0, 'num_gpus_per_learner_worker': 0, 'num_cpus_per_learner_worker': 1, 'local_gpu_idx': 0, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'env': 'CartPole-v1', 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'auto_wrap_old_gym_envs': True, 'action_mask_key': 'action_mask', '_is_atari': None, 'env_runner_cls': None, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': True, 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 5e-05, 'grad_clip': None, 'grad_clip_by': 'global_norm', 'train_batch_size': 4000, 'model': {'_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [64, 64], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'encoder_latent_dim': None, 'always_check_shapes': False, 'lstm_use_prev_action_reward': -1, '_use_default_native_models': -1}, 'optimizer': {}, 'max_requests_in_flight_per_sampler_worker': 2, '_learner_class': None, '_enable_learner_api': True, 'explore': True, 'exploration_config': {}, 'algorithm_config_overrides_per_module': {}, 'policy_map_capacity': 100, 'policy_mapping_fn': <function AlgorithmConfig.DEFAULT_POLICY_MAPPING_FN at 0x7fc583815a60>, 'policies_to_train': None, 'policy_states_are_swappable': False, 'observation_fn': None, 'count_steps_by': 'env_steps', 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'offline_sampling': False, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': None, 'off_policy_estimation_methods': {}, 'ope_split_batch_by_episode': True, 'evaluation_num_workers': 1, 'always_attach_evaluation_results': False, 'enable_async_evaluation': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'export_native_model_files': False, 'checkpoint_trainable_policies_only': False, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'max_num_worker_restarts': 1000, 'delay_between_worker_restarts_s': 60.0, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'worker_health_probe_timeout_s': 60, 'worker_restore_timeout_s': 1800, 'rl_module_spec': SingleAgentRLModuleSpec(module_class=<class 'ray.rllib.algorithms.ppo.torch.ppo_torch_rl_module.PPOTorchRLModule'>, observation_space=None, action_space=None, model_config_dict=None, catalog_class=<class 'ray.rllib.algorithms.ppo.ppo_catalog.PPOCatalog'>, load_state_path=None), '_enable_rl_module_api': True, '_AlgorithmConfig__prior_exploration_config': {'type': 'StochasticSampling'}, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, '_disable_initialize_loss_from_dummy_batch': False, 'simple_optimizer': False, 'policy_map_cache': -1, 'worker_cls': -1, 'synchronize_filters': -1, 'replay_sequence_length': None, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'use_kl_loss': True, 'kl_coeff': 0.2, 'kl_target': 0.01, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'policies': {'default_policy': (None, None, None, None)}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'torch', 'num_cpus_for_driver': 1, 'num_workers': 2}, 'time_since_restore': 106.6597375869751, 'iterations_since_restore': 6, 'perf': {'cpu_util_percent': 45.812, 'ram_util_percent': 23.851999999999997}}\n",
      "{'custom_metrics': {}, 'episode_media': {}, 'info': {'learner': {'__all__': {'num_agent_steps_trained': 128.0, 'num_env_steps_trained': 4000.0, 'total_loss': 9.78742753823937}, 'default_policy': {'total_loss': 9.78742753823937, 'policy_loss': -0.00591888237667125, 'vf_loss': 9.792860803827802, 'vf_loss_unclipped': 3760.2198508451743, 'vf_explained_var': 0.025109749803665097, 'entropy': 0.5688012369723716, 'mean_kl_loss': 0.004856028993056428, 'default_optimizer_lr': 5.000000000000001e-05, 'curr_lr': 5e-05, 'curr_entropy_coeff': 0.0, 'curr_kl_coeff': 0.05000000074505806}}, 'num_env_steps_sampled': 28000, 'num_env_steps_trained': 0, 'num_agent_steps_sampled': 28000, 'num_agent_steps_trained': 0}, 'sampler_results': {'episode_reward_max': 461.0, 'episode_reward_min': 14.0, 'episode_reward_mean': 154.17, 'episode_len_mean': 154.17, 'episode_media': {}, 'episodes_this_iter': 17, 'policy_reward_min': {}, 'policy_reward_max': {}, 'policy_reward_mean': {}, 'custom_metrics': {}, 'hist_stats': {'episode_reward': [63.0, 25.0, 14.0, 99.0, 95.0, 36.0, 110.0, 174.0, 93.0, 68.0, 97.0, 54.0, 210.0, 42.0, 121.0, 30.0, 101.0, 91.0, 194.0, 35.0, 74.0, 64.0, 142.0, 83.0, 86.0, 181.0, 116.0, 182.0, 47.0, 42.0, 128.0, 46.0, 80.0, 144.0, 58.0, 62.0, 30.0, 156.0, 63.0, 101.0, 124.0, 109.0, 124.0, 218.0, 191.0, 44.0, 112.0, 154.0, 151.0, 63.0, 327.0, 70.0, 279.0, 234.0, 101.0, 87.0, 250.0, 188.0, 116.0, 182.0, 175.0, 94.0, 202.0, 266.0, 119.0, 149.0, 403.0, 374.0, 173.0, 347.0, 327.0, 202.0, 149.0, 237.0, 251.0, 218.0, 79.0, 153.0, 225.0, 201.0, 198.0, 214.0, 290.0, 247.0, 105.0, 166.0, 202.0, 173.0, 310.0, 183.0, 376.0, 110.0, 113.0, 461.0, 183.0, 173.0, 213.0, 93.0, 182.0, 320.0], 'episode_lengths': [63, 25, 14, 99, 95, 36, 110, 174, 93, 68, 97, 54, 210, 42, 121, 30, 101, 91, 194, 35, 74, 64, 142, 83, 86, 181, 116, 182, 47, 42, 128, 46, 80, 144, 58, 62, 30, 156, 63, 101, 124, 109, 124, 218, 191, 44, 112, 154, 151, 63, 327, 70, 279, 234, 101, 87, 250, 188, 116, 182, 175, 94, 202, 266, 119, 149, 403, 374, 173, 347, 327, 202, 149, 237, 251, 218, 79, 153, 225, 201, 198, 214, 290, 247, 105, 166, 202, 173, 310, 183, 376, 110, 113, 461, 183, 173, 213, 93, 182, 320]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 1.4077060518795492, 'mean_inference_ms': 4.268199872738181, 'mean_action_processing_ms': 0.33523552299368153, 'mean_env_wait_ms': 0.1642982354113774, 'mean_env_render_ms': 0.0}, 'num_faulty_episodes': 0, 'connector_metrics': {'ObsPreprocessorConnector_ms': 0.02038741111755371, 'StateBufferConnector_ms': 0.00566411018371582, 'ViewRequirementAgentConnector_ms': 0.8517355918884277}}, 'episode_reward_max': 461.0, 'episode_reward_min': 14.0, 'episode_reward_mean': 154.17, 'episode_len_mean': 154.17, 'episodes_this_iter': 17, 'policy_reward_min': {}, 'policy_reward_max': {}, 'policy_reward_mean': {}, 'hist_stats': {'episode_reward': [63.0, 25.0, 14.0, 99.0, 95.0, 36.0, 110.0, 174.0, 93.0, 68.0, 97.0, 54.0, 210.0, 42.0, 121.0, 30.0, 101.0, 91.0, 194.0, 35.0, 74.0, 64.0, 142.0, 83.0, 86.0, 181.0, 116.0, 182.0, 47.0, 42.0, 128.0, 46.0, 80.0, 144.0, 58.0, 62.0, 30.0, 156.0, 63.0, 101.0, 124.0, 109.0, 124.0, 218.0, 191.0, 44.0, 112.0, 154.0, 151.0, 63.0, 327.0, 70.0, 279.0, 234.0, 101.0, 87.0, 250.0, 188.0, 116.0, 182.0, 175.0, 94.0, 202.0, 266.0, 119.0, 149.0, 403.0, 374.0, 173.0, 347.0, 327.0, 202.0, 149.0, 237.0, 251.0, 218.0, 79.0, 153.0, 225.0, 201.0, 198.0, 214.0, 290.0, 247.0, 105.0, 166.0, 202.0, 173.0, 310.0, 183.0, 376.0, 110.0, 113.0, 461.0, 183.0, 173.0, 213.0, 93.0, 182.0, 320.0], 'episode_lengths': [63, 25, 14, 99, 95, 36, 110, 174, 93, 68, 97, 54, 210, 42, 121, 30, 101, 91, 194, 35, 74, 64, 142, 83, 86, 181, 116, 182, 47, 42, 128, 46, 80, 144, 58, 62, 30, 156, 63, 101, 124, 109, 124, 218, 191, 44, 112, 154, 151, 63, 327, 70, 279, 234, 101, 87, 250, 188, 116, 182, 175, 94, 202, 266, 119, 149, 403, 374, 173, 347, 327, 202, 149, 237, 251, 218, 79, 153, 225, 201, 198, 214, 290, 247, 105, 166, 202, 173, 310, 183, 376, 110, 113, 461, 183, 173, 213, 93, 182, 320]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 1.4077060518795492, 'mean_inference_ms': 4.268199872738181, 'mean_action_processing_ms': 0.33523552299368153, 'mean_env_wait_ms': 0.1642982354113774, 'mean_env_render_ms': 0.0}, 'num_faulty_episodes': 0, 'connector_metrics': {'ObsPreprocessorConnector_ms': 0.02038741111755371, 'StateBufferConnector_ms': 0.00566411018371582, 'ViewRequirementAgentConnector_ms': 0.8517355918884277}, 'num_healthy_workers': 2, 'num_in_flight_async_reqs': 0, 'num_remote_worker_restarts': 0, 'num_agent_steps_sampled': 28000, 'num_agent_steps_trained': 0, 'num_env_steps_sampled': 28000, 'num_env_steps_trained': 0, 'num_env_steps_sampled_this_iter': 4000, 'num_env_steps_trained_this_iter': 0, 'num_env_steps_sampled_throughput_per_sec': 220.66174074421932, 'num_env_steps_trained_throughput_per_sec': 0.0, 'timesteps_total': 28000, 'num_steps_trained_this_iter': 0, 'agent_timesteps_total': 28000, 'timers': {'training_iteration_time_ms': 17819.864, 'sample_time_ms': 12459.081, 'synch_weights_time_ms': 11.289}, 'counters': {'num_env_steps_sampled': 28000, 'num_env_steps_trained': 0, 'num_agent_steps_sampled': 28000, 'num_agent_steps_trained': 0}, 'done': False, 'episodes_total': 463, 'training_iteration': 7, 'trial_id': 'default', 'date': '2023-12-03_18-41-34', 'timestamp': 1701646894, 'time_this_iter_s': 18.132742643356323, 'time_total_s': 124.79248023033142, 'pid': 817316, 'hostname': 'udc-aw34-10c0', 'node_ip': '10.250.132.124', 'config': {'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'num_learner_workers': 0, 'num_gpus_per_learner_worker': 0, 'num_cpus_per_learner_worker': 1, 'local_gpu_idx': 0, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'env': 'CartPole-v1', 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'auto_wrap_old_gym_envs': True, 'action_mask_key': 'action_mask', '_is_atari': None, 'env_runner_cls': None, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': True, 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 5e-05, 'grad_clip': None, 'grad_clip_by': 'global_norm', 'train_batch_size': 4000, 'model': {'_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [64, 64], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'encoder_latent_dim': None, 'always_check_shapes': False, 'lstm_use_prev_action_reward': -1, '_use_default_native_models': -1}, 'optimizer': {}, 'max_requests_in_flight_per_sampler_worker': 2, '_learner_class': None, '_enable_learner_api': True, 'explore': True, 'exploration_config': {}, 'algorithm_config_overrides_per_module': {}, 'policy_map_capacity': 100, 'policy_mapping_fn': <function AlgorithmConfig.DEFAULT_POLICY_MAPPING_FN at 0x7fc583815a60>, 'policies_to_train': None, 'policy_states_are_swappable': False, 'observation_fn': None, 'count_steps_by': 'env_steps', 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'offline_sampling': False, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': None, 'off_policy_estimation_methods': {}, 'ope_split_batch_by_episode': True, 'evaluation_num_workers': 1, 'always_attach_evaluation_results': False, 'enable_async_evaluation': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'export_native_model_files': False, 'checkpoint_trainable_policies_only': False, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'max_num_worker_restarts': 1000, 'delay_between_worker_restarts_s': 60.0, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'worker_health_probe_timeout_s': 60, 'worker_restore_timeout_s': 1800, 'rl_module_spec': SingleAgentRLModuleSpec(module_class=<class 'ray.rllib.algorithms.ppo.torch.ppo_torch_rl_module.PPOTorchRLModule'>, observation_space=None, action_space=None, model_config_dict=None, catalog_class=<class 'ray.rllib.algorithms.ppo.ppo_catalog.PPOCatalog'>, load_state_path=None), '_enable_rl_module_api': True, '_AlgorithmConfig__prior_exploration_config': {'type': 'StochasticSampling'}, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, '_disable_initialize_loss_from_dummy_batch': False, 'simple_optimizer': False, 'policy_map_cache': -1, 'worker_cls': -1, 'synchronize_filters': -1, 'replay_sequence_length': None, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'use_kl_loss': True, 'kl_coeff': 0.2, 'kl_target': 0.01, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'policies': {'default_policy': (None, None, None, None)}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'torch', 'num_cpus_for_driver': 1, 'num_workers': 2}, 'time_since_restore': 124.79248023033142, 'iterations_since_restore': 7, 'perf': {'cpu_util_percent': 47.31923076923078, 'ram_util_percent': 23.857692307692307}}\n",
      "{'custom_metrics': {}, 'episode_media': {}, 'info': {'learner': {'__all__': {'num_agent_steps_trained': 128.0, 'num_env_steps_trained': 4000.0, 'total_loss': 9.839803146655118}, 'default_policy': {'total_loss': 9.839803146655118, 'policy_loss': -0.002566052557054613, 'vf_loss': 9.8420722571009, 'vf_loss_unclipped': 4556.925773506734, 'vf_explained_var': 0.014353877890592953, 'entropy': 0.5670169381571731, 'mean_kl_loss': 0.0059387901446572395, 'default_optimizer_lr': 5.000000000000001e-05, 'curr_lr': 5e-05, 'curr_entropy_coeff': 0.0, 'curr_kl_coeff': 0.05000000074505806}}, 'num_env_steps_sampled': 32000, 'num_env_steps_trained': 0, 'num_agent_steps_sampled': 32000, 'num_agent_steps_trained': 0}, 'sampler_results': {'episode_reward_max': 500.0, 'episode_reward_min': 30.0, 'episode_reward_mean': 185.0, 'episode_len_mean': 185.0, 'episode_media': {}, 'episodes_this_iter': 13, 'policy_reward_min': {}, 'policy_reward_max': {}, 'policy_reward_mean': {}, 'custom_metrics': {}, 'hist_stats': {'episode_reward': [42.0, 121.0, 30.0, 101.0, 91.0, 194.0, 35.0, 74.0, 64.0, 142.0, 83.0, 86.0, 181.0, 116.0, 182.0, 47.0, 42.0, 128.0, 46.0, 80.0, 144.0, 58.0, 62.0, 30.0, 156.0, 63.0, 101.0, 124.0, 109.0, 124.0, 218.0, 191.0, 44.0, 112.0, 154.0, 151.0, 63.0, 327.0, 70.0, 279.0, 234.0, 101.0, 87.0, 250.0, 188.0, 116.0, 182.0, 175.0, 94.0, 202.0, 266.0, 119.0, 149.0, 403.0, 374.0, 173.0, 347.0, 327.0, 202.0, 149.0, 237.0, 251.0, 218.0, 79.0, 153.0, 225.0, 201.0, 198.0, 214.0, 290.0, 247.0, 105.0, 166.0, 202.0, 173.0, 310.0, 183.0, 376.0, 110.0, 113.0, 461.0, 183.0, 173.0, 213.0, 93.0, 182.0, 320.0, 365.0, 329.0, 359.0, 410.0, 337.0, 309.0, 161.0, 338.0, 285.0, 198.0, 500.0, 312.0, 318.0], 'episode_lengths': [42, 121, 30, 101, 91, 194, 35, 74, 64, 142, 83, 86, 181, 116, 182, 47, 42, 128, 46, 80, 144, 58, 62, 30, 156, 63, 101, 124, 109, 124, 218, 191, 44, 112, 154, 151, 63, 327, 70, 279, 234, 101, 87, 250, 188, 116, 182, 175, 94, 202, 266, 119, 149, 403, 374, 173, 347, 327, 202, 149, 237, 251, 218, 79, 153, 225, 201, 198, 214, 290, 247, 105, 166, 202, 173, 310, 183, 376, 110, 113, 461, 183, 173, 213, 93, 182, 320, 365, 329, 359, 410, 337, 309, 161, 338, 285, 198, 500, 312, 318]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 1.4031728572685227, 'mean_inference_ms': 4.26210816876935, 'mean_action_processing_ms': 0.33505568682836395, 'mean_env_wait_ms': 0.16480833426655328, 'mean_env_render_ms': 0.0}, 'num_faulty_episodes': 0, 'connector_metrics': {'ObsPreprocessorConnector_ms': 0.019361495971679688, 'StateBufferConnector_ms': 0.006230354309082031, 'ViewRequirementAgentConnector_ms': 0.8188767433166504}}, 'episode_reward_max': 500.0, 'episode_reward_min': 30.0, 'episode_reward_mean': 185.0, 'episode_len_mean': 185.0, 'episodes_this_iter': 13, 'policy_reward_min': {}, 'policy_reward_max': {}, 'policy_reward_mean': {}, 'hist_stats': {'episode_reward': [42.0, 121.0, 30.0, 101.0, 91.0, 194.0, 35.0, 74.0, 64.0, 142.0, 83.0, 86.0, 181.0, 116.0, 182.0, 47.0, 42.0, 128.0, 46.0, 80.0, 144.0, 58.0, 62.0, 30.0, 156.0, 63.0, 101.0, 124.0, 109.0, 124.0, 218.0, 191.0, 44.0, 112.0, 154.0, 151.0, 63.0, 327.0, 70.0, 279.0, 234.0, 101.0, 87.0, 250.0, 188.0, 116.0, 182.0, 175.0, 94.0, 202.0, 266.0, 119.0, 149.0, 403.0, 374.0, 173.0, 347.0, 327.0, 202.0, 149.0, 237.0, 251.0, 218.0, 79.0, 153.0, 225.0, 201.0, 198.0, 214.0, 290.0, 247.0, 105.0, 166.0, 202.0, 173.0, 310.0, 183.0, 376.0, 110.0, 113.0, 461.0, 183.0, 173.0, 213.0, 93.0, 182.0, 320.0, 365.0, 329.0, 359.0, 410.0, 337.0, 309.0, 161.0, 338.0, 285.0, 198.0, 500.0, 312.0, 318.0], 'episode_lengths': [42, 121, 30, 101, 91, 194, 35, 74, 64, 142, 83, 86, 181, 116, 182, 47, 42, 128, 46, 80, 144, 58, 62, 30, 156, 63, 101, 124, 109, 124, 218, 191, 44, 112, 154, 151, 63, 327, 70, 279, 234, 101, 87, 250, 188, 116, 182, 175, 94, 202, 266, 119, 149, 403, 374, 173, 347, 327, 202, 149, 237, 251, 218, 79, 153, 225, 201, 198, 214, 290, 247, 105, 166, 202, 173, 310, 183, 376, 110, 113, 461, 183, 173, 213, 93, 182, 320, 365, 329, 359, 410, 337, 309, 161, 338, 285, 198, 500, 312, 318]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 1.4031728572685227, 'mean_inference_ms': 4.26210816876935, 'mean_action_processing_ms': 0.33505568682836395, 'mean_env_wait_ms': 0.16480833426655328, 'mean_env_render_ms': 0.0}, 'num_faulty_episodes': 0, 'connector_metrics': {'ObsPreprocessorConnector_ms': 0.019361495971679688, 'StateBufferConnector_ms': 0.006230354309082031, 'ViewRequirementAgentConnector_ms': 0.8188767433166504}, 'num_healthy_workers': 2, 'num_in_flight_async_reqs': 0, 'num_remote_worker_restarts': 0, 'num_agent_steps_sampled': 32000, 'num_agent_steps_trained': 0, 'num_env_steps_sampled': 32000, 'num_env_steps_trained': 0, 'num_env_steps_sampled_this_iter': 4000, 'num_env_steps_trained_this_iter': 0, 'num_env_steps_sampled_throughput_per_sec': 225.7268672798406, 'num_env_steps_trained_throughput_per_sec': 0.0, 'timesteps_total': 32000, 'num_steps_trained_this_iter': 0, 'agent_timesteps_total': 32000, 'timers': {'training_iteration_time_ms': 17807.445, 'sample_time_ms': 12459.951, 'synch_weights_time_ms': 10.579}, 'counters': {'num_env_steps_sampled': 32000, 'num_env_steps_trained': 0, 'num_agent_steps_sampled': 32000, 'num_agent_steps_trained': 0}, 'done': False, 'episodes_total': 476, 'training_iteration': 8, 'trial_id': 'default', 'date': '2023-12-03_18-41-52', 'timestamp': 1701646912, 'time_this_iter_s': 17.727654457092285, 'time_total_s': 142.5201346874237, 'pid': 817316, 'hostname': 'udc-aw34-10c0', 'node_ip': '10.250.132.124', 'config': {'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'num_learner_workers': 0, 'num_gpus_per_learner_worker': 0, 'num_cpus_per_learner_worker': 1, 'local_gpu_idx': 0, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'env': 'CartPole-v1', 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'auto_wrap_old_gym_envs': True, 'action_mask_key': 'action_mask', '_is_atari': None, 'env_runner_cls': None, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': True, 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 5e-05, 'grad_clip': None, 'grad_clip_by': 'global_norm', 'train_batch_size': 4000, 'model': {'_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [64, 64], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'encoder_latent_dim': None, 'always_check_shapes': False, 'lstm_use_prev_action_reward': -1, '_use_default_native_models': -1}, 'optimizer': {}, 'max_requests_in_flight_per_sampler_worker': 2, '_learner_class': None, '_enable_learner_api': True, 'explore': True, 'exploration_config': {}, 'algorithm_config_overrides_per_module': {}, 'policy_map_capacity': 100, 'policy_mapping_fn': <function AlgorithmConfig.DEFAULT_POLICY_MAPPING_FN at 0x7fc583815a60>, 'policies_to_train': None, 'policy_states_are_swappable': False, 'observation_fn': None, 'count_steps_by': 'env_steps', 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'offline_sampling': False, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': None, 'off_policy_estimation_methods': {}, 'ope_split_batch_by_episode': True, 'evaluation_num_workers': 1, 'always_attach_evaluation_results': False, 'enable_async_evaluation': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'export_native_model_files': False, 'checkpoint_trainable_policies_only': False, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'max_num_worker_restarts': 1000, 'delay_between_worker_restarts_s': 60.0, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'worker_health_probe_timeout_s': 60, 'worker_restore_timeout_s': 1800, 'rl_module_spec': SingleAgentRLModuleSpec(module_class=<class 'ray.rllib.algorithms.ppo.torch.ppo_torch_rl_module.PPOTorchRLModule'>, observation_space=None, action_space=None, model_config_dict=None, catalog_class=<class 'ray.rllib.algorithms.ppo.ppo_catalog.PPOCatalog'>, load_state_path=None), '_enable_rl_module_api': True, '_AlgorithmConfig__prior_exploration_config': {'type': 'StochasticSampling'}, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, '_disable_initialize_loss_from_dummy_batch': False, 'simple_optimizer': False, 'policy_map_cache': -1, 'worker_cls': -1, 'synchronize_filters': -1, 'replay_sequence_length': None, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'use_kl_loss': True, 'kl_coeff': 0.2, 'kl_target': 0.01, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'policies': {'default_policy': (None, None, None, None)}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'torch', 'num_cpus_for_driver': 1, 'num_workers': 2}, 'time_since_restore': 142.5201346874237, 'iterations_since_restore': 8, 'perf': {'cpu_util_percent': 42.74, 'ram_util_percent': 23.855999999999998}}\n",
      "{'custom_metrics': {}, 'episode_media': {}, 'info': {'learner': {'__all__': {'num_agent_steps_trained': 128.0, 'num_env_steps_trained': 4000.0, 'total_loss': 9.853311756272305}, 'default_policy': {'total_loss': 9.853311756272305, 'policy_loss': -0.004176118023900081, 'vf_loss': 9.856955245613797, 'vf_loss_unclipped': 4634.521766906608, 'vf_explained_var': 0.019107592194827636, 'entropy': 0.5685692925188841, 'mean_kl_loss': 0.010652864086428468, 'default_optimizer_lr': 5.000000000000001e-05, 'curr_lr': 5e-05, 'curr_entropy_coeff': 0.0, 'curr_kl_coeff': 0.05000000074505806}}, 'num_env_steps_sampled': 36000, 'num_env_steps_trained': 0, 'num_agent_steps_sampled': 36000, 'num_agent_steps_trained': 0}, 'sampler_results': {'episode_reward_max': 500.0, 'episode_reward_min': 30.0, 'episode_reward_mean': 214.73, 'episode_len_mean': 214.73, 'episode_media': {}, 'episodes_this_iter': 12, 'policy_reward_min': {}, 'policy_reward_max': {}, 'policy_reward_mean': {}, 'custom_metrics': {}, 'hist_stats': {'episode_reward': [181.0, 116.0, 182.0, 47.0, 42.0, 128.0, 46.0, 80.0, 144.0, 58.0, 62.0, 30.0, 156.0, 63.0, 101.0, 124.0, 109.0, 124.0, 218.0, 191.0, 44.0, 112.0, 154.0, 151.0, 63.0, 327.0, 70.0, 279.0, 234.0, 101.0, 87.0, 250.0, 188.0, 116.0, 182.0, 175.0, 94.0, 202.0, 266.0, 119.0, 149.0, 403.0, 374.0, 173.0, 347.0, 327.0, 202.0, 149.0, 237.0, 251.0, 218.0, 79.0, 153.0, 225.0, 201.0, 198.0, 214.0, 290.0, 247.0, 105.0, 166.0, 202.0, 173.0, 310.0, 183.0, 376.0, 110.0, 113.0, 461.0, 183.0, 173.0, 213.0, 93.0, 182.0, 320.0, 365.0, 329.0, 359.0, 410.0, 337.0, 309.0, 161.0, 338.0, 285.0, 198.0, 500.0, 312.0, 318.0, 152.0, 241.0, 500.0, 270.0, 460.0, 298.0, 331.0, 337.0, 447.0, 238.0, 500.0, 262.0], 'episode_lengths': [181, 116, 182, 47, 42, 128, 46, 80, 144, 58, 62, 30, 156, 63, 101, 124, 109, 124, 218, 191, 44, 112, 154, 151, 63, 327, 70, 279, 234, 101, 87, 250, 188, 116, 182, 175, 94, 202, 266, 119, 149, 403, 374, 173, 347, 327, 202, 149, 237, 251, 218, 79, 153, 225, 201, 198, 214, 290, 247, 105, 166, 202, 173, 310, 183, 376, 110, 113, 461, 183, 173, 213, 93, 182, 320, 365, 329, 359, 410, 337, 309, 161, 338, 285, 198, 500, 312, 318, 152, 241, 500, 270, 460, 298, 331, 337, 447, 238, 500, 262]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 1.3977400524836368, 'mean_inference_ms': 4.257181751063326, 'mean_action_processing_ms': 0.33503627651918927, 'mean_env_wait_ms': 0.1649991892509428, 'mean_env_render_ms': 0.0}, 'num_faulty_episodes': 0, 'connector_metrics': {'ObsPreprocessorConnector_ms': 0.019086360931396484, 'StateBufferConnector_ms': 0.0055582523345947266, 'ViewRequirementAgentConnector_ms': 0.7802252769470215}}, 'episode_reward_max': 500.0, 'episode_reward_min': 30.0, 'episode_reward_mean': 214.73, 'episode_len_mean': 214.73, 'episodes_this_iter': 12, 'policy_reward_min': {}, 'policy_reward_max': {}, 'policy_reward_mean': {}, 'hist_stats': {'episode_reward': [181.0, 116.0, 182.0, 47.0, 42.0, 128.0, 46.0, 80.0, 144.0, 58.0, 62.0, 30.0, 156.0, 63.0, 101.0, 124.0, 109.0, 124.0, 218.0, 191.0, 44.0, 112.0, 154.0, 151.0, 63.0, 327.0, 70.0, 279.0, 234.0, 101.0, 87.0, 250.0, 188.0, 116.0, 182.0, 175.0, 94.0, 202.0, 266.0, 119.0, 149.0, 403.0, 374.0, 173.0, 347.0, 327.0, 202.0, 149.0, 237.0, 251.0, 218.0, 79.0, 153.0, 225.0, 201.0, 198.0, 214.0, 290.0, 247.0, 105.0, 166.0, 202.0, 173.0, 310.0, 183.0, 376.0, 110.0, 113.0, 461.0, 183.0, 173.0, 213.0, 93.0, 182.0, 320.0, 365.0, 329.0, 359.0, 410.0, 337.0, 309.0, 161.0, 338.0, 285.0, 198.0, 500.0, 312.0, 318.0, 152.0, 241.0, 500.0, 270.0, 460.0, 298.0, 331.0, 337.0, 447.0, 238.0, 500.0, 262.0], 'episode_lengths': [181, 116, 182, 47, 42, 128, 46, 80, 144, 58, 62, 30, 156, 63, 101, 124, 109, 124, 218, 191, 44, 112, 154, 151, 63, 327, 70, 279, 234, 101, 87, 250, 188, 116, 182, 175, 94, 202, 266, 119, 149, 403, 374, 173, 347, 327, 202, 149, 237, 251, 218, 79, 153, 225, 201, 198, 214, 290, 247, 105, 166, 202, 173, 310, 183, 376, 110, 113, 461, 183, 173, 213, 93, 182, 320, 365, 329, 359, 410, 337, 309, 161, 338, 285, 198, 500, 312, 318, 152, 241, 500, 270, 460, 298, 331, 337, 447, 238, 500, 262]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 1.3977400524836368, 'mean_inference_ms': 4.257181751063326, 'mean_action_processing_ms': 0.33503627651918927, 'mean_env_wait_ms': 0.1649991892509428, 'mean_env_render_ms': 0.0}, 'num_faulty_episodes': 0, 'connector_metrics': {'ObsPreprocessorConnector_ms': 0.019086360931396484, 'StateBufferConnector_ms': 0.0055582523345947266, 'ViewRequirementAgentConnector_ms': 0.7802252769470215}, 'num_healthy_workers': 2, 'num_in_flight_async_reqs': 0, 'num_remote_worker_restarts': 0, 'num_agent_steps_sampled': 36000, 'num_agent_steps_trained': 0, 'num_env_steps_sampled': 36000, 'num_env_steps_trained': 0, 'num_env_steps_sampled_this_iter': 4000, 'num_env_steps_trained_this_iter': 0, 'num_env_steps_sampled_throughput_per_sec': 229.9821593057696, 'num_env_steps_trained_throughput_per_sec': 0.0, 'timesteps_total': 36000, 'num_steps_trained_this_iter': 0, 'agent_timesteps_total': 36000, 'timers': {'training_iteration_time_ms': 17761.352, 'sample_time_ms': 12421.725, 'synch_weights_time_ms': 10.479}, 'counters': {'num_env_steps_sampled': 36000, 'num_env_steps_trained': 0, 'num_agent_steps_sampled': 36000, 'num_agent_steps_trained': 0}, 'done': False, 'episodes_total': 488, 'training_iteration': 9, 'trial_id': 'default', 'date': '2023-12-03_18-42-09', 'timestamp': 1701646929, 'time_this_iter_s': 17.399669885635376, 'time_total_s': 159.91980457305908, 'pid': 817316, 'hostname': 'udc-aw34-10c0', 'node_ip': '10.250.132.124', 'config': {'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'num_learner_workers': 0, 'num_gpus_per_learner_worker': 0, 'num_cpus_per_learner_worker': 1, 'local_gpu_idx': 0, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'env': 'CartPole-v1', 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'auto_wrap_old_gym_envs': True, 'action_mask_key': 'action_mask', '_is_atari': None, 'env_runner_cls': None, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': True, 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 5e-05, 'grad_clip': None, 'grad_clip_by': 'global_norm', 'train_batch_size': 4000, 'model': {'_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [64, 64], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'encoder_latent_dim': None, 'always_check_shapes': False, 'lstm_use_prev_action_reward': -1, '_use_default_native_models': -1}, 'optimizer': {}, 'max_requests_in_flight_per_sampler_worker': 2, '_learner_class': None, '_enable_learner_api': True, 'explore': True, 'exploration_config': {}, 'algorithm_config_overrides_per_module': {}, 'policy_map_capacity': 100, 'policy_mapping_fn': <function AlgorithmConfig.DEFAULT_POLICY_MAPPING_FN at 0x7fc583815a60>, 'policies_to_train': None, 'policy_states_are_swappable': False, 'observation_fn': None, 'count_steps_by': 'env_steps', 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'offline_sampling': False, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': None, 'off_policy_estimation_methods': {}, 'ope_split_batch_by_episode': True, 'evaluation_num_workers': 1, 'always_attach_evaluation_results': False, 'enable_async_evaluation': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'export_native_model_files': False, 'checkpoint_trainable_policies_only': False, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'max_num_worker_restarts': 1000, 'delay_between_worker_restarts_s': 60.0, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'worker_health_probe_timeout_s': 60, 'worker_restore_timeout_s': 1800, 'rl_module_spec': SingleAgentRLModuleSpec(module_class=<class 'ray.rllib.algorithms.ppo.torch.ppo_torch_rl_module.PPOTorchRLModule'>, observation_space=None, action_space=None, model_config_dict=None, catalog_class=<class 'ray.rllib.algorithms.ppo.ppo_catalog.PPOCatalog'>, load_state_path=None), '_enable_rl_module_api': True, '_AlgorithmConfig__prior_exploration_config': {'type': 'StochasticSampling'}, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, '_disable_initialize_loss_from_dummy_batch': False, 'simple_optimizer': False, 'policy_map_cache': -1, 'worker_cls': -1, 'synchronize_filters': -1, 'replay_sequence_length': None, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'use_kl_loss': True, 'kl_coeff': 0.2, 'kl_target': 0.01, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'policies': {'default_policy': (None, None, None, None)}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'torch', 'num_cpus_for_driver': 1, 'num_workers': 2}, 'time_since_restore': 159.91980457305908, 'iterations_since_restore': 9, 'perf': {'cpu_util_percent': 47.552, 'ram_util_percent': 23.844}}\n",
      "{'custom_metrics': {}, 'episode_media': {}, 'info': {'learner': {'__all__': {'num_agent_steps_trained': 128.0, 'num_env_steps_trained': 4000.0, 'total_loss': 9.874682309530945}, 'default_policy': {'total_loss': 9.874682309530945, 'policy_loss': -0.005450161245427152, 'vf_loss': 9.879820536957112, 'vf_loss_unclipped': 5133.115044372168, 'vf_explained_var': -0.01655502874713971, 'entropy': 0.5501277061667778, 'mean_kl_loss': 0.0062387635663019555, 'default_optimizer_lr': 5.000000000000001e-05, 'curr_lr': 5e-05, 'curr_entropy_coeff': 0.0, 'curr_kl_coeff': 0.05000000074505806}}, 'num_env_steps_sampled': 40000, 'num_env_steps_trained': 0, 'num_agent_steps_sampled': 40000, 'num_agent_steps_trained': 0}, 'sampler_results': {'episode_reward_max': 500.0, 'episode_reward_min': 30.0, 'episode_reward_mean': 244.11, 'episode_len_mean': 244.11, 'episode_media': {}, 'episodes_this_iter': 10, 'policy_reward_min': {}, 'policy_reward_max': {}, 'policy_reward_mean': {}, 'custom_metrics': {}, 'hist_stats': {'episode_reward': [62.0, 30.0, 156.0, 63.0, 101.0, 124.0, 109.0, 124.0, 218.0, 191.0, 44.0, 112.0, 154.0, 151.0, 63.0, 327.0, 70.0, 279.0, 234.0, 101.0, 87.0, 250.0, 188.0, 116.0, 182.0, 175.0, 94.0, 202.0, 266.0, 119.0, 149.0, 403.0, 374.0, 173.0, 347.0, 327.0, 202.0, 149.0, 237.0, 251.0, 218.0, 79.0, 153.0, 225.0, 201.0, 198.0, 214.0, 290.0, 247.0, 105.0, 166.0, 202.0, 173.0, 310.0, 183.0, 376.0, 110.0, 113.0, 461.0, 183.0, 173.0, 213.0, 93.0, 182.0, 320.0, 365.0, 329.0, 359.0, 410.0, 337.0, 309.0, 161.0, 338.0, 285.0, 198.0, 500.0, 312.0, 318.0, 152.0, 241.0, 500.0, 270.0, 460.0, 298.0, 331.0, 337.0, 447.0, 238.0, 500.0, 262.0, 128.0, 500.0, 272.0, 500.0, 500.0, 272.0, 500.0, 500.0, 290.0, 500.0], 'episode_lengths': [62, 30, 156, 63, 101, 124, 109, 124, 218, 191, 44, 112, 154, 151, 63, 327, 70, 279, 234, 101, 87, 250, 188, 116, 182, 175, 94, 202, 266, 119, 149, 403, 374, 173, 347, 327, 202, 149, 237, 251, 218, 79, 153, 225, 201, 198, 214, 290, 247, 105, 166, 202, 173, 310, 183, 376, 110, 113, 461, 183, 173, 213, 93, 182, 320, 365, 329, 359, 410, 337, 309, 161, 338, 285, 198, 500, 312, 318, 152, 241, 500, 270, 460, 298, 331, 337, 447, 238, 500, 262, 128, 500, 272, 500, 500, 272, 500, 500, 290, 500]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 1.3929060490196037, 'mean_inference_ms': 4.257442864586644, 'mean_action_processing_ms': 0.336107069272602, 'mean_env_wait_ms': 0.16470233461078088, 'mean_env_render_ms': 0.0}, 'num_faulty_episodes': 0, 'connector_metrics': {'ObsPreprocessorConnector_ms': 0.011066913604736328, 'StateBufferConnector_ms': 0.005754709243774414, 'ViewRequirementAgentConnector_ms': 0.7847051620483398}}, 'episode_reward_max': 500.0, 'episode_reward_min': 30.0, 'episode_reward_mean': 244.11, 'episode_len_mean': 244.11, 'episodes_this_iter': 10, 'policy_reward_min': {}, 'policy_reward_max': {}, 'policy_reward_mean': {}, 'hist_stats': {'episode_reward': [62.0, 30.0, 156.0, 63.0, 101.0, 124.0, 109.0, 124.0, 218.0, 191.0, 44.0, 112.0, 154.0, 151.0, 63.0, 327.0, 70.0, 279.0, 234.0, 101.0, 87.0, 250.0, 188.0, 116.0, 182.0, 175.0, 94.0, 202.0, 266.0, 119.0, 149.0, 403.0, 374.0, 173.0, 347.0, 327.0, 202.0, 149.0, 237.0, 251.0, 218.0, 79.0, 153.0, 225.0, 201.0, 198.0, 214.0, 290.0, 247.0, 105.0, 166.0, 202.0, 173.0, 310.0, 183.0, 376.0, 110.0, 113.0, 461.0, 183.0, 173.0, 213.0, 93.0, 182.0, 320.0, 365.0, 329.0, 359.0, 410.0, 337.0, 309.0, 161.0, 338.0, 285.0, 198.0, 500.0, 312.0, 318.0, 152.0, 241.0, 500.0, 270.0, 460.0, 298.0, 331.0, 337.0, 447.0, 238.0, 500.0, 262.0, 128.0, 500.0, 272.0, 500.0, 500.0, 272.0, 500.0, 500.0, 290.0, 500.0], 'episode_lengths': [62, 30, 156, 63, 101, 124, 109, 124, 218, 191, 44, 112, 154, 151, 63, 327, 70, 279, 234, 101, 87, 250, 188, 116, 182, 175, 94, 202, 266, 119, 149, 403, 374, 173, 347, 327, 202, 149, 237, 251, 218, 79, 153, 225, 201, 198, 214, 290, 247, 105, 166, 202, 173, 310, 183, 376, 110, 113, 461, 183, 173, 213, 93, 182, 320, 365, 329, 359, 410, 337, 309, 161, 338, 285, 198, 500, 312, 318, 152, 241, 500, 270, 460, 298, 331, 337, 447, 238, 500, 262, 128, 500, 272, 500, 500, 272, 500, 500, 290, 500]}, 'sampler_perf': {'mean_raw_obs_processing_ms': 1.3929060490196037, 'mean_inference_ms': 4.257442864586644, 'mean_action_processing_ms': 0.336107069272602, 'mean_env_wait_ms': 0.16470233461078088, 'mean_env_render_ms': 0.0}, 'num_faulty_episodes': 0, 'connector_metrics': {'ObsPreprocessorConnector_ms': 0.011066913604736328, 'StateBufferConnector_ms': 0.005754709243774414, 'ViewRequirementAgentConnector_ms': 0.7847051620483398}, 'num_healthy_workers': 2, 'num_in_flight_async_reqs': 0, 'num_remote_worker_restarts': 0, 'num_agent_steps_sampled': 40000, 'num_agent_steps_trained': 0, 'num_env_steps_sampled': 40000, 'num_env_steps_trained': 0, 'num_env_steps_sampled_this_iter': 4000, 'num_env_steps_trained_this_iter': 0, 'num_env_steps_sampled_throughput_per_sec': 223.08020856860034, 'num_env_steps_trained_throughput_per_sec': 0.0, 'timesteps_total': 40000, 'num_steps_trained_this_iter': 0, 'agent_timesteps_total': 40000, 'timers': {'training_iteration_time_ms': 17778.291, 'sample_time_ms': 12433.254, 'synch_weights_time_ms': 11.04}, 'counters': {'num_env_steps_sampled': 40000, 'num_env_steps_trained': 0, 'num_agent_steps_sampled': 40000, 'num_agent_steps_trained': 0}, 'done': False, 'episodes_total': 498, 'training_iteration': 10, 'trial_id': 'default', 'date': '2023-12-03_18-42-27', 'timestamp': 1701646947, 'time_this_iter_s': 17.935551643371582, 'time_total_s': 177.85535621643066, 'pid': 817316, 'hostname': 'udc-aw34-10c0', 'node_ip': '10.250.132.124', 'config': {'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 0, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'num_learner_workers': 0, 'num_gpus_per_learner_worker': 0, 'num_cpus_per_learner_worker': 1, 'local_gpu_idx': 0, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': True, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'torch_compile_learner': False, 'torch_compile_learner_what_to_compile': <TorchCompileWhatToCompile.FORWARD_TRAIN: 'forward_train'>, 'torch_compile_learner_dynamo_backend': 'inductor', 'torch_compile_learner_dynamo_mode': None, 'torch_compile_worker': False, 'torch_compile_worker_dynamo_backend': 'onnxrt', 'torch_compile_worker_dynamo_mode': None, 'env': 'CartPole-v1', 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'auto_wrap_old_gym_envs': True, 'action_mask_key': 'action_mask', '_is_atari': None, 'env_runner_cls': None, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': True, 'update_worker_filter_stats': True, 'use_worker_filter_stats': True, 'rollout_fragment_length': 'auto', 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'gamma': 0.99, 'lr': 5e-05, 'grad_clip': None, 'grad_clip_by': 'global_norm', 'train_batch_size': 4000, 'model': {'_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [64, 64], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': False, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'encoder_latent_dim': None, 'always_check_shapes': False, 'lstm_use_prev_action_reward': -1, '_use_default_native_models': -1}, 'optimizer': {}, 'max_requests_in_flight_per_sampler_worker': 2, '_learner_class': None, '_enable_learner_api': True, 'explore': True, 'exploration_config': {}, 'algorithm_config_overrides_per_module': {}, 'policy_map_capacity': 100, 'policy_mapping_fn': <function AlgorithmConfig.DEFAULT_POLICY_MAPPING_FN at 0x7fc583815a60>, 'policies_to_train': None, 'policy_states_are_swappable': False, 'observation_fn': None, 'count_steps_by': 'env_steps', 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'offline_sampling': False, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': None, 'off_policy_estimation_methods': {}, 'ope_split_batch_by_episode': True, 'evaluation_num_workers': 1, 'always_attach_evaluation_results': False, 'enable_async_evaluation': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': None, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'export_native_model_files': False, 'checkpoint_trainable_policies_only': False, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'max_num_worker_restarts': 1000, 'delay_between_worker_restarts_s': 60.0, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'worker_health_probe_timeout_s': 60, 'worker_restore_timeout_s': 1800, 'rl_module_spec': SingleAgentRLModuleSpec(module_class=<class 'ray.rllib.algorithms.ppo.torch.ppo_torch_rl_module.PPOTorchRLModule'>, observation_space=None, action_space=None, model_config_dict=None, catalog_class=<class 'ray.rllib.algorithms.ppo.ppo_catalog.PPOCatalog'>, load_state_path=None), '_enable_rl_module_api': True, '_AlgorithmConfig__prior_exploration_config': {'type': 'StochasticSampling'}, '_tf_policy_handles_more_than_one_loss': False, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, '_disable_initialize_loss_from_dummy_batch': False, 'simple_optimizer': False, 'policy_map_cache': -1, 'worker_cls': -1, 'synchronize_filters': -1, 'replay_sequence_length': None, 'lr_schedule': None, 'use_critic': True, 'use_gae': True, 'use_kl_loss': True, 'kl_coeff': 0.2, 'kl_target': 0.01, 'sgd_minibatch_size': 128, 'num_sgd_iter': 30, 'shuffle_sequences': True, 'vf_loss_coeff': 1.0, 'entropy_coeff': 0.0, 'entropy_coeff_schedule': None, 'clip_param': 0.3, 'vf_clip_param': 10.0, 'vf_share_layers': -1, 'lambda': 1.0, 'input': 'sampler', 'policies': {'default_policy': (None, None, None, None)}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'torch', 'num_cpus_for_driver': 1, 'num_workers': 2}, 'time_since_restore': 177.85535621643066, 'iterations_since_restore': 10, 'perf': {'cpu_util_percent': 45.928000000000004, 'ram_util_percent': 23.855999999999995}}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'evaluation': {'sampler_results': {'episode_reward_max': 500.0,\n",
       "   'episode_reward_min': 230.0,\n",
       "   'episode_reward_mean': 364.9,\n",
       "   'episode_len_mean': 364.9,\n",
       "   'episode_media': {},\n",
       "   'episodes_this_iter': 10,\n",
       "   'policy_reward_min': {},\n",
       "   'policy_reward_max': {},\n",
       "   'policy_reward_mean': {},\n",
       "   'custom_metrics': {},\n",
       "   'hist_stats': {'episode_reward': [408.0,\n",
       "     351.0,\n",
       "     426.0,\n",
       "     361.0,\n",
       "     342.0,\n",
       "     230.0,\n",
       "     326.0,\n",
       "     380.0,\n",
       "     500.0,\n",
       "     325.0],\n",
       "    'episode_lengths': [408, 351, 426, 361, 342, 230, 326, 380, 500, 325]},\n",
       "   'sampler_perf': {'mean_raw_obs_processing_ms': 0.6504975932918182,\n",
       "    'mean_inference_ms': 2.163296072450403,\n",
       "    'mean_action_processing_ms': 0.17500374415149428,\n",
       "    'mean_env_wait_ms': 0.08238694439195607,\n",
       "    'mean_env_render_ms': 0.0},\n",
       "   'num_faulty_episodes': 0,\n",
       "   'connector_metrics': {'ObsPreprocessorConnector_ms': 0.005948543548583984,\n",
       "    'StateBufferConnector_ms': 0.004074573516845703,\n",
       "    'ViewRequirementAgentConnector_ms': 0.3033614158630371}},\n",
       "  'episode_reward_max': 500.0,\n",
       "  'episode_reward_min': 230.0,\n",
       "  'episode_reward_mean': 364.9,\n",
       "  'episode_len_mean': 364.9,\n",
       "  'episode_media': {},\n",
       "  'episodes_this_iter': 10,\n",
       "  'policy_reward_min': {},\n",
       "  'policy_reward_max': {},\n",
       "  'policy_reward_mean': {},\n",
       "  'custom_metrics': {},\n",
       "  'hist_stats': {'episode_reward': [408.0,\n",
       "    351.0,\n",
       "    426.0,\n",
       "    361.0,\n",
       "    342.0,\n",
       "    230.0,\n",
       "    326.0,\n",
       "    380.0,\n",
       "    500.0,\n",
       "    325.0],\n",
       "   'episode_lengths': [408, 351, 426, 361, 342, 230, 326, 380, 500, 325]},\n",
       "  'sampler_perf': {'mean_raw_obs_processing_ms': 0.6504975932918182,\n",
       "   'mean_inference_ms': 2.163296072450403,\n",
       "   'mean_action_processing_ms': 0.17500374415149428,\n",
       "   'mean_env_wait_ms': 0.08238694439195607,\n",
       "   'mean_env_render_ms': 0.0},\n",
       "  'num_faulty_episodes': 0,\n",
       "  'connector_metrics': {'ObsPreprocessorConnector_ms': 0.005948543548583984,\n",
       "   'StateBufferConnector_ms': 0.004074573516845703,\n",
       "   'ViewRequirementAgentConnector_ms': 0.3033614158630371},\n",
       "  'num_agent_steps_sampled_this_iter': 3649,\n",
       "  'num_env_steps_sampled_this_iter': 3649,\n",
       "  'timesteps_this_iter': 3649}}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ray.rllib.algorithms.ppo import PPOConfig\n",
    "\n",
    "config = (  # 1. Configure the algorithm,\n",
    "    PPOConfig()\n",
    "    .environment(\"CartPole-v1\")\n",
    "    .rollouts(num_rollout_workers=2)\n",
    "    .framework(\"torch\")\n",
    "    .training(model={\"fcnet_hiddens\": [64, 64]})\n",
    "    .evaluation(evaluation_num_workers=1)\n",
    ")\n",
    "\n",
    "algo = config.build()  # 2. build the algorithm,\n",
    "\n",
    "for _ in range(10):\n",
    "    print(algo.train())  # 3. train it,\n",
    "\n",
    "algo.evaluate()  # 4. and evaluate it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch 1.12.0",
   "language": "python",
   "name": "pytorch-1.12.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}


## INTRODUCTION TO THE MODULE

We continue our study of Policy Gradient methods, looking to more advanced approaches. These can bring efficiency (PG requires large data samples) and ease of implementation. We consider the Actor-Critic class of models, in addition to trust region policy methods. The latter methods are modern and powerful.

## LEARNING OUTCOMES

At the conclusion of this module, you should be able to:

- Explain how Actor-Critic Methods work and explain their benefits
- Explain the mechanics, benefits, and challenges of Trust Region Policy Optimization (TRPO)
- Explain the mechanics and benefits of Proximal Policy Optimization (PPO)



{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model-Based RL\n",
    "\n",
    "### University of Virginia\n",
    "### Reinforcement Learning\n",
    "#### Last updated: December 6, 2023\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### SOURCES \n",
    "\n",
    "- Reinforcement Learning, RS Sutton & AG Barto, 2nd edition. Chapter 8\n",
    "\n",
    "### LEARNING OUTCOMES\n",
    "\n",
    "- Explain how methods like Dyna can be used\n",
    "- Understand how to apply rollout algorithms to improve a policy\n",
    "- Understand how Monte Carlo Tree Search works\n",
    "\n",
    "### CONCEPTS\n",
    "\n",
    "- Dyna and Dyna-style methods\n",
    "- Using an environment to simulate future states, rewards\n",
    "- Distribution model vs sample model\n",
    "- Rollout algorithms\n",
    "- Monte Carlo Tree Search\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![tahiti](images/tahiti.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Models are used all the time, not just by computers but also by humans\n",
    "\n",
    "When we see this beautiful photo of Tahiti, we think of what a vacation might be like.\n",
    "\n",
    "We might think of snorkeling or sunbathing. Models - simplified versions of a situation - help us plan."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I. Working with Models for Planning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model: anything an agent can use to predict how environment will respond to actions  \n",
    "`f: (state, action) -> (next_state, reward)`\n",
    "\n",
    "Model-based approaches rely on **planning**: the model generates simulated experience   \n",
    "Model-free approaches rely on **learning**: real experience is generated by the environment\n",
    "\n",
    "*Distribution models* produces all possibilities with their probabilities. This is the model assumed in MDPs.  \n",
    "\n",
    "*Sample models* produce one of the possibilities. Example: simulate drawn card from a deck.\n",
    "\n",
    "Distribution models are more general. Given this model, can always simulate a result or trajectory.\n",
    "\n",
    "**Using a Model**  \n",
    "\n",
    "The main benefit to a model:\n",
    "\n",
    "- Agent can plan ahead,\n",
    "- See what would happen for different choices,\n",
    "- Decide between its options,\n",
    "- The results can be used in a learned policy\n",
    "\n",
    "**Real experience**  \n",
    "\n",
    "Can be used to 1) update model 2) improve value function, policy\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II. Dyna\n",
    "\n",
    "Integrating planning, acting, and learning\n",
    "\n",
    "[fill this in]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### III. Rollout Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Made at *decision time*, when we encounter the new state and need to select an action\n",
    "\n",
    "Based on Monte Carlo control, they work like this:\n",
    "\n",
    "- Decide on a number of trajectories to simulate, and number of time points per trajectory\n",
    "- Simulate multiple trajectories from the given state, taking each possible action and then following the policy\n",
    "- For each of the starting actions, compute the return over the trajectories\n",
    "- Average the returns given each action to estimate the action values\n",
    "\n",
    "The algorithm uses an important heuristic: the computation should be devoted to imminent events.  \n",
    "Oftentimes, exhaustive search is expensive and unnecessary, as many (state,action) pairs may not be valuable.\n",
    "\n",
    "This approach doesn't find optimal policy; it's simpler and can improve current policy.  \n",
    "Can be surprisingly effective.  \n",
    "However, can be computationally expensive as it's done in real time and may require many trajectories.\n",
    "\n",
    "Can be parallelized on multiple machines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![rollout](images/rollout.png)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IV. Monte Carlo Tree Search (MCTS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Successful example of rollout algorithm. Helped computer Go improve from amateur to grandmaster.\n",
    "\n",
    "Similar to mentioned rollout algo, but accumulates value estimates from Monte Carlo simulation.\n",
    "\n",
    "Can be effective if there is fast environment model simulator.\n",
    "\n",
    "Uses simple policy called *rollout policy*\n",
    "\n",
    "Core idea:  \n",
    "**successively focus multiple simulations, starting from current state and extending initial portions of promising trajectories**\n",
    "\n",
    "MC estimates maintained for subset of state-action pairs most likely reached in a few steps. This forms a tree rooted at current state.\n",
    "\n",
    "Based on value estimates for some of the actions, make selections. This is *tree policy*.\n",
    "\n",
    "Steps:\n",
    "- **Selection** - Starting at root, follow tree policy to select a leaf node\n",
    "- **Expansion** - On some iterations, expand tree from selected leaf node by adding child node via unexplored actions\n",
    "- **Simulation** - From selected node or new child node, simulate complete episode, selecting actions from rollout policy\n",
    "- **Backup** - Return from simulated episode is backed up to update action values attached to edges  in tree policy.  \n",
    "              Only values in the tree are saved.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![mcts](images/mcts.png)\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

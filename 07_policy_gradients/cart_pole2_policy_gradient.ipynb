{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-GBod1nv65dN"
   },
   "source": [
    "## Cart Pole Demo 2 using OpenAI gym  \n",
    "## Policy Gradient\n",
    "\n",
    "### University of Virginia\n",
    "### Reinforcement Learning\n",
    "#### Last updated: October 12, 2023\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NGcrQ7zURq9_"
   },
   "source": [
    "**Source:** Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow, 2nd Edition. Aurélien Géron.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G6__XiTWDIiT"
   },
   "source": [
    "### First a Refresh\n",
    "\n",
    "Let's briefly review the content from Demo 1: Basics and Simple Policy\n",
    "\n",
    "We will work with [Gym](https://gym.openai.com/) from Open AI.\n",
    "\n",
    "We revisit the [**CartPole**](https://gym.openai.com/videos/2019-10-21--mqt8Qj1mwo/CartPole-v1/original.mp4) problem.\n",
    "\n",
    "The *simple policy* didn't perform very well: the average reward was about 42.\n",
    "\n",
    "We want to see if we can do better using a Policy Gradient algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rA_dd0h6Eoap"
   },
   "source": [
    "### Setup and First Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 551,
     "status": "ok",
     "timestamp": 1647113952405,
     "user": {
      "displayName": "Adam Tashman",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "12607898702703460737"
     },
     "user_tz": 300
    },
    "id": "ed_DyvHy6pbh"
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KA4JgtuL5jC9"
   },
   "source": [
    "Load the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 318,
     "status": "ok",
     "timestamp": 1647113952718,
     "user": {
      "displayName": "Adam Tashman",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "12607898702703460737"
     },
     "user_tz": 300
    },
    "id": "UcGLRi6F6vcQ"
   },
   "outputs": [],
   "source": [
    "env = gym.make(\"CartPole-v1\")\n",
    "state = env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VtQFJsxj7aBj"
   },
   "source": [
    "Given the state, we take an action. The next state comes from the environment, which is encoded in `gym`.\n",
    "\n",
    "Components:   \n",
    "[0]: cart horizontal position (0.0 = center)  \n",
    "[1]: velocity (positive means right)  \n",
    "[2]: angle of the pole (0.0 = vertical)  \n",
    "[3]: pole's angular velocity (positive means clockwise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 200,
     "status": "ok",
     "timestamp": 1646928493600,
     "user": {
      "displayName": "Adam Tashman",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "12607898702703460737"
     },
     "user_tz": 300
    },
    "id": "M30S6Bf_6yNv",
    "outputId": "daac7f13-4719-4ad3-e790-584874e0a987"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.02744055,  0.04389752, -0.01207079, -0.019533  ])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 182,
     "status": "ok",
     "timestamp": 1646928494836,
     "user": {
      "displayName": "Adam Tashman",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "12607898702703460737"
     },
     "user_tz": 300
    },
    "id": "7A5peNKiBXsh",
    "outputId": "5f78c93e-5a90-4b10-a475-06ee7ffe2207"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# state space number of components\n",
    "env.observation_space.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8GkZmsIN7ky6"
   },
   "source": [
    "The action space consists of two options: \n",
    "\n",
    "[0]: move cart left   \n",
    "[1]: move cart right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1647113953248,
     "user": {
      "displayName": "Adam Tashman",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "12607898702703460737"
     },
     "user_tz": 300
    },
    "id": "ySQ8EpsV7Ngr",
    "outputId": "7ae1c393-4b03-4f61-f9a0-27c1266c4a0a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discrete(2)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_68lpUuVCc-_"
   },
   "source": [
    "Let's take an action, draw a sample and look at the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1647113955823,
     "user": {
      "displayName": "Adam Tashman",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "12607898702703460737"
     },
     "user_tz": 300
    },
    "id": "gKFEIHjL7n7h",
    "outputId": "85b697de-14ef-4e50-96e8-d6715d6259a2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state [ 0.02822481 -0.01191032 -0.01980916  0.04693194]\n",
      "reward 1.0\n",
      "done False\n",
      "info {}\n"
     ]
    }
   ],
   "source": [
    "# move right\n",
    "action = 1\n",
    "\n",
    "# take a step and get next state, reward from environment\n",
    "staticmethod, reward, done, info = env.step(action)\n",
    "\n",
    "print('state', state)\n",
    "print('reward', reward)\n",
    "print('done', done)\n",
    "print('info', info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bqxI0vej88Kh"
   },
   "source": [
    "**Reward and Episode**  \n",
    "\n",
    "For each time step that the cart keeps the pole balanced, it earns reward 1.\n",
    "\n",
    "If the pole tilts too much or if the cart moves off screen, `reward=0` and `done=True` (the episode will end).\n",
    "\n",
    "When the episode ends, a new episode may begin. The process learns cumulatively from each episode."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Yv5gKVde9pvQ"
   },
   "source": [
    "**Simple policy**:  \n",
    "\n",
    "When the pole leans left (negative angle), move left. When the pole leans right (positive angle), move right.\n",
    "\n",
    "Run many episodes and visualize their reward distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 1053,
     "status": "ok",
     "timestamp": 1647113964731,
     "user": {
      "displayName": "Adam Tashman",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "12607898702703460737"
     },
     "user_tz": 300
    },
    "id": "dREsiRKf9CbR"
   },
   "outputs": [],
   "source": [
    "def simple_policy(obs):\n",
    "    angle = obs[2]\n",
    "    return 0 if angle < 0 else 1\n",
    "\n",
    "num_episodes = 1000\n",
    "num_steps = 100\n",
    "rewards = []\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    ep_reward = 0\n",
    "    obs = env.reset()\n",
    "    for step in range(num_steps):\n",
    "        action = simple_policy(obs)\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        ep_reward += reward\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    rewards.append(ep_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "executionInfo": {
     "elapsed": 662,
     "status": "ok",
     "timestamp": 1647113967867,
     "user": {
      "displayName": "Adam Tashman",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "12607898702703460737"
     },
     "user_tz": 300
    },
    "id": "nzdjBfiA-yab",
    "outputId": "f47d4b69-6033-4c3c-f38b-c4a7d95ecdae"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.boxplot(rewards, vert=False)\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2so9LpTSEb33"
   },
   "source": [
    "### Neural Network Policy\n",
    "\n",
    "Now we try a more sophisticated policy: let's use a neural network.\n",
    "\n",
    "The network will take **state as input**. The output node will contain the probability of the actions.\n",
    "\n",
    "Since there are two actions (left, right), we require one output node.  \n",
    "Node will output probability of right (so prob of left is implied).\n",
    "\n",
    "For simplicity, we will use one hidden layer. \n",
    "\n",
    "Number of nodes in hidden layer is a hyperparameter.\n",
    "\n",
    "**Instantiate the Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 3668,
     "status": "ok",
     "timestamp": 1647114018722,
     "user": {
      "displayName": "Adam Tashman",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "12607898702703460737"
     },
     "user_tz": 300
    },
    "id": "ZFXOXI_jCtgi"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "num_inputs = env.observation_space.shape[0]\n",
    "num_hidden_neurons = 10\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(num_hidden_neurons, activation=\"relu\", input_shape=[num_inputs]),\n",
    "    keras.layers.Dense(1, activation=\"sigmoid\"),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jsmQEX73IA73"
   },
   "source": [
    "**Training the Model**\n",
    "\n",
    "We'll need some functions to preprocess the rewards and to evolve the system.\n",
    "\n",
    "Game Plan: \n",
    "\n",
    "- write a function to evolve the system one step\n",
    "- write a function that runs multiple episodes, calling the *single step* function\n",
    "- write a function to discount rewards, to adjust for time value of money\n",
    "- normalize the discounted rewards, as neural networks perform better on normalized data\n",
    "- set up the parameters and hyperparameters\n",
    "- code a training loop\n",
    "\n",
    "Here is **code to play out a single step and compute the gradient of the loss**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 236,
     "status": "ok",
     "timestamp": 1647114044245,
     "user": {
      "displayName": "Adam Tashman",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "12607898702703460737"
     },
     "user_tz": 300
    },
    "id": "h5F4i77ECzUo"
   },
   "outputs": [],
   "source": [
    "def play_single_step(env, state, model, loss_fn):\n",
    "    with tf.GradientTape() as tape:\n",
    "        left_prob = model(state[np.newaxis])\n",
    "\n",
    "        # want action=left w probability=left_prob, so sample from uniform\n",
    "        action = (tf.random.uniform([1, 1]) > left_prob) # go right if uniform > left_prob\n",
    "\n",
    "        #  y_target=1 if action=left, target=0 if action=right\n",
    "        y_target = tf.constant([[1.]]) - tf.cast(action, tf.float32)\n",
    "\n",
    "        # compute the loss function\n",
    "        loss = tf.reduce_mean(loss_fn(y_target, left_prob))\n",
    "\n",
    "    # compute the gradient of the loss wrt model parameters\n",
    "    grads = tape.gradient(loss, model.trainable_variables)\n",
    "\n",
    "    # take the selected action and get the results\n",
    "    state, reward, done, info = env.step(int(action[0, 0].numpy()))\n",
    "    return state, reward, done, grads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cCxzETZgPdDX"
   },
   "source": [
    "Next, define a function to play out several episodes.\n",
    "\n",
    "This will use `play_single_step()` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionInfo": {
     "elapsed": 334,
     "status": "ok",
     "timestamp": 1647114078383,
     "user": {
      "displayName": "Adam Tashman",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "12607898702703460737"
     },
     "user_tz": 300
    },
    "id": "IFipt4rOmunn"
   },
   "outputs": [],
   "source": [
    "def play_multiple_episodes(env, n_episodes, n_max_steps, model, loss_fn):\n",
    "    all_rewards = []\n",
    "    all_grads = []\n",
    "    for episode in range(n_episodes):\n",
    "        # for each episode, reset rewards, grads, state\n",
    "        current_rewards = []\n",
    "        current_grads = []\n",
    "        obs = env.reset()\n",
    "        for step in range(n_max_steps):\n",
    "          # take a step\n",
    "            obs, reward, done, grads = play_single_step(env, obs, model, loss_fn)\n",
    "            current_rewards.append(reward)\n",
    "            current_grads.append(grads)\n",
    "            if done:\n",
    "                break\n",
    "        # append rewards, grads for episode\n",
    "        all_rewards.append(current_rewards)\n",
    "        all_grads.append(current_grads)\n",
    "    return all_rewards, all_grads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CVQBHNWZRiwv"
   },
   "source": [
    "**Discounting** \n",
    "\n",
    "Rewards need to be adjusted for time value of money, so we apply a discount factor.\n",
    "\n",
    "We define a discounting function to:\n",
    "\n",
    "- take a list of rewards as input\n",
    "- return an array containing the sum of future discounted rewards at each time step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "executionInfo": {
     "elapsed": 216,
     "status": "ok",
     "timestamp": 1647114167297,
     "user": {
      "displayName": "Adam Tashman",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "12607898702703460737"
     },
     "user_tz": 300
    },
    "id": "8klyZXBOMpkH"
   },
   "outputs": [],
   "source": [
    "def discount_rewards(rewards, disc_factor):\n",
    "    disc_rewards = np.array(rewards, float)\n",
    "    for step in range(len(rewards) - 2, -1, -1):\n",
    "        disc_rewards[step] += disc_rewards[step + 1] * disc_factor\n",
    "    return disc_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1647114168129,
     "user": {
      "displayName": "Adam Tashman",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "12607898702703460737"
     },
     "user_tz": 300
    },
    "id": "gBrPkH2eUTas",
    "outputId": "f4a38a76-14f5-4d59-c4fc-7f586ed3f712"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ -21.3625, -127.75  , -145.    , -100.    ])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test the function\n",
    "\n",
    "rewards = [100, 10, -50, -100]\n",
    "disc_factor = 0.95\n",
    "\n",
    "discount_rewards(rewards, disc_factor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tPo0qkAyUTEc"
   },
   "source": [
    "**Normalizing the sum of discounted rewards**\n",
    "\n",
    "We define a function to normalize the sum of future discounted rewards across episodes by:\n",
    "\n",
    "- subtracting the mean (center)\n",
    "- dividing by standard deviation (scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "executionInfo": {
     "elapsed": 229,
     "status": "ok",
     "timestamp": 1647114183455,
     "user": {
      "displayName": "Adam Tashman",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "12607898702703460737"
     },
     "user_tz": 300
    },
    "id": "uI_eJoOySeMP"
   },
   "outputs": [],
   "source": [
    "def discount_and_normalize_rewards(episode_rewards, disc_factor):\n",
    "\n",
    "    # for each episode, compute sum of future discounted reward\n",
    "    all_disc_rewards = [discount_rewards(rewards, disc_factor) for rewards in episode_rewards]\n",
    "    \n",
    "    # concatenate all rewards across episode to compute statistics\n",
    "    flat_disc_rewards = np.concatenate(all_disc_rewards)\n",
    "    reward_mean = flat_disc_rewards.mean()\n",
    "    reward_std = flat_disc_rewards.std()\n",
    "\n",
    "    # normalize each value\n",
    "    return [(disc_rewards - reward_mean) / reward_std for disc_rewards in all_disc_rewards]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 270,
     "status": "ok",
     "timestamp": 1647114186854,
     "user": {
      "displayName": "Adam Tashman",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "12607898702703460737"
     },
     "user_tz": 300
    },
    "id": "YYsPVCfjShMw",
    "outputId": "fcf4b898-2ec2-4444-dcba-5a8c9a6d3aec"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([ 0.34418365, -0.96991597, -1.18298817, -0.62714765]),\n",
       " array([1.51901435, 0.91685379])]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test the function\n",
    "\n",
    "rewards1 = [100, 10, -50, -100]\n",
    "rewards2 = [50, 25]\n",
    "disc_factor = 0.95\n",
    "\n",
    "discount_and_normalize_rewards([rewards1, rewards2], disc_factor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lHxVqyTNZ_lC"
   },
   "source": [
    "Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "executionInfo": {
     "elapsed": 266,
     "status": "ok",
     "timestamp": 1647116372140,
     "user": {
      "displayName": "Adam Tashman",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "12607898702703460737"
     },
     "user_tz": 300
    },
    "id": "s7oDfBd_Zhtc"
   },
   "outputs": [],
   "source": [
    "# params and hyperparams\n",
    "\n",
    "num_iterations = 50\n",
    "num_episodes_per_update = 10\n",
    "num_max_steps = 100\n",
    "discount_factor = 0.95\n",
    "\n",
    "# optimizer and loss function\n",
    "\n",
    "optimizer = keras.optimizers.Adam(learning_rate=0.01)\n",
    "loss_fn = keras.losses.binary_crossentropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "elrLaax9aHca"
   },
   "source": [
    "### Training loop\n",
    "\n",
    "We will use a variant of the REINFORCE algorithm introduced earlier.\n",
    "\n",
    "Recall that REINFORCE policy parameter updates are made by using the product of two terms:\n",
    "\n",
    "- the return on a path $G_t$ \n",
    "- the gradient of the policy taking an action divided by the probability of taking the action\n",
    "\n",
    "Update formula:\n",
    "\n",
    "$\\boldsymbol{\\theta_{t+1}} = \\boldsymbol{\\theta_t} + \\alpha G_t \\frac{\\nabla \\pi(A_t|S_t, \\boldsymbol{\\theta_t})}{\\pi(A_t|S_t, \\boldsymbol{\\theta_t})} $\n",
    "\n",
    "The below code will loop over each policy parameter to: \n",
    "\n",
    "- compute the product of the return and the gradient for each of the episodes and time steps  \n",
    "- average the products and use this average to update the parameters\n",
    "\n",
    "The goal is to improve the policy with these gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "executionInfo": {
     "elapsed": 212418,
     "status": "ok",
     "timestamp": 1647116587399,
     "user": {
      "displayName": "Adam Tashman",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "12607898702703460737"
     },
     "user_tz": 300
    },
    "id": "ib-eH18gaH84"
   },
   "outputs": [],
   "source": [
    "for iteration in range(num_iterations):\n",
    "\n",
    "    rewards_all, grads_all = play_multiple_episodes(\n",
    "                                                    env, \n",
    "                                                    num_episodes_per_update, \n",
    "                                                    num_max_steps, \n",
    "                                                    model, \n",
    "                                                    loss_fn)\n",
    "    \n",
    "    final_rewards_all = discount_and_normalize_rewards(rewards_all,\n",
    "                                                       discount_factor)\n",
    "    grads_mean = []\n",
    "\n",
    "    # loop over policy params (trainable_variables)\n",
    "    for var_ix in range(len(model.trainable_variables)):\n",
    "\n",
    "        # average the return-weighted gradients\n",
    "        mean_grads = tf.reduce_mean(\n",
    "            [return_final * grads_all[episode_index][step][var_ix]\n",
    "             for episode_index, final_rewards in enumerate(final_rewards_all)\n",
    "                 for step, return_final in enumerate(final_rewards)], axis=0)\n",
    "        \n",
    "        grads_mean.append(mean_grads)\n",
    "\n",
    "    optimizer.apply_gradients(zip(grads_mean, model.trainable_variables))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jfyxl6-jnkHC"
   },
   "source": [
    "**Policy Evaluation**\n",
    "\n",
    "Now that the model is trained, we can use it as the policy and evaluate performance.\n",
    "\n",
    "Run several episodes, apply the model as the policy and compute rewards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 127618,
     "status": "ok",
     "timestamp": 1647116739670,
     "user": {
      "displayName": "Adam Tashman",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "12607898702703460737"
     },
     "user_tz": 300
    },
    "id": "xMGlbruXnjTg",
    "outputId": "b422bdf9-24b2-40b7-880c-5f7a78824bb3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 0 of 1000\n",
      "episode 50 of 1000\n",
      "episode 100 of 1000\n",
      "episode 150 of 1000\n",
      "episode 200 of 1000\n",
      "episode 250 of 1000\n",
      "episode 300 of 1000\n",
      "episode 350 of 1000\n",
      "episode 400 of 1000\n",
      "episode 450 of 1000\n",
      "episode 500 of 1000\n",
      "episode 550 of 1000\n",
      "episode 600 of 1000\n",
      "episode 650 of 1000\n",
      "episode 700 of 1000\n",
      "episode 750 of 1000\n",
      "episode 800 of 1000\n",
      "episode 850 of 1000\n",
      "episode 900 of 1000\n",
      "episode 950 of 1000\n"
     ]
    }
   ],
   "source": [
    "num_episodes = 1000\n",
    "num_steps = 100\n",
    "rewards = []\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    if episode % 50 == 0:\n",
    "      print('episode', episode, 'of', num_episodes)\n",
    "    ep_reward = 0\n",
    "    state = env.reset()\n",
    "    for step in range(num_steps): # steps in each episode\n",
    "        left_prob = model(state[np.newaxis]) # apply model as policy\n",
    "\n",
    "        # want action=left w probability=left_prob, so sample from uniform\n",
    "        action = tf.random.uniform([1, 1]) > left_prob\n",
    "\n",
    "        # given action, take a step and collect the results\n",
    "        state, reward, done, info = env.step(int(action[0, 0].numpy()))\n",
    "\n",
    "        ep_reward += reward\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    rewards.append(ep_reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fMhTvTSXshyr"
   },
   "source": [
    "Print reward statistics: minimum, mean, max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 202,
     "status": "ok",
     "timestamp": 1647116766681,
     "user": {
      "displayName": "Adam Tashman",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "12607898702703460737"
     },
     "user_tz": 300
    },
    "id": "d1WaXTeFkLZL",
    "outputId": "b490d8fb-bcfe-4275-ae16-290e9ff32165"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16.0, 98.501, 100.0)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.min(rewards), np.mean(rewards), np.max(rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oabweJ4BufSp"
   },
   "source": [
    "Recall that earlier, the simple policy produced a mean reward of 42, so this is much better on average!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0NeAXVaQwuG6"
   },
   "source": [
    "**Question 1**. See if there are any simple modifications you can make to improve the mean reward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NoB4_RIIw1em"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2**. Is REINFORCE a Monte Carlo method? Explain your answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 3**. Is REINFORCE an on-policy method or an off-policy method? Explain your answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 4**. What is the largest challenge with policy gradient methods?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 5a**. Select a baseline function of your choosing and modify REINFORCE to use this baseline. Explain why you selected this baseline. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 5b**. To understand whether REINFORCE with baseline performs better than REINFORCE, plot total rewards versus episode number for each method. Explain your result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNxQWtLY7/+iWwdMTUmrCZL",
   "collapsed_sections": [],
   "name": "cart_pole2_policy_gradient.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

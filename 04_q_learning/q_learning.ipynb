{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q-Learning\n",
    "\n",
    "### University of Virginia\n",
    "### Reinforcement Learning\n",
    "#### Last updated: November 3, 2023\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### SOURCES \n",
    "\n",
    "- Reinforcement Learning, RS Sutton & AG Barto, 2nd edition. Chapter 6\n",
    "- Mastering Reinforcement Learning with Python, Enes Bilgin. Chapter 5\n",
    "\n",
    "### LEARNING OUTCOMES\n",
    "\n",
    "- Explain how Q-Learning works and how it learns off policy\n",
    "- Use Q-Learning to compute value functions  \n",
    "- Perform sensitivity analysis on a Q-Learning algorithm\n",
    "\n",
    "### CONCEPTS\n",
    "\n",
    "- Q-Learning to act off policy\n",
    "- The Q-Learning algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I. Q-table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We recall the big picture of what we're trying to do:  \n",
    "Given state space $S$ and action space $A$, learn values $Q(S,A)$  \n",
    "These are organized in an array called the *Q-table*.\n",
    "\n",
    "Q-Learning is a method for building this table.\n",
    "\n",
    "We initialize the table (zeros, random values with zeros at terminal condition, etc.) and then use TD(0) updates for training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./Q-Learning_Matrix_Initialized_and_After_Training.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II. Q-Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q-Learning is an **off-policy TD control algorithm** that was an early breakthrough in RL.\n",
    "\n",
    "Quick reminder of what off-policy means:\n",
    "\n",
    "We want action-value estimates. To make improvements requires exploring. These two things are at odds.\n",
    "\n",
    "Consider: You're looking for a faster route to work. If you try different routes, some will be slower.  \n",
    "These slower routes shouldn't factor into the timing of the optimal route. You separate optimal route timing from exploration.\n",
    "\n",
    "We do this by maintaining two policies:\n",
    "- behavior policy for learning\n",
    "- target policy for learning optimality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we show the update equation for improving $q_\\pi(s,a)$  \n",
    "It is very similar to the update step for the state value.\n",
    "\n",
    "Since we will use sample data, $Q$ will denote estimates of $q_\\pi$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$Q(s,a) := Q(s,a) + \\alpha [r + \\gamma \\underset{a}{\\operatorname{\\max}} Q(s',a) -  Q(s,a)]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explaning the different components:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./q_learning_update.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An important difference is the $\\underset{a}{\\operatorname{\\max}} Q(s',a)$ term where you might have expected $Q(s',a)$  \n",
    "\n",
    "The agent computes the most valuable action and uses this in updating.\n",
    "\n",
    "However, the agent many not actually take this step when $S_{t+1}=s'$, $A_{t+1}=a$ \n",
    "\n",
    "This is what it means to act off policy: the target policy is separated from the behavior policy.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Septic Shock**\n",
    "\n",
    "Next, let's look at a computational example. The objective is to reduce the chance of septic shock, measured by the proxy SOFA score, by using a drug called a vasopressor. The values are for illustration only. Following the code are a series of exercises that we will work through.\n",
    "\n",
    "Background:  \n",
    "- **Septic shock**: a life-threatening condition that happens when blood pressure drops to a dangerously low level after an infection\n",
    "- **Sequential Organ Failure Assessment (SOFA) score** is a scoring system that assesses the performance of several organ systems in the body. We will use this to measure state.\n",
    "- **Vasopressor (vaso)** a drug that healthcare providers use to make blood vessels constrict in people with low blood pressure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "np.set_printoptions(formatter={'float': '{: 0.3f}'.format})\n",
    "\n",
    "# Initialize states, actions, Q function\n",
    "\n",
    "# states\n",
    "sofa_levels = [0,1,2,3]\n",
    "num_states = len(sofa_levels)\n",
    "terminal_state = 3\n",
    "\n",
    "# actions\n",
    "vaso_dose = [0,1,2,3,4]\n",
    "num_actions = len(vaso_dose)\n",
    "\n",
    "# initialize array to store action values Q\n",
    "#Q = np.random.normal(size=(num_states, num_actions))\n",
    "Q = np.zeros(shape=(num_states, num_actions))\n",
    "#Q[terminal_state,:] = 0 # no action taken from terminal state, so no value\n",
    "\n",
    "\n",
    "def act(epsilon, action_values):\n",
    "    '''\n",
    "    epsilon-greedy policy: return action using epsilon-greedy strategy\n",
    "    '''\n",
    "    action_size = len(action_values)\n",
    "    if np.random.rand() <= epsilon: # random draw with prob epsilon\n",
    "        return random.randrange(action_size)\n",
    "    return np.argmax(action_values)  # returns action\n",
    "\n",
    "def calc_reward(state):\n",
    "    '''\n",
    "    simple reward function for illustration. lower state value is better\n",
    "    '''\n",
    "    if state == 3:\n",
    "        reward = -100\n",
    "    elif state == 2:\n",
    "        reward = -10\n",
    "    elif state == 1:\n",
    "        reward = 0\n",
    "    else:\n",
    "        reward = 10\n",
    "    return reward\n",
    "\n",
    "def determine_next_state(state, action):\n",
    "    '''\n",
    "    return next state from the environment\n",
    "    to be replaced with simulated data or alternative    \n",
    "    '''\n",
    "    if (state in [0,1,2]) & (action == 0): # no dose raises state\n",
    "        next_state = min(terminal_state, state + 1)\n",
    "    elif action in [3,4]: # higher doses lowers state (floored at zero)\n",
    "        next_state = max(0, state - 1)\n",
    "    else:\n",
    "        next_state = random.choice([1,2])\n",
    "    return next_state\n",
    "\n",
    "# Run the Process\n",
    "num_episodes = 5000\n",
    "max_timesteps = 100\n",
    "epsilon = 0.1\n",
    "alpha = 0.1 # weight on new data \n",
    "gamma = 0.99 # discount factor\n",
    "\n",
    "for ep in range(num_episodes):\n",
    "    if ep % 10 == 0:\n",
    "        print('episode',ep+1)\n",
    "    #print('(state,action,reward,next_state) transitions')\n",
    "    sofa_level = 0 # initialize state\n",
    "    done = False\n",
    "    for tm in range(max_timesteps):\n",
    "        \n",
    "        # given state, get action from policy\n",
    "        vaso_dose = act(epsilon, Q[sofa_level,:])\n",
    "        \n",
    "        next_sofa = determine_next_state(sofa_level, vaso_dose)\n",
    "        reward = calc_reward(next_sofa)\n",
    "        transition = (sofa_level,vaso_dose,reward,next_sofa)\n",
    "        #print(transition)\n",
    "        \n",
    "        # update Q(S,A) using TD(0)\n",
    "        # Q(S,A) = Q(S,A) + alpha (r + gamma * max_a Q(S',a) - Q(S,A))\n",
    "        Q[sofa_level,vaso_dose] += alpha*(reward+gamma*np.amax(Q[next_sofa,:])-Q[sofa_level,vaso_dose])        \n",
    "                \n",
    "        sofa_level = next_sofa # update sofa for next iteration\n",
    "        \n",
    "        # terminal state check\n",
    "        if next_sofa == terminal_state:\n",
    "            done = True\n",
    "            break\n",
    "    if ep % 10 == 0:\n",
    "        print('Q \\n', Q)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 1**\n",
    "\n",
    "If the agent is in state 0, what is the most valuable action? what is least valuable action? Enter your final Q estimate here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 2**\n",
    "\n",
    "How do your answers change with different $\\alpha$? different $\\epsilon$? Enter your final Q estimates here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 3**\n",
    "\n",
    "We initialized Q with standard normal deviates. How do your answers in (1) change if you initialize Q with zeros?  \n",
    "Enter your final Q estimates here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 4**\n",
    "\n",
    "Does Q seem to converge? It will converge given enough iterations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 5**\n",
    "\n",
    "Modify the code to return all transitions as a list of tuples. Paste the first 10 transitions below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### III. Limitations of Q-Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we've learned, Q-learning involves storing and updating a table or array of values $Q(S,A)$ where each element represents the value of a *(state,action)* tuple. This is called a *Q table*.\n",
    "\n",
    "As the number of states and actions (the *state-action space*) grows, this approach becomes unmanageable in terms of both storage and computation. This occurs for continuous variables or discrete variables with a massive number of possible values.\n",
    "\n",
    "There are two approaches to handle this issue:\n",
    "\n",
    "- Quantize the values \n",
    "\n",
    "For example, medication doses might be bucketed into dose ranges  \n",
    "\n",
    "- Function approximators for Q  \n",
    "\n",
    "The function approximation is now very popular, with neural nets playing a major role.\n",
    "\n",
    "**Going Deep**\n",
    "\n",
    "When deep neural networks are used with Q-Learning, the model is called a *Deep Q-Network*. We will study these next.\n",
    "\n",
    "In general, pairing reinforcement learning with a deep neural network is called *Deep Reinforcement Learning*, abbreviated Deep RL."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
